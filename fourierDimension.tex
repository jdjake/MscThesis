%% The following is a directive for TeXShop to indicate the main file
%%!TEX root = diss.tex

\chapter{Fourier Dimension and Fractal Avoidance}
\label{ch:FourierDimension}

There is much interplay between geometric measure theory and Fourier analysis. In particular, the Frostman dimension of a measure $\mu$ can be related to a integrability condition on it's Fourier transform. If we replace this integrability condition with a uniform bound on the decay of the Fourier transform, we obtain a stricter requirement. This leads to the \emph{Fourier dimension} of a measure. A natural question then arises; do sets supporting measures with large Fourier dimension naturally contain patterns?

\section{Basic Technique}

\begin{theorem}
	Let $K$ be a collection of at most $A \cdot N^s$, strongly non-diagonal sidelength $1/N$ dyadic cubes in $[0,1]^{nd}$. Then there exists a random set $X \subset [0,1]^d$, which is a union of sidelength $1/N$ dyadic cubes in $[0,1]^d$, and a constant $C$, depending only on $A$, $d$, and $n$, such that with probability $1 - C/\log N$,
	%
	\begin{itemize}
		\item $X^d \cap K = \emptyset$,
		\item If $\mu$ is the probability measure equal to a constant multiple of the Lebesgue measure on $X$, and supported on $X$, then for any $m \in \{ -N, \dots, N \}^d$,
		%
		\[ |\widehat{\mu}(m) - \widehat{dx}(m)| \leq \frac{C (\log N)^{1 - 1/n}}{N^{d-s/n}}. \]
	\end{itemize}
\end{theorem}
\begin{proof}
	Fix $p = 1/(AN^s \log N)^{1/n}$. Let $\{ X_Q \}$ be a family of independant and identically distributed $\{ 0, 1 \}$ valued Bernoulli random variable, for each sidelength $1/N$ dyadic cube, such that $\mathbf{P}(X_Q = 1) = p$ for each $Q$, and define $X = \bigcup \{ Q : X_Q = 1 \}$. Then Chernoff's inequality implies there exists a universal constant $c$ such that if $S = \sum_Q X_Q$, then
	%
	\[ \mathbf{P} \left( S \geq (1/2)(N^d p) \right) \geq 1 - 2 e^{- c N^d p}. \]
	%
	Substituting in parameters, we conclude
	%
	\begin{equation} \label{fourierdim1} \mathbf{P} \left( S \geq \left[ 1/2 A^{1/n} (\log N)^{1/n} \right] \cdot N^{d - s/n} \right) \geq 1 - 2 \exp \left( \frac{-N^{d-s/n}}{A^{1/n} (\log N)^{1/n}} \right) \end{equation}
	%
	Thus $X$ is the union of a large number of cubes with high probability.

	Given any cube $Q_1 \times \dots \times Q_n \in K$, we have
	%
	\[ \mathbf{P}(Q_1 \times \dots Q_n \subset X) = \mathbf{P}(Q_1 \in X, \dots, Q_n \in X) = p^n. \]
	%
	Thus
	%
	\[ \mathbf{E}(\# \{ Q_1 \times \dots \times Q_n \in X^n \}) \leq A N^s p^n = 1/\log N. \]
	%
	In particular, Markov's inequality implies
	%
	\begin{equation} \label{fourierdim2}
	\begin{split}
		\mathbf{P}(\# \{ Q_1 \times \dots \times Q_n \in X^n \} = 0) &= \mathbf{P}(\# \{ Q_1 \times \dots \times Q_n \in X^n \} < 1)\\
		&\geq 1 - 1/\log N.
	\end{split}
	\end{equation}
	%
	Thus $X^d$ avoids $K$ completely with high probability.

	Now we analyze the Fourier decay of the set $X$, conditioning on the value of $S$. For each cube $Q$, define
	%
	\[ f_Q = \begin{cases} \left( \frac{N^d}{S} - 1 \right) \cdot \mathbf{I}_Q & : X_Q = 1, \\ \left( -1 \right) \cdot \mathbf{I}_Q & : X_Q = 0. \end{cases} \]
	%
	If $f = \sum_Q f_Q$, then $f dx = d\mu - dx$. For each $x$, $\mathbf{E}[f_Q(x)|S] = 0$. If $x \not \in Q$, this is obvious, since $f_Q(x) = 0$ always holds, and if $x \in Q$, we find
	%
	\begin{align*}
		\mathbf{E}[f_Q(x)|S] &= \mathbf{P}(X_Q = 1|S) (N^d/S - 1) + \mathbf{P}(X_Q = 0|S) (-1)\\
		&= (S/N^d)(N^d/S - 1) + (1 - S/N^d)(-1) = 0,
	\end{align*}
	%
	This implies that for each $m \in \mathbf{Z}^d$,
	%
	\[ \mathbf{E} \left[ \widehat{f_Q}(m)|S \right] = \int \mathbf{E}[f_Q(x) | S] e^{- 2 \pi i x \cdot m}\; dx = 0. \]
	%
	Now $|\widehat{f_Q}(m)| \leq 1/S$ for all $Q$, and since the family $\big\{ \widehat{f_Q}(m) \big\}$ are independent random variables as $Q$ varies over all dyadic cubes, we can apply Hoeffding's inequality to conclude that there exists a universal constant $c$ such that for the random variable $\widehat{f}(m) = \sum \widehat{f_Q}(m)$,
	%
	\[ \mathbf{P} \left( |\widehat{f}(m)| \geq \log N /S \right) \leq 2/N^{c \log N} \]
	%
	If we now take a union bound over all $m \in \{ -N, \dots, N \}^d$, we can guarantee that
	%
	\begin{equation} \label{fourierdim3}
		\mathbf{P} \left( |\widehat{f}(m)| \leq \log N/S\ \text{for all $m \in \{ -N, \dots, N\}^d$} \right) \geq 1 - 2^{d+1}/N^{c \log N - d}.
	\end{equation}
	%
	Thus $\widehat{f}$ has good decay with high probability.

	Combining \eqref{fourierdim1}, \eqref{fourierdim2}, and \eqref{fourierdim3}, we conclude that there exists a constant $C$ such that with probability at least
	%
	\[ 1 - 2 \exp \left( \frac{-N^{d-s/n}}{A^{1/n} (\log N)^{1/n}} \right) - 1/\log N - \frac{2^{d+1}}{N^{c \log N - d}} \geq 1 - C / \log N, \]
	%
	the set $X$ avoids $K$, and for all $m \in \{ -N, \dots, N \}^d$,
	%
	\[ |\widehat{f}(m)| \leq \frac{C (\log N)^{1-1/n}}{N^{d-s/n}}. \qedhere \]
\end{proof}

\section{Another Try}

In the last few chapters, we have discovered that the presence of many configurations is not guaranteed by large Hausdorff dimension. We now turn to an analysis of a different dimensional quantity which in many cases gives much more structure than Hausdorff dimension. We construct a set $E$

\begin{proof}
	Let $S_0 = \{ 0 \}$. Then, given $S_k$, let $N^s$
	%
	\[ S_{k+1} = \left\{ (j, mN_{k+1}^{1-\alpha} + X_{jk}) : j \in S_k, 0 \leq m < N_{k+1}^\alpha \right\} \]
	% N_k = M_k^{1/\alpha}
	where $X_{jk}$ is chosen uniformly randomly from all integers in $[0,N_k^\alpha)$. Write $E_k = \{ \bigcup Q_j : j \in S_k \}$. Then $\#(S_{k+1}) = N_{k+1}^\alpha \#(S_k)$, and since $\#(S_0) = 1$, we find $\#(S_k) = (N_1 \dots N_k)^\alpha$. Now define $E_k = \{ \bigcup Q_j : j \in S_k \}$, and probability measures
	%
	\[ f_k = (N_1 \dots N_k)^{1 - \alpha} \mathbf{I}_{E_k}. \]
	%
	Then $\widehat{f_{k+1}}(m) - \widehat{f_k}(m) = \sum_{j \in S_k} Y_j(m)$, where
	%
	\[ Y_j(m) = \int_{I_j} (f_{k+1}(x) - f_k(x)) e(mx)\; dx \]
	%
	Note that $\mathbf{E}[f_{k+1}(x)] = \mathbf{E}[f_k(x)]$ for each $x$, so that $\mathbf{E}[Y_j(m)] = 0$. Note also that
	%
	\[ |Y_j(m)| \leq 2l_k / l_{k+1}^{1-\alpha} = \frac{2 N_{k+1}^{1-\alpha}}{(N_1 \dots N_k)^\alpha}. \]
	%
	Thus Hoeffding's inequality implies
	%
	\begin{align*}
		\mathbf{P} \left( |\widehat{f_{k+1}}(m) - \widehat{f_k}(m)| \geq t \right) &\leq 4 \exp \left( - \left( t \frac{(N_1 \dots N_k)^\alpha}{4 N_{k+1}^{1-\alpha} \#(S_k)^{1/2}} \right)^2 \right)\\
		&\leq 4 \exp \left( - \left( t \frac{(N_1 \dots N_k)^{\alpha/2}}{2 N_{k+1}^{1-\alpha}} \right)^2 \right)
	\end{align*}
	%
	Thus we can guarantee with high probability that for all $m$,
	%
	\[ |\widehat{f_{k+1}}(m) - \widehat{f_k}(m)| \lessapprox \frac{N_{k+1}^{1-\alpha}}{(N_1 \dots N_k)^{\alpha/2}} \]
	%
	So if $N_{k+1} \lesssim (N_1 \dots N_k)^\varepsilon$ for all $\varepsilon > 0$, we obtain the right bound. But we need something deeper.

	Take 2: Write
	%
	\[ f_{k+1}(x) = (N_1 \dots N_{k+1})^{1-\alpha} \sum_{j \in S_k} \sum_{m = 0}^{N_{k+1}^{1-\alpha}} Y_{jm} \]
	%
	where $Y_{jm}$ is the indicator function of
	%
	\[ \left[ \frac{j}{N_1 \dots N_k} + \frac{m}{N_1 \dots N_{k+1}^\alpha} + \frac{X_{jk}}{N_1 \dots N_{k+1}}, \frac{j}{N_1 \dots N_k} + \frac{m}{N_1 \dots N_{k+1}^\alpha} + \frac{X_{jk} + 1}{N_1 \dots N_{k+1}} \right] \]
	%
	Note that
	%
	\[ \widehat{Y_{jm}}(n) = \frac{1}{2 \pi i n} e \left( \frac{j n}{N_1 \dots N_k} + \frac{m n}{N_1 \dots N_{k+1}^\alpha} + \frac{X_{jk} n}{N_1 \dots N_{k+1}} \right) \left( e \left( \frac{n}{N_1 \dots N_{k+1}} \right) - 1 \right), \]
	%
	and
	%
	\begin{align*}
		\mathbf{E} \left(\widehat{Y_{jm}}(n) \right) &= \frac{1}{2 \pi i n N_{k+1}^\alpha} e \left( \frac{jn}{N_1 \dots N_k} + \frac{mn}{N_1 \dots N_{k+1}^\alpha} \right) \left( e \left( \frac{ N_{k+1}^\alpha n}{N_1 \dots N_{k+1}} \right) - 1 \right).
	\end{align*}
	%
	and $|\widehat{Y_{jm}}| \leq 1/(N_1 \dots N_{k+1})$. Thus Hoeffding's inequality implies, if we ignore the expectation,
	%
	%
	\begin{align*}
		\mathbf{P} \left( |\widehat{f_{k+1}}(n)| \geq t (N_1 \dots N_{k+1})^{1-\alpha} \right) &\leq 4 \exp \left( \frac{-t^2 (N_1 \dots N_{k+1})^2}{4(N_1 \dots N_{k+1})^\alpha} \right)\\
		&\leq 4 \exp \left( - \left( t (N_1 \dots N_{k+1})^{1 - \alpha/2} / 2 \right)^2 \right).
	\end{align*}
	%
	Thus with high probability, for all $|n| \leq N_1 \dots N_{k+1}$,
	%
	\[ |\widehat{f_{k+1}}(n)| \lessapprox \frac{1}{(N_1 \dots N_{k+1})^{\alpha/2}}. \]
\end{proof}




\section{Schmerkin's Method}

We now try and adapt Schmerkin's Method to our situation, introducing translations into the avoiding set. Let $K \subset \mathbf{Z}_N^n$. Our goal is to find $X \subset \mathbf{Z}_N$ such that if $x_1, \dots, x_n$ are distinct, then $(x_1, \dots, x_n) \not \in K$, such that if $f: \mathbf{Z}_N \to \mathbf{R}$ is the probability distribution which uniformly randomly chooses a point on $X$, such that the Fourier transform of $f$ has small $L^\infty$ norm.

We consider an intermediary scale $M$, with $M \divides N$. Then we consider the set
%
\[ K' = \{ x \in \mathbf{Z}_N^n : \text{There is $y \in K$ with $x - y \in M \mathbf{Z}_N^n$} \}. \]
%
We construct $X$ such that if $x_1, \dots, x_n \in X$ are distinct, then $(x_1, \dots, x_n) \not \in Y$. In particular, this means that if we define, for each $x \in X$, a uniformly random selected integer $n_x \in M \mathbf{Z}_N$, then we can define $x' = x + n_x$, and $X' = \{ x' : x \in X \}$. Now if $I_x: \mathbf{Z}_N \to \mathbf{R}$ denotes the delta function at $x'$, then
%
\[ \widehat{I_x}(k) = (1/N) e(-kx'/N) = (1/N) e(-kx/N) e(-n_x). \]
%
And so
%
\begin{align*}
	\mathbf{E}(\widehat{I_x}(k)) &= (M/N) e(-kx/N) \sum_{m = 1}^{N/M} e(-(kM/N)m).
\end{align*}
%
If $kM/N \not \in \mathbf{Z}$, $\mathbf{E}(\widehat{I_x}(k)) = 0$. If $kM/N \in \mathbf{Z}$, then $\mathbf{E}(\widehat{I_x}(k)) = e(-kx/N)$. And
%
\[ |\widehat{I_x}(k)| \leq 1. \]
%
Since $I_x$ is independant, we can apply Hoeffding's inequality to conclude that
%
\[ \mathbf{P} \left( \widehat{f}(k) \geq t \right) \leq \exp \left( \frac{-t^2}{4|X|} \right) \]
%
so $\widehat{f}(k) \lesssim |X|^{1/2}$ with high probability. This should give the Fourier dimension bound we want? Except we do lose some dimension by introducing the parameter $M$. If we pick $N$ to be very large, but $M$ very small, then this shouldn't introduce many problems? What about if we pick $M$ randomly to prevent problems on the linear frequencies?






\section{Tail Bounds on Cube Selection}

The aim of this section is to obtain tail bounds on the total number of intersections between the random set we select in the discrete selection schema, with the goal of obtaining a Fourier dimension bound on the set we construct. Consider the random process generating our interval selection scheme. We have three fixed lengths $l$, $r$, and $s$. We start with a set $E$, which is a non-empty union of cubes in $\B^d_l$, and a set $G$, which is a union of cubes in $\B^{dn}_s$. For each cube $J \in \B^d_r(E)$, we select a single random element of $\B^d_s(J)$, denoted $J_I$. We let $U = \bigcup J_I$. The aim of this section is to obtain tail bounds on the size of the random set
%
\[ \mathcal{K}(U) = \{ K \in \B^{dn}_s(G): K \in U^n, K\ \text{strongly non-diagonal} \}. \]
%
Here, this is obtained by applying the statistical decoupling techniques of Bourgain and Tzafriri.

To understand the distribution of this random variable, it will be convenient to simplify notation. We write $\mathbf{I}(I_J = I_0)$ as $X_{J,I_0}$. We then set
%
\[ Z = \# \mathcal{K}(U) = \sum_{J_1, \dots, J_n} \sum_K \left( \prod_{i = 1}^n X_{J_i,K_i} \right), \]
%
where $J_1, \dots, J_n$ ranges over all distinct choices of $n$ elements from $\B^d_l(E)$, and $K = K_1 \times \dots \times K_n$ ranges over all elements of $\B^{dn}_l(G) \cap \B^{dn}_s(J_1 \times \dots \times J_n)$. The random cubes $\{ I_J \}$ form an independant family, which should make the distribution easier to bound. But $Z$ is a multiplicative linear combination of this independant family. Random variables of this form as known as a \emph{chaos}.

The idea behind statistical decoupling is to obtain tail bounds on the chaos $Z$ by studying the tail bounds on the modified random variable
%
\[ W = \sum_{J_1, \dots, J_n} \sum_K \left( \prod_{i = 1}^n X_{J_i,K_i}^i \right), \]
%
where $X^1, \dots, X^n$ are independant and identically distributed copies of $X$.

\begin{lemma} \label{decouplinglemma}
	Let $F: [0,\infty) \to [0,\infty)$ be a monotone, convex function. Then
	%
	\[ \EE(F(Z)) \leq \EE(F(n^n W)). \]
\end{lemma}
\begin{proof}
	Consider a partition $\mathcal{J}_1, \dots, \mathcal{J}_n$ selected at random from all possible partitions of $\B^d_s(E)$ into $n$ classes. Then, for any distinct set of intervals $J_1, \dots, J_n \in \B^d_s(E)$,
	%
	\begin{equation} \label{partitionprob}
		\PP(J_1 \in \mathcal{J}_1, \dots, J_n \in \mathcal{J}_n) = \PP(J_1 \in \mathcal{J}_1) \dots \PP(J_n \in \mathcal{J}_n) = 1/n^n.
	\end{equation}
	%
	Thus if we consider
	%
	\[ Z' = \sum_{J_1 \in \mathcal{J}_1, \dots, J_n \in \mathcal{J}_n} \sum_K \left( \prod_{i = 1}^n X_{J_i,K_i} \right), \]
	%
	where $J_i$ ranges over elements of $\mathcal{J}_i$ for each $i \in [n]$. If we let $\EE_{\mathcal{J}}$ denote conditioning with respect to all random variables except those depending on $\mathcal{J}_1, \dots, \mathcal{J}_n$, then \eqref{partitionprob} implies
	%
	\begin{align*}
		\EE_{\mathcal{J}}(Z') &= \sum_{J_1, \dots, J_n} \sum_K \PP(J_1 \in \mathcal{J}_1, \dots, J_n \in \mathcal{J}_n) \left( \prod_{i = 1}^n X_{J_i,K_i} \right) = Z/n^n.
	\end{align*}
	%
	Thus Jensen's inequality implies that
	%
	\[ F(Z) = F(n^n \EE_{\mathcal{J}}(Z')) \leq \EE_{\mathcal{J}} F(n^n Z'). \]
	%
	In particular, this means we can select a single, non-random partition $\mathcal{J}_1, \dots, \mathcal{J}_n$ of $\B^d_s(E)$ such that $F(Z) \leq F(n^n Z')$. Because $\mathcal{J}_1, \dots, \mathcal{J}_n$ are disjoint sets, $Z'$ is identically distributed to
	%
	\[ W' = \sum_{J_1 \in \mathcal{J}_1, \dots, J_n \in \mathcal{J}_n} \sum_K \left( \prod_{i = 1}^n X_{J_i,K_i}^i \right). \]
	%
	But $W' \leq W$, and so by monotonicity,
	%
	\[ \EE(F(Z)) \leq \EE(F(n^n W')) \leq \EE(F(n^n W)). \qedhere \]
\end{proof}

\begin{lemma}
	MOMENT BOUND THEOREM.
\end{lemma}
\begin{proof}
	We now bound the moments of $W$, which by Lemma \ref{decouplinglemma} bounds the moments of $Z$. We write
	%
	\[ W^m = \sum_{J_{j,i}} \sum_{K_1, \dots, K_m} \left( \prod_{i = 1}^n \prod_{j = 1}^m X_{J_{j,i}, K_{j,i}}^i \right). \]
	%
	Thus
	%
	\[ \EE(W^m) = \sum_{J_{j,i}} \sum_{K_1, \dots, K_m} \prod_{i = 1}^n \EE \left( \prod_{j = 1}^m X_{J_{j,i}, K_{j,i}}^i \right) \]
	%
	The inner expectation is zero except if $\pi(K_{j,i}) = J_{j,i}$ for all $j$ and $i$, and also, if $\pi(K_{j_0,i}) = \pi(K_{j_1,i})$, then $K_{j_0,i} = K_{j_1,i}$. And then the expectation is $\varepsilon^{\# \{ K_{1,i}, \dots, K_{m,i} \}}$.
\end{proof}

\section{Singular Value Decomposition of Tensors}

An \emph{$n$ tensor} with respect to $\RR^{k_1}, \dots, \RR^{k_n}$, is a multi-linear function
%
\[ A: \RR^{k_1} \times \dots \times \RR^{k_n} \to \RR. \]
%
The family of all $n$ tensors forms a vector space, denoted $\RR^{k_1} \otimes \dots \otimes \RR^{k_n}$, or $\bigotimes \RR^{k_i}$. Given a family of vectors $v_i$, with $v_i \in \RR^{k_i}$, we can consider a tensor $v_1 \otimes \dots \otimes v_n$, such that
%
\[ (v_1 \otimes \dots \otimes v_n)(w_1, \dots, w_n) = (v_1 \cdot w_1) \dots (v_n \cdot w_n). \]
%
It is easily verified that a basis for $\RR^{k_1} \otimes \dots \otimes \RR^{k_n}$ is given by
%
\[ \{ e_{j_1} \otimes \dots \otimes e_{j_n} : j_i \in [k_i] \}. \]
%
Thus a tensor $A$ can be identified with a family of coefficients
%
\[ \{ A_{j_1 \dots j_n} : 1 \leq j_i \leq k_i \} \]
%
such that
%
\[ A(v_1,\dots,v_n) = \sum A_{j_1 \dots j_n} \cdot e_{j_1} \otimes \dots \otimes e_{j_n}. \]
%
From this perspective, a tensor is a multi-dimensional array given by $n$ indices. This basis induces a natural inner product for which the basis is orthonormal. The norm with respect to this inner product is called the \emph{Frobenius norm}, denoted $\| A \|_F$.

Recall that if $A$ is a rank $r$, $m \times n$ matrix, then the \emph{singular value decomposition} says that there are orthonormal families of vectors $\{ u_1, \dots, u_r \}$ and $\{ v_1, \dots, v_r \}$ in $\RR^m$ and $\RR^n$ respectively, and values $s_1, \dots, s_r > 0$ such that $A = \sum s_i (u_i^T v_i)$. We may view $A$ as a 2 tensor in $\RR^m \otimes \RR^n$ by setting $A(v,w) = v^T A w$. Then the singular value decomposition says that $A = \sum s_i (u_i \otimes v_i)$. Thus, viewing $A$ as a bilinear function, for any vectors $x \in \RR^m$ and $y$ in $\RR^n$, $A(x,y) = \sum s_i (u_i \cdot x) (v_i \cdot y)$. We want to try and generalize this result to $n$ tensors in $\RR^{k_1} \otimes \dots \otimes \RR^{k_n}$.

One can reduce certain quantities over tensors to quantities defined with respect to linear maps. For instance, for each index $i_0 \in [n]$, we can define a linear map
%
\[ A_i: \bigotimes_{i' \neq i} \RR^{k_{i'}} \to \RR^{k_i} \]
%
by letting $A_{i_0}(v_1, \dots, \widehat{v_i}, \dots, v_n)$ to be the unique vector $w \in \RR^{k_i}$ such that for any vector $v_i \in \RR^{k_i}$,
%
\[ v_i \cdot w = A(v_1, \dots, v_n). \]
%
The matrix $A_i$ is known as the \emph{$i$'th unfolding of $A$}. We define the $i$ rank of $A$ to be $\text{rank}_i(A) = \text{rank}(A_i)$. This generalizes the column and row rank of matrices. Unlike in the case of matrices, these quantities can differ for different indices $i$. One can also obtain the $i$ rank

The natural definition of the rank of a tensor in the study of the higher order singular value decomposition is defined in analogy with the fact that a rank $r$ matrix can be written as the sum of $r$ rank one matrices. We say a tensor is rank one if it can be written in the form $v_1 \otimes \dots \otimes v_n$ for some vectors $v_i \in \RR^{k_i}$. The \emph{rank} of a tensor $A$, denoted $\text{rank}(A)$, is the minimal number of rank one tensors which can be written in a linear combination to form $A$. We have $\text{rank}(A_i) \leq \text{rank}(A)$ for each $i$, but there may be a gap in these ranks.

To obtain an SVD decomposition, fix an index $i$. Then we can perform a singular value decomposition on $A_i$, obtaining orthonormal basis

\section{Decoupling Concentration Bounds}

We require some methods in non-asymptotic probability theory in order to understand the concentration structure of the random quantities we study. Most novel of these is relying on a probabilistic decoupling bound of Bourgain.

In this section, we fix a single underlying probability space, assumed large enough for us to produce any additional random quantities used in this section. Given a scalar valued random variable $X$, we define its \emph{Orlicz norm} as
%
\[ \| X \|_{\psi_2} = \inf \left\{ \lambda \geq 0 : \EE \left(\exp \left( |X|^2/\lambda^2 \right) \right) \leq 2 \right\}. \]
%
The family of random variables with finite Orlicz norm forms a Banach space, known as the space of \emph{subgaussian random variables}.

\begin{theorem}
	Let $X$ be a sub-gaussian random variable.
	%
	\begin{itemize}
		\item For all $t \geq 0$,
		%
		\[ \PP \left( |X| \geq t \right) \leq 2 \exp \left( \frac{-t^2}{\| X \|_{\psi_2}^2} \right). \]

		\item Conversely, if there is a constant $K > 0$ such that for all $t \geq 0$,
		%
		\[ \PP \left( |X| \geq t \right) \leq 2 \exp \left( \frac{-t^2}{K^2} \right), \]
		%
		then $\| X \|_{\psi_2} \leq 2K$.

		\item For any bounded random variable $X$, $\| X \|_{\psi_2} \lesssim \| X \|_\infty$.

		\item $\| X - \EE X \|_{\psi_2} \lesssim \| X \|$.

		\item If $X_1, \dots, X_n$ are independent, then
		%
		\[ \| X_1 + \dots + X_n \|_{\psi_2} \lesssim \left( \| X_1 \|_{\psi_2}^2 + \dots + \| X_n \|_{\psi_2}^2 \right)^{1/2}. \] 
	\end{itemize}
\end{theorem}
\begin{proof} See \cite{Vershynin}. \end{proof}

Given a random vector $X$, taking values in $\RR^n$, we say it is \emph{uniformly subgaussian} if there is a constant $A$ such that for any $x \in \RR^n$, $\| X \cdot x \|_{\psi_2} \leq A \cdot |x|$. We define
%
\[ \| X \|_{\psi_2} = \sup_{|x| = 1} \| X \cdot x \|_{\psi_2}, \]
%
so that $\| X \cdot x \|_{\psi_2} \leq \| X \|_{\psi_2} \cdot |x|$ for any random vector $X$.

The classic Hanson-Wright inequality gives a concentration bound for the magnitude of a quadratic form $X^T A X$ in terms of the matrix norms of $A$, assuming $X$ is a random vector with independant, mean zero, subgaussian coordinates.

\begin{theorem}[Hanson-Wright]
	Let $X = (X_1, \dots, X_k)$ be a random vector with independant, mean zero, subgaussian coordinates, with $\| X_i \|_{\psi_2} \leq K$ for each $i$, and let $A$ be a diagonal-free $k \times k$ matrix. Then there is a universal constant $c$ such that for every $t \geq 0$,
	%
	\[ \PP \left( |X^TAX| \geq t \right) \leq 2 \exp \left( -c \cdot \min \left( \frac{t^2}{K^4 \| A \|_F^2}, \frac{t}{K^2 \| A \|} \right) \right). \]
	%
	Here $\| A \|$ is the operator norm of $A$, and $\| A \|_F$ it's Frobenius norm.
\end{theorem}

In our work, we require a modified version of the Hanson Wright inequality where we replace the independant random variables $X_1, \dots, X_k$ with independant random vectors taking values in $\RR^d$, and where $A$ is replaced with a $n$ tensor on $\RR^{k \times d}$. To make things more tractable, we assume that $A_I = 0$ for each multi-index $I = ((i_1,j_1), \dots, (i_n,j_n))$ if there is $r \neq r'$ with $i_r = i_{r'}$. If this is satisfied, we say $A$ is a \emph{non-diagonal tensor}.

\begin{theorem}
	Let $X = (X_1, \dots, X_k)$, where $X_1, \dots, X_k$ are independant, mean zero random vectors taking values in $\RR^d$, with $\| X_i \|_{\psi_2} \leq K$ for each $i$. Let $A$ be a non-diagonal $n$-tensor in $\RR^{k \times d}$. Then for $t \geq 0$,
	%
	\[ \PP \left( |A(X,\dots,X)| \geq t \right) \leq 2 \cdot \exp \left( - c(n) \cdot \min \left( \frac{t^2}{K^4 \| A \|_F^2}, \dots, \frac{t^{2/n}}{K^{4/n} \| A \|_F^{2/n}} \right) \right). \]
\end{theorem}

The main technique involved is a statistical decoupling result due to J. Bourgain and L. Tzafriri. We follow the proof of the Hanson-Wright inequality from \cite{Vershynin}, altering various calculations to incorporate the vector-valued coordinates $X_1, \dots, X_n$, and incorporating more robust calculations of the moments of higher order Gaussian chaos.

\begin{lemma}[Decoupling]
	If $F: \RR \to \RR$ is convex, and $X = (X_1, \dots, X_k)$, where $X_1, \dots, X_k$ are independant, mean zero random vectors taking values in $\RR^d$. If $A$ is a non-diagonal $n$-tensor in $\RR^{d \times k}$, then
	%
	\[ \EE(F(A(X,\dots,X))) \leq \EE(F(n^n \cdot A(X^1,X^2,\dots,X^n))), \]
	%
	where $X^1, \dots, X^n$ are independant copies of $X$.
\end{lemma}
\begin{proof}
	Given a set $I \subset [k]$, let $\pi_I: \RR^{k \times d} \to \RR^{k \times d}$ be defined by
	%
	\[ \pi_{I}(X)_{ij} = \begin{cases} X_{ij} &: \text{if}\ i \in I, \\ 0 &: \text{if}\ i \not \in I \end{cases}. \]
	%
	Thus given $X = (X_1, \dots, X_k)$, the vector $\pi_I(X)$ is obtained by zeroeing out all vectors $X_i$ with $i \not \in I$. Consider a random partition of $[k]$ into $n$ classes $I_1, \dots, I_n$, chosen uniformly at random. Thus given any $x \in \RR^{d \times k}$, we have
	%
	\[ \EE(A(\pi_{I_1}(x),\dots,\pi_{I_n}(x))) = \sum_{I=((i_1,j_1),\dots,(i_n,j_n))} A_I \cdot x_{i_1 j_1} \dots x_{i_n j_n} \PP(i_1 \in I_1, \dots, i_n \in I_n). \]
	%
	If $A_I \neq 0$, the indices $\{ i_r \}$ are distinct from one another. Since $(I_1,\dots,I_n)$ is selected uniformly at random, this means the positions of the indices $i_r$ are independent of one another. Thus
	%
	\[ \PP(i_1 \in I_1, \dots, i_n \in I_n) = \PP(i_1 \in I_1) \dots \PP(i_n \in I_n) = 1/n^n. \]
	%
	Thus we have shown
	%
	\[ A(x,\dots,x) = n^n \cdot \EE(A(\pi_{I_1}(x), \dots, \pi_{I_n}(x))). \]
	%
	In particular, substituting $X$ for $x$, we have
	%
	\[ A(X,\dots,X) = n^n \cdot \EE_I(A(\pi_{I_1}(X), \dots, \pi_{I_n}(X))) \]
	%
	Applying Jensen's inequality, we find
	%
	\begin{align*}
		\EE_X(F(A(X,\dots,X))) &= \EE_X(F(n^n \cdot \EE_I(A(\pi_{I_1}(X), \dots, \pi_{I_n}(X)))))\\
		&\leq \EE_X \EE_I F(n^n \cdot A(\pi_{I_1}(X), \dots, \pi_{I_n}(X)))\\
		&= \EE_I \EE_X F(n^n \cdot A(\pi_{I_1}(X), \dots, \pi_{I_n}(X))).
	\end{align*}
	%
	Thus we may select a \emph{deterministic} partition $I$ for which
	%
	\[ \EE(F(A(X,\dots,X))) \leq \EE(F(n^n \cdot A(\pi_{I_1}(X),\dots,\pi_{I_n}(X)))). \]
	%
	Since the classes $I_1, \dots, I_n$ are disjoint from one another, $(\pi_{I_1}(X), \dots, \pi_{I_n}(X))$ is identically distributed to $(\pi_{I_1}(X^1), \dots, \pi_{I_n}(X^n))$, so
	%
	\[ \EE(F(n^n \cdot A(\pi_{I_1}(X),\dots,\pi_{I_n}(X)))) = \EE(F(n^n \cdot A(\pi_{I_1}(X^1),\dots,\pi_{I_n}(X^n)))). \]
	%
	Let $Z = A(X^1,\dots,X^n) - A(\pi_{I_1}(X^1),\dots,\pi_{I_n}(X^n))$. If $\EE'$ denotes conditioning with respect to $\pi_{I_1}(X^1), \dots, \pi_{I_n}(X^n)$, then this expectation fixes $A(\pi_{I_1}(X_1),\dots,\pi_{I_n}(X_n))$, but $\EE'(Z) = 0$. Thus we can apply Jensen's inequality again to conclude
	%
	\begin{align*}
		\EE(F(n^n \cdot A(\pi_{I_1}(X^1),\dots,\pi_{I_n}(X^n)))) &= \EE(F(n^n \cdot \EE'(A(\pi_{I_1}(X^1), \dots, \pi_{I_n}(X^n)) + Z)))\\
		&= \EE(F(n^n \cdot \EE'(A(X^1,\dots,X^n))))\\
		&\leq \EE(\EE'(F(n^n A(X^1,\dots,X^n))))\\
		&= \EE(F(n^n A(X^1,\dots,X^n))). \qedhere
	\end{align*}
\end{proof}

\begin{remark}
	Note this theorem also implies that for each $\lambda \in \RR$,
	%
	\[ \EE(F((\lambda/n^n) \cdot A(X,\dots,X))) \leq \EE(F(\lambda \cdot A(X^1,\dots,X^n))). \]
\end{remark}

\begin{lemma}[Comparison]
	Let $X^1,\dots,X^n$ be independant random vectors taking values in $\RR^{k \times d}$
\end{lemma}

To make things easy for ourselves, we assume that $A$ vanishes along it's diagonal. Reinterpreting the quantities in the Hanson-Wright inequality in this setting gives the result. For each $i$ and $j$, we define $A_{ij}$ be an $d \times d$ matrix, then we set $X^T A X = \sum X_i^T A_{ij} X_j$,
%
\[ \| A \|_F = \sqrt{\sum \| A_{ij} \|_F^2}, \quad\text{and}\quad \| A \| = \sup \left\{ |\sum x_i^T A_{ij} y_j| : \sum |x_i|^2 = \sum |y_j|^2 = 1 \right\}. \]
%
These are precisely the definitions of the operator norm and Frobenius norm if we interpret $A$ as a $dn \times dn$ matrix by expanding out the entries of each $A_{ij}$, and $X$ as a random vector taking values in $\RR^{dn}$ by expanding out the entries of each $X_i$.

\endinput