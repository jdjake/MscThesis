%% The following is a directive for TeXShop to indicate the main file
%%!TEX root = diss.tex

\chapter{Background}
\label{ch:Background}

\section{Configuration Avoidance}

We begin by describing the problem we wish to solve. We consider an ambient set $\AAA$. It's \emph{$n$-point configuration space} is
%
\[ \Config^n(\AAA) = \{ (x_1, \dots, x_n) \in X^n: x_i \neq x_j\ \text{if $i \neq j$} \}. \]
%
An \emph{$n$ point configuration}, or \emph{pattern}, is a subset of $\Config^n(\AAA)$. More generally, we define the general \emph{configuration space} of $\AAA$ as $\Config(\AAA) = \bigcup_{n = 1}^\infty \Config^n(\AAA)$, and a \emph{pattern}, or \emph{configuration}, on $\AAA$ is a subset of $\Config(\AAA)$.

Our main focus on the thesis is the \emph{pattern avoidance problem}. For a fixed configuration $\C$ on $\AAA$, we say a set $X \subset \AAA$ \emph{avoids} $\C$ if $\Config(X)$ is disjoint from $\C$. The pattern avoidance problem asks to find sets $X$ of maximal size avoiding a fixed configuration $\C$. The set $\C$ often describes the presence of algebraic or geometric structure, and so we are trying to find large sets which do not possess this structure.

\begin{example}[Isoceles Triangle Configuration]
	Let
	%
	\[ \C = \left\{ (x_1, x_2, x_3) \in \Config^3(\RR^2) : |x_1-x_2| = |x_1-x_3| \right\}. \]
	%
	Then $\C$ is a 3-point configuration, and a set $X \subset \RR^2$ avoids $\C$ if and only if it does not contain all vertices of an isoceles triangle. %Notice that $|x_1 - x_2| = |x_1 - x_3|$ holds if and only if $|x_1 - x_2|^2 = |x_1 - x_3|^2$, which is an algebraic equation in the coordinates of $x_1,x_2$, and $x_3$. Thus $\C$ is an algebraic hypersurface of degree two in $\RR^6$.
\end{example}

\begin{example}[Linear Independence Configuration]
	Let $V$ be a vector space over a field $K$. We set
	%
	\[ \C = \bigcup_{n = 1}^\infty \{ (x_1, \dots, x_n) \in \C^n(V): \text{for any}\ a_1, \dots, a_n \in K,\ a_1x_1 + \dots + a_nx_n \neq 0 \}. \]
	%
	A subset $X \subset V$ avoids $\C$ if and only if $X$ is a linearly independent subset of $X$. Note that if $V$ is infinite dimensional, then $\C$ cannot be replaced by a $n$ point configuration for any $n$; arbitrarily large tuples must be considered. We will be interested in the case where $K = \QQ$, and $V = \RR$, where we will be looking for analytically large, linearly independant subsets of $V$.
\end{example}

%\begin{example}[General Position Configuration]
%	Suppose we wish to find a subset $X$ of $\RR^d$ such that for each positive integer $k \leq d$, and for each collection of $k+1$ distinct points $x_1, \dots, x_{k+1} \in X$, the points do not lie in a $k-1$ dimensional hyperplane. For each $k \leq d$, set
	%
%	\[ \C^{k+1} = \{ (x_0, x_1, \dots, x_k) \in \Config^{k+1}(\RR^d): x_1-x_0, \dots, x_k - x_0\ \text{are linearly dependant} \}. \]
	%
%	If we define $\C = \bigcup_{k = 2}^d \C^k$, then a set $X$ avoids $\C$ precisely when all finite collection of distinct points in $X$ lie in general position. Notice that
	%
%	\[ \C^{k+1} = \bigcup \left\{ \text{span}(y_1, \dots, y_k) \times \{ y \} : y = (y_1, \dots, y_k) \in \Config^k(\RR^d) \right\} \cap \Config^{k+1}(\RR^d). \]
	%
%	so each $\C^{k+1}$ is essentially a union of $k$ dimensional hyperplanes.
%\end{example}

Even though our problem formulation assumes configurations are formed by distinct sets of points, one can still formulate avoidance problems involving repeated points in our framework by a simple trick.

\begin{example}[Sum Set Configuration]
	Let $G$ be an abelian group, and fix $Y \subset G$. Set
	%
	\[ \C^1 = \{ g \in \Config^1(G): g + g \in Y \} \quad \text{and} \quad \C^2 = \{ (g_1,g_2) \in \Config^2(G): g_1 + g_2 \in Y \}. \]
	%
	Then set $\C = \C^1 \cup \C^2$. A set $X \subset G$ avoids $\C$ if and only if $(X + X) \cap Y = \emptyset$.
\end{example}

Depending on the structure of the ambient space $\AAA$ and the configuration $\C$, there are various ways of measuring the size of sets $X \subset \AAA$:
%
\begin{itemize}
	\item If $\AAA$ is finite, the goal is to find a set $X$ with large cardinality.
	\item If $\{ \AAA_n \}$ is an increasing family of finite sets with $\AAA = \lim \AAA_n$, the goal is to find a set $X$ such that $X \cap \AAA_n$ has large cardinality asymptotically in $n$.
	\item If $\AAA = \RR^d$, but $\C$ is a discrete configuration, then a satisfactory goal is to find a set $X$ with large Lebesgue measure avoiding $\C$.
\end{itemize}
%
In this thesis, inspired by results in these three settings, we establish methods for avoiding non-discrete configurations $\C$ in $\RR^d$. Here, Lebesgue measure completely fails to measure the size of pattern avoiding solutions, as the next theorem shows, under the often true assumption that $\C$ is \emph{translation invariant}, i.e. that if $(a_1, \dots, a_n) \in \C$ and $b \in \RR^d$, $(a_1 + b, \dots, a_n + b) \in \C$.

\begin{theorem}
	Let $\C$ be a $n$-point configuration on $\RR^d$. Suppose
	%
	\begin{enumerate}
		\item \label{translationinvariance} $\C$ is translation invariant.
		\item \label{nonDiscreteConfig} For any $\varepsilon > 0$, there is $(a_1, \dots, a_n) \in \C$ with $\diam \{ a_1, \dots, a_n \} \leq \varepsilon$.
	\end{enumerate}
	%
	Then no set with positive Lebesgue measure avoids $\C$.
\end{theorem}
\begin{proof}
	Let $X \subset \RR^d$ have positive Lebesgue measure. The Lebesgue density theorem shows that there exists a point $x \in X$ such that
	%
	\begin{equation} \label{densityApplication} \lim_{l(Q) \to 0} \frac{|X \cap Q|}{|Q|} = 1, \end{equation}
	%
	where $Q$ ranges over all cubes in $\RR^d$, and $l(Q)$ denotes the sidelength of $Q$. Fix $\varepsilon > 0$, to be specified later, and choose $r$ small enough that $|X \cap Q| \geq (1 - \varepsilon) |Q|$ for any cube $Q$ with $x \in Q$ and $l(Q) \leq r$. Now let $Q_0$ denote the cube centered at $x$ with $l(Q_0) \leq r$.

	Applying Property \ref{nonDiscreteConfig}, we find $C = (a_1, \dots, a_n) \in \C$ such that $\diam \{ a_1, \dots, a_n \} \leq l(Q)/2$. For each $p \in Q_0$, let $C(p) = (a_1(p), \dots, a_n(p))$, where $a_i(p) = p + (a_i - a_1)$. Property \ref{translationinvariance} implies $C(p) \in \C$ for each $p \in \RR^d$. A union bound shows
	%
	\begin{equation} \label{equation548} \left| \{ p \in Q_0 : C(p) \not \in \C(X) \} \right| \leq \sum_{i = 1}^d \left| \{ p \in Q_0 : a_i(p) \not \in X \} \right|.
	\end{equation}
	%
	Note that $a_i(p) \not \in X$ precisely when $p + (a_i - a_1) \not \in X$, so
	%
	\[ |\{ p \in Q_0 : a_i(p) \not \in X \}| = |(Q_0 + (a_i - a_1)) \cap X^c|. \]
	%
	Note $Q_0 + (a_i - a_1)$ is a cube with the same sidelength as $Q_0$. Since $|a_i - a_1| \leq r/2$, $x \in Q_0 + (a_i - a_1)$. Thus \eqref{densityApplication} shows
	%
	\begin{equation} \label{equation543} |Q_0 + (a_i - a_1)) \cap X^c| \leq \varepsilon |Q_0|. \end{equation}
	%
	Combining \eqref{equation548} and \eqref{equation543}, we find
	%
	\[ \left| \{ p \in Q_0 : C(p) \not \in \C(X) \} \right| \leq \varepsilon d |Q_0|. \]
	%
	Provided $\varepsilon d < 1$, this means there is $p \in Q_0$ with $C(p) \in \C(X)$.
\end{proof}

Since no set of positive Lebesgue measure can avoid non-discrete configurations, we cannot use the Lebesgue measure to quantify the size of pattern avoiding sets. Fortunately, there is a quantity which can distinguish between the size of sets of measure zero. This is the \emph{fractional dimension} of a set.

%We primarily use two variants of fractional dimension in this thesis: Minkowski dimension, Hausdorff dimension. Both assign the same dimension to any smooth manifold, but vary over more singular sets. The main difference between the first two is that Minkowski dimension measures relative density at a single scale, whereas Hausdorff dimension measures relative density simultaneously at a countable set of scales.









\section{Fractional Dimension}

There are many variants of fractional dimension. Here we choose to use the Minkowski dimension, and the Hausdorff dimension. They assign the same dimension to any smooth manifold, but vary over more singular sets. The biggest difference is that Minkowski dimension measures relative density at a single scale, whereas Hausdorff dimension measures relative density at countably many scales. This makes Hausdorff dimension more stable under analytical operations.

We begin by discussing the Minkowski dimension, which is the simplest to define. Given a length $l$, and a bounded set $E \subset \RR^d$, we let $N(l,E)$ denote the length $l$ covering number of $E$, i.e. the minimum number of sidelength $l$ cubes required to cover $E$. We define the \emph{lower} and \emph{upper} Minkowski dimension as
%
\[ \lowminkdim(E) = \liminf_{l \to 0} \frac{\log(N(l,E))}{\log(1/l)} \quad \upminkdim(E) = \limsup_{l \to 0} \frac{\log(N(l,E))}{\log(1/l)}. \]
%
If $\upminkdim(E) = \lowminkdim(E)$, then we refer to this common quantity as the \emph{Minkowski dimension} of $E$, denoted $\minkdim(E)$. Thus $\lowminkdim(E) < s$ if there {\it exists} a sequence of lengths $\{ l_k \}$ converging to zero with $N(l_k,E) \leq (1/l_k)^s$, and $\upminkdim(E) < s$ if $N(l,E) \leq (1/l)^s$ for \emph{all} sufficiently small lengths $l$.

\begin{remark}
	We have a trivial measure bound $|E| \leq N(l,E) \cdot l^d$. If $\lowminkdim(E) = s < d$, then there is an infinite sequence $\{ l_k \}$ such that $|E| \leq l_k^{d-s}$, and taking $k \to \infty$, we conclude $|E| = 0$. Thus Minkowski dimension is a way of distinguishing between sets of measure zero, which is precisely what we needed in the last section.
\end{remark}

Hausdorff dimension is slightly more technical to define. It is obtained by finding a canonical `$s$ dimensional measure' $H^s$ on $\RR^d$ for $s \in [0,\infty)$, which for $s = 1$, measures the `length' of a set, for $s = 2$, measure the `area', and so on and so forth. We then define the dimension of $E$ to be the supremum of the values $s$ such that $H^s(E) < \infty$. For $E \subset \RR^d$ and $\delta > 0$, we define the \emph{Hausdorff content}
%
\[ H_\delta^s(E) = \inf \left\{ \sum_{k = 1}^\infty l(Q_k)^s : E \subset \bigcup_{k = 1}^\infty Q_k, l(Q_k) \leq \delta \right\}. \]
%
We then set $H^s(E) = \lim_{\delta \to 0} H_\delta^s(E)$. It is easy to see $H^s$ is an exterior measure, and $H^s(E \cup F) = H^s(E) + H^s(F)$ if the Hausdorff distance $d(E,F)$ between $E$ and $F$ is positive. So $H^s$ is actually a metric exterior measure, and the Caratheodory extension theorem shows all Borel sets are measurable with respect to $H^s$.

\begin{lemma} \label{HausdorffBoundary}
	Consider $t < s$, and a set $E$.
	%
	\begin{itemize}
		\item If $H^t(E) < \infty$, then $H^s(E) = 0$.
		\item If $H^s(E) \neq 0$, then $H^t(E) = \infty$.
	\end{itemize}
\end{lemma}
\begin{proof}
	If, for any cover of $E$ by a collection of intervals $\{ Q_k \}$, $\sum l(Q_k)^t \leq A$, and $l(Q_k) \leq \delta$, then
	%
	\[ \sum l(Q_k)^s \leq \sum l(Q_k)^{s-t} l(Q_k)^t \leq \delta^{s-t} A. \]
	%
	Thus $H^s_\delta(E) \leq \delta^{s-t} A$, and taking $\delta \to 0$, we conclude $H^s(E) = 0$. The latter point is just proved by taking contrapositives.
\end{proof}

\begin{remark}
	It is easy to see directly from the definition that $H^d$ is the Lebesgue measure on $\RR^d$. Thus $H^d[-N,N]^d = (2N)^d$. If $s > d$, Lemma \ref{HausdorffBoundary} shows $H^s[-N,N]^d = 0$, and so by countable additivity, taking $N \to \infty$ shows $H^s(\RR^d) = 0$. Thus $H^s(E) = 0$ for all $E$ if $s > d$.
\end{remark}

Given any Borel set $E$, the last remark, combined with Lemma \ref{HausdorffBoundary}, implies there is a unique value $s_0 \in [0,d]$ such that $H^s(E) = 0$ for $s > s_0$, and $H^s(E) = \infty$ for $0 \leq s < s_0$. We refer to $s_0$ as the \emph{Hausdorff dimension} of $E$, denoted $\hausdim(E)$.

\begin{remark}
	If $\hausdim(E) < d$, then $|E| = H^d(E) = 0$. Thus, just as Minkowski dimension is a way of distinguishing between sets of measure zero, so too is the Hausdorff dimension.
\end{remark}

\begin{theorem}
	For any bounded set $E$, $\hausdim(E) \leq \lowminkdim(E) \leq \upminkdim(E)$.
\end{theorem}
\begin{proof}
	Suppose there is a sequence $\{ l_k \}$ with $l_k \to 0$ such that for all $\varepsilon > 0$, $N(l_k,E) \leq (1/l_k)^{s - \varepsilon}$ for sufficiently large $k$. We then consider the simple bound $H^s_{l_k}(E) \leq N(l_k,E) \cdot l_k^s \leq l_k^\varepsilon$, and taking $l_k \to 0$ shows $H^s(E) = 0$. Thus $\hausdim(E) \leq s$, and since $\{ l_k \}$ was arbitrary, we conclude $\hausdim(E) \leq \lowminkdim(E)$.
\end{proof}

The fact that Hausdorff dimension is defined with respect to multiple scales makes it more stable under analytical operations. In particular,
%
\[ \hausdim \left\{ \bigcup E_k \right\} = \sup \left\{ \hausdim(E_k). \right\} \]
%
This need not be true for the Minkowski dimension; a single point has Minkowski dimension zero, but the rational numbers, which are a countable union of points, have Minkowski dimension one. An easy way to make Minkowski dimension countably stable is to define the \emph{modified Minkowski dimensions}
%
\begin{align*}
	\lmbdim(E) &= \inf \left\{ s : E \subset \bigcup_{i = 1}^\infty E_i, \lowminkdim(E_i) \leq s \right\}\\
	&\text{and}\\
	\umbdim(E) &= \inf \left\{ s : E \subset \bigcup_{i = 1}^\infty E_i, \upminkdim(E_i) \leq s \right\}.
\end{align*}
%
This notion of dimension, in a disguised form, appears later on in this thesis.







\section{Dyadic Scales}

It is now useful to introduce the dyadic notation we utilize throughout this thesis. At the cost of losing certain topological information about $\RR^d$, applying dyadic techniques often allows us to elegantly discretize certain problems in Euclidean space. We introduce fractional dimension through a dyadic framework, and all our constructions will be done dyadically.

We fix a sequence of positive integers $\{ N_k : k \geq 1 \}$, referred to as \emph{branching factors}. Using these integers, we define a sequence of positive rational numbers $\{ l_k : k \geq 0 \}$, recursively such that $l_0 = 1$, and $l_{k+1} = l_k/N_{k+1}$. For each $k \geq 0$, we can define
%
\[ \DQ_k^d = \left\{ [ a_1, a_1 + l_k ] \times \dots \times [ a_d, a_d + l_k ] : a \in l_k \cdot \ZZ^d \right\}, \]
%
and $\DQ^d = \bigcup_{k \geq 0} \DQ_m^k$. We call the set $\DQ_k^d$ the \emph{dyadic cubes of generation $k$}. For any set $E \subset \RR^d$, we let $\DQ_k^d(E) = \{ Q \in \DQ_k^d : Q \cap E \neq \emptyset \}$.

For the purposes of discretization, and to simplify notation, it is very useful to identify $\DQ_k^d$ with the set $\Sigma_k^d = \ZZ^d \times [N_1]^d \times \dots \times [N_k]^d$ of indices\footnote{We view each $N_k$ as the $k$'th step of a certain construction. If we have to utilize multiple scales in a single step of a construction, then we can write $N_k = M_k K_k$, and abuse notation, writing $[N_k]^d = [K_k]^d \times [M_k]^d$.}. Given $j = (j_0, \dots, j_k) \in \Sigma_m^d$, we let $Q_j \in \DQ_m^d$ denote the cube with left-hand corner $a = \sum_{k = 0}^m j_kl_k$. For the purpose of iterative discretizations, it is useful to go one step further; we have a canonical map $j \mapsto j^*$ which takes $j = (j_0, \dots, j_{k+1}) \in \Sigma_{k+1}^d$ to it's `parent index' $j^* = (j_0, \dots, j_k) \in \Sigma_k^d$, and so we can compute an inverse limit
%
\[ \Sigma^d = \ZZ^d \times \prod_{k = 1}^\infty [N_k]^d. \]
%
The set $\Sigma^d$ is `almost' equal to $\RR^d$, which is why the sequence $\Sigma_k^d$ is useful for iterative discretization. More precisely, we have a map $\pi: \Sigma^d \to \RR^d$ given by $\pi(j) = \sum_{k = 0}^\infty j_kl_k$, and then $\#(\pi^{-1}(x)) = O_d(1)$ for each $x \in \RR^d$, so points are duplicated a neglible number of times\footnote{This is analogous to the fact that certain real numbers have two different decimal expansions.}. Given a sequence of sets $\{ S_k \}$, with $S_k \subset \Sigma_k^d$ for each $k$, and $(S_{k+1})^* \subset S_k$, we can define $E_k = \bigcup \{ Q_j : j \in S_k \}$, then $E_{k+1} \subset E_k$ for each $k$, and $E = \bigcap E_k$. If we form the inverse limit $S = \lim S_k \subset \Sigma^d$, then $\pi(S) = E$.

%One very useful property of the cubes in $\DQ^d$ is that cubes are either nested within one another, or \emph{almost disjoint} from one another, in the sense that only their boundaries intersect. Thus we can think of $\DQ^d$ as a forest under the partial ordering of inclusion, with the roots corresponding to the elements of $\DQ_0^d$. Each cube $Q \in \DQ^d_k$ has $N_{k+1}^d$ children. For each cube $Q \in \DQ_{k+1}^d$, we will let $Q^* \in \DQ_k^d$ denote it's parent, i.e. the unique cube in $\DQ_k^d$ with $Q \subset Q^*$. Similarily, given an index $I = (I_0, \dots, I_{k+1}) \in \Sigma_{k+1}$, we let $I^* = (I_0, \dots, I_k) \in \Sigma_k$, so that $Q_I \subset Q_{I^*}$.

The most common class of dyadic cubes in analysis is obtained from setting $N_k = 2$ for each $k$. We reserve a special notation for this class of dyadic cubes: the class of all such cubes is denoted by $\DD^d$, the generation $k$ cubes by $\DD^d_k$, and the resulting indices by $\{ \Delta_k^d \}$ and $\Delta^d$. These dyadic cubes have a constant branching factor, which makes them easy to analyze. But unfortunately, it is necessary in our methods to let the branching factor to tend to $\infty$. This is why we have to introduce the more general family of dyadic cubes given above.

%We can also give the spaces $\Sigma_k^d$ and $\Sigma^d$ a metric space structure, by defining, for $I \neq J$, $d(I,J) = l_k$, where $k$ is the smallest index such that $I_k \neq J_k$. Aside from the fact that some points in $\RR^d$ are duplicated in $\Sigma^d$, the main difference between the two spaces is that the balls in $\Sigma^d$ are discretized to the scales $\{ l_k \}$. From the point of view of geometric measure theory, the first point is neglible, but we shall find that discretization does matter. The degree to which the geometry of $\Sigma^d$ models the geometry of $\RR^d$ from the perspective of geometric measure theory is a key topic in this thesis. We shall find that the rate at which the lengths $l_k$ tend to zero will be a key factor in this relationship.

\section{Frostman Measures}

%\begin{example}
%	Let $s = 0$. Then $H_\delta^0(E)$ is the number of $\delta$ balls it takes to cover $E$, which tends to $\infty$ as $\delta \to 0$ unless $E$ is finite, and in the finite case, $H_\delta^0(E) \to \# E$. Thus $H^0$ is just the counting measure.
%\end{example}

%\begin{example}
%	Let $s = d$. If $E$ has Lebesgue measure zero, then for any $\varepsilon > 0$, there exists a sequence of balls $\{ B(x_k,r_k) \}$ covering $E$ with
	%
%	\[ \sum_{k = 1}^\infty r_k^d < \varepsilon^d. \]
	%
%	Then we know $r_k < \varepsilon$, so $H^s_\varepsilon(E) < \varepsilon^d$. Letting $\varepsilon \to 0$, we conclude $H^d(E) = 0$. Thus $H^d$ is absolutely continuous with respect to the Lebesgue measure. The measure $H^d$ is translation invariant, so $H^d$ is actually a constant multiple of the Lebesgue measure.
%\end{example}

It is often easy to upper bound Hausdorff dimension, but non-trivial to \emph{lower bound} the Hausdorff dimension of a given set. A key technique to finding a lower bound is \emph{Frostman's lemma}, which says that a set has large Hausdorff dimension if and only if it supports a probability measure which obeys a certain decay law on small sets. We say a measure $\mu$ is a \emph{Frostman measure} of dimension $s$ if it is non-zero, compactly supported, and for any cube $Q$, $\mu(Q) \lesssim l(Q)^s$. A simple approximation argument shows that this is equivalent to the bound $\mu(Q) \lesssim l(Q)^s$ for $Q \in \DD^d$. The proof of Frostman's lemma will utilize a technique often useful, known as the \emph{mass distribution principle}.

\begin{lemma}[Mass Distribution Principle]
	Let $\mu: \DQ^d \to [0,\infty)$ be a function such that for any $k$, and for any $Q_0 \in \DQ^d_k$,
	%
	\[ \sum \left\{ \mu(Q) : Q \in \DQ^d_{k+1}, Q^* = Q_0 \right\} = \mu(Q_0) \]
	%
	Then $\mu$ extends uniquely to a regular Borel measure on $\RR^d$.
\end{lemma}
\begin{proof}
	We can define a sequence of regular Borel measures $\{ \mu_k \}$ by setting, for each $f \in C_c(\RR^d)$,
	%
	\[ \int f d\mu_k = \sum \left\{ \mu(Q) \int_Q f : Q \in \DQ_k^d \right\} \]
	%
	Similarily, define a family of operators $\{ E_k \}$ on regular Borel measures by the formula
	%
	\[ \int f(x) dE_k(\nu) = \sum \left\{ \nu(Q) \int_Q f : Q \in \DQ_k^d \right\}. \]
	%
	The main condition of the theorem then says that $E_j(\mu_k) = \mu_j$ if $j \leq k$. Note that the operators $\{ E_k \}$ are each continuous with respect to the weak topology; If $\nu_i \to \nu$ weakly, then $\nu_i(Q) \to \nu(Q)$ for each fixed $Q \in \DQ_k^d$. Thus if $f \in C_c(\RR^d)$, then the support of $f$ intersects only finitely many cubes in $\DQ_k^d$, and this implies
	%
	\[ \int f(x) dE_k(\nu_i) = \sum \nu_i(Q) \int_Q f\; dx \to \sum \nu(Q) \int_Q f\; dx = \int f dE_k(\nu). \]
	%
	Now fix an interval $Q \in \DQ_1^d$. The measures $\{ \mu_k|_Q \}$, restricted to $Q$, all have total mass $\mu(Q)$. Thus the Banach-Alaoglu theorem implies that is a subsequence $\mu_{k_i}|_Q$ converging weakly on $Q$ to some measure $\mu_Q$. By continuity,
	%
	\[ E_j(\mu_Q) = \lim E_j(\mu_{k_i}|_Q) = \lim E_j(\mu_{k_i})|_Q = \mu_j|_Q. \]
	%
	This means precisely that $\mu_Q$ is the extension of $\mu$ to a Borel measure on $Q$. If we patch together the measures $\mu_Q$ over all choices of $Q \in \DQ_0^d$, we obtain a measure extending $\mu$ on all of $\RR^d$. The uniqueness of the extension is guaranteed by the fact that the intervals upon which $\mu$ are defined generate the entire Borel sigma algebra.
\end{proof}

%\begin{lemma}
%	let $\mu^+$ be a function from $\B$ to $[0,\infty)$ such that for any $I \in \B(1/M^k,\RR^d)$,
	%
%	\[ \sum \left\{ \mu^+(J) :J \in \B(1/M^{k+1},I) \right\} \leq \mu^+(I) \]
	%
%	Assume there exists $c > 0$ such that for all $k$,
	%
%	\[ \sum \left\{ \mu^+(I) : I \in \B(1/M^k,I) \right\} \geq c \]
	%
%	and
	%
%	\[ \sum \left\{ \mu^+(I) : I \in \B(1,I) \right\} < \infty \]	
	%
%	Then there exists a non-zero Borel measure $\mu$ such that $\mu(I) \leq \mu^+(I)$ for $I \in \B$.
%\end{lemma}
%\begin{proof}
%	As in the last lemma, define the operators $E_k$ and the measures $\mu_k$. By weak compactness, a subsequence of these measures converge weakly to some measure $\mu$, and $E_k(\mu) = \lim E_k(\mu_{j_k}) \leq \mu_k$. The measure $\mu$ is nonzero, since $\| \mu_{j_k} \| \geq c$ for each $k$, and so $\| \mu \| \geq c$.
%\end{proof}

\begin{lemma}[Frostman's Lemma]
	If $E$ is Borel, $H^s(E) > 0$ if and only if there exists an $s$ dimensional Frostman measure supported on $E$.
\end{lemma}
\begin{proof}
	Suppose that $\mu$ is $s$ dimensional and supported on $E$. If $H^s(F) = 0$, then for $\varepsilon > 0$ there is a sequence of cubes $\{ Q_k \}$ with $\sum_{k = 1}^\infty l(Q_k)^s \leq \varepsilon$. But then
	%
	\[ \mu(F) \leq \mu \left( \bigcup_{k = 1}^\infty Q_k \right) \leq \sum_{k = 1}^\infty \mu(Q_k) \lesssim \sum_{k = 1}^\infty l(Q_k)^s \leq \varepsilon. \]
	%
	Taking $\varepsilon \to 0$, we conclude $\mu(F) = 0$. Thus $\mu$ is absolutely continuous with respect to $H^s$. But since $\mu(E) > 0$, this means that $H^s(E) > 0$.

	Conversely, suppose $H^s(E) > 0$. Then by translating, we may assume that $H^s(E \cap [0,1]^d) > 0$, and so without loss of generality we may assume $E \subset [0,1]^d$. Fix $m$, and for each $Q \in \DQ_k^d$, define $\mu^+(Q) = H^s_m(E \cap Q)$. Then $\mu^+(Q) \leq 1/2^{ks}$, and $\mu^+$ is subadditive. We use it to recursively define a Frostman measure $\mu$, such that $\mu(Q) \leq \mu^+(Q)$ for each $Q \in \DQ_k^d$. We initially define $\mu$ by setting $\mu([0,1]^d) = \mu^+([0,1]^d)$. Given $Q \in \DQ_k^d$, we enumerate it's children as $Q_1, \dots, Q_M \in \DQ_{k+1}^d$. We then consider any values of $A_1, \dots, A_M$ such that
	%
	\[ A_1 + \dots + A_M = \mu(Q),\quad \text{and}\ A_k \leq \mu^+(Q_k)\ \text{for each $k$}. \]
	%
	This is feasible to do because $\mu^+(Q_1) + \dots + \mu^+(Q_M) \geq \mu^+(Q)$. We then define $\mu(Q_k) = A_k$ for each $k$. The recursive constraint is satisfied, so $\mu$ is well defined. The mass distribution principle then implies that $\mu$ extends to a full measure, which satisfies $\mu(Q) \leq \mu^+(Q) \leq 1/2^{ks}$ for each $Q \in \DQ_k^d$, so it is a Frostman measure with dimension $s$.
\end{proof}

Frostman's lemma implies that to study the Hausdorff dimension of the set, it suffices to understand the class of measures which can be supported on that set. The next section establishes techniques for understanding the properties of these measures.




\section{Dyadic Fractional Dimension}

We wish to establish results about fractional dimension `dyadically'. We begin with Minkowski dimension. If we are constructing a set $E \subset \RR^d$ as $\pi \left( \lim S_k \right)$, where $\{ S_k \}$ is a constructing sequence, it is often easy to establish estimates on $N(l,E)$ for $l = l_k$ simply by counting the cardinality of $S_k$, as the next lemma indicates. Provided that $S_{k+1}^* = S_k$, we have $\#(\DQ_k^d(E)) = \#(S_k)$ for each $k$.

\begin{lemma} \label{comparableCovers}
	For any set $E$, $\#(\DQ_k^d(E)) \sim_d N(l_k,E)$.
\end{lemma}
\begin{proof}
	Since $\DQ_k^d(E)$ is a cover of $E$ by sidelength $l_k$ cubes, minimality shows $N(l_k,E) \leq \#(\DQ_k^d(E))$. On the other hand, given \emph{any} cover of $E$ by sidelength $l_k$ cubes, we can replace each cube in the cover by at most $2^d$ cubes in $\DQ_k^d$, so $N(l_k,E) \leq 2^d \#(\DQ_k^d(E))$.
\end{proof}

Thus it is natural to ask whether it is true that for any set $E$,
%
\begin{equation} \label{definingSequence}
	\begin{aligned}
		\lowminkdim(E) &= \liminf_{k \to \infty} \frac{\log \left[ \#(\DQ_k^d(E)) \right]}{\log(1/l_k)}\\
		&\text{and}\\
		\upminkdim(E) &= \limsup_{k \to \infty} \frac{\log \left[ \#(\DQ_k^d(E)) \right]}{\log(1/l_k)}.
	\end{aligned}
\end{equation}
%
The answer depends on the choice of branching factors $\{ N_k \}$. If \eqref{definingSequence} holds for all sets $E$, we refer to the sequence $\{ N_k \}$ as a \emph{defining sequence} for the Minkowski dimension.

%Subsets of $\Sigma^d$ can also be assigned a Minkowski dimension. We define
%
%\[ \lowminkdim(E) = \liminf_{k \to \infty} \frac{\log(\#(\sigma_k^d(E)))}{\log(1/l_k)}\quad\text{and}\quad\upminkdim(E) = \limsup_{k \to \infty} \frac{\log(\#(\sigma_k^d(E)))}{\log(1/l_k)}. \]
%
%This makes sense, because $\Sigma^d$ only really has `balls' of radius $\{ l_k \}$, for each $k$, and \emph{any} cover of $E$ by balls of radius $l_k$ contains $\sigma_k^d(E)$. In order 
%we have $N(E,l_k) = \# (\Sigma_k^d(E))$, since \emph{any} cover of $E$ by balls of radius $l_k$

\begin{lemma} \label{definingsequenceminkowski}
	If, for all $\varepsilon > 0$, $N_k \leq (N_1 \dots N_{k-1})^\varepsilon$ for large enough $k$, then $\{ N_k \}$ is a defining sequence for the Minkowski dimension.
\end{lemma}
\begin{proof}
	Fix a length $l$, and find $k$ with $l_{k+1} \leq l \leq l_k$. Then applying Lemma \ref{comparableCovers} shows
	%
	\[ N(l,E) \leq N(l_{k+1},E) \lesssim_d \#(\DQ_{k+1}^d(E)) \]
	%
	and
	%
	\[ N(l,E) \geq N(l_k,E) \gtrsim_d \#(\DQ_k^d(E)). \]
	%
	Thus
	%
	\[ \frac{\log \left[ N(l,E) \right]}{\log(1/l)} \leq \left[ \frac{\log(1/l_{k+1})}{\log(1/l_k)} \right] \frac{\log \left[ \#(\DQ_{k+1}^d(E)) \right]}{\log(1/l_{k+1})} + O_d(1/k) \]
	%
	and
	%
	\[ \frac{\log \left[ N(l,E) \right]}{\log(1/l)} \geq \left[ \frac{\log(1/l_k)}{\log(1/l_{k+1})} \right] \frac{\log \left[ \#(\DQ_k^d(E)) \right]}{\log(1/l_k)} + O_d(1/k). \]
	%
	Thus, provided that
	%
	\begin{equation} \label{equivalenceofscales}
		\frac{\log(1/l_{k+1})}{\log(1/l_k)} \to 1,
	\end{equation}
	%
	we conclude that $\{ l_k \}$ is a defining sequence for the Minkowski dimension. But $l_{k+1} = l_k/N_k$, and $l_k = 1/N_1 \dots N_{k-1}$, so \eqref{equivalenceofscales} is equivalent to the condition that
	%
	\[ \frac{\log(N_k)}{\log(N_1) + \dots + \log(N_{k-1})} \to 0, \]
	%
	and this is equivalent to the assumption of the theorem.
\end{proof}

Any constant branching factor results in a defining sequence for the Minkowski dimension. In particular, we can construct sets in $\DD^d$ without any problems occuring. But more importantly for our work, we can let the branching factors tend to $\infty$ surprisingly fast. If $N_k = 2^{\lfloor 2^{k \psi(k)} \rfloor}$, where $\psi(k)$ is any decreasing sequence of positive numbers tending to zero, but for which $k \psi(k) \to \infty$, then $\{ N_k \}$ is a defining sequence. To see this, we begin by noting that $\log N_k = 2^{k \psi(k)} + O(1)$. Thus
%
%
\begin{align*}
	\frac{\log(N_k)}{\log(N_1) + \dots + \log(N_{k-1})} &\lesssim \frac{2^{k \psi(k)}}{2^{\psi(1)} + 2^{2 \psi(2)} + \dots + 2^{(k-1) \psi(k-1)}}\\
	&\leq \frac{2^{k \psi(k)}}{2^{\psi(k-1)} + 2^{2 \psi(k-1)} + \dots 2^{(k-1) \psi(k-1)}}\\
	&= \frac{2^{k \psi(k)} ( 2^{\psi(k-1)} - 1 )}{2^{k \psi(k-1)} - 2^{\psi(k-1)}} \\
	&\lesssim 2^{k[\psi(k) - \psi(k-1)]} (2^{\psi(k-1)} - 1)\\
	&\leq 2^{\psi(k-1)} - 1 \to 0.
\end{align*}
%
We refer to any sequence $\{ l_k \}$ constructed by $\{ N_k \}$ satisfying the properties above as a \emph{subhyperdyadic} sequence. If a sequence is generated by branching factors of the form $2^{\lfloor 2^{ck} \rfloor}$ for a fixed $c > 0$, the lengths are referred to as \emph{hyperdyadic}. The next example shows that hyperdyadic sequences are essentially the `boundary' for defining sequences.

\begin{example}
	Construct a subset of $\RR$ as follows. Let $[N_k] = [K_k] \times [M_k]$. Define $S_0 = \{ 0 \}$. Then, given $S_k$, recursively define
	%
	\[ S_{k+1} = (j,1,m) : j \in S_k, m \in [M_k] \]
	%
	Then set $E = \pi(\lim S_k)$. If we set $E_k = \bigcup \{ Q_j : j \in S_k \}$, then $E_{k+1}$ can be constructed by dividing each sidelength $l_k$ dyadic interval in $E_k$ into $K_k$ intervals, and then keeping only the first interval in the set. Note that $\#(S_{k+1}) = M_k \cdot \#(S_k)$, and since $\#(S_0) = 1$, $\#(S_k) = M_1 \dots M_k$. Thus $N(l_k,E) \sim \#(S_k)$, so
	%
	\begin{align*}
		\frac{\log \left[ N(l_k,E) \right]}{\log(1/l_k)} &\sim \frac{\log \left[ \#(S_k) \right]}{\log(1/l_k)}\\
		&= \frac{\log(M_1) + \dots + \log(M_k)}{\log(N_1) + \dots + \log(N_k)}\\
		&= 1 - \frac{\log(K_1) + \dots + \log(K_k)}{\log(N_1) + \dots + \log(N_k)}.
	\end{align*}
	%
	On the other hand, if $r_k = l_k/K_{k+1}$, $N(r_k,E) \sim_d \#(S_k) = M_1 \dots M_k$, so
	%
	\begin{align*}
		\frac{\log \left[ N(r_k,E) \right]}{\log(1/r_k)} &\sim \frac{\log \left[ \#(S_k) \right]}{\log(1/r_k)}\\
		&= \frac{\log(M_1) + \dots + \log(M_k)}{\log(N_1) + \dots + \log(N_k) + \log(K_{k+1})}\\
		&= 1 - \frac{\log(K_1) + \dots + \log(K_{k+1})}{\log(N_1) + \dots + \log(N_k) + \log(K_{k+1})}.
	\end{align*}
	%
	Set $N_k = 2^{\lfloor 2^{ck} \rfloor}$, and $K_k = 2^{\lfloor c 2^{ck} \rfloor}$. Then
	%
	\[ \log(K_1) + \dots + \log(K_k) = O(k) + c \sum_{i = 1}^k 2^{ck} = O(k) + c \frac{2^{c(k+1)} - 2^c}{2^c - 1} \]
	%
	and
	%
	\[ \log(N_1) + \dots + \log(N_k) = O(k) + \sum_{i = 1}^k 2^{ck} = O(k) + \frac{2^{c(k+1)} - 2^c}{2^c - 1}. \]
	%
	Thus
	%
	\[ \frac{\log \left[ N(l_k,E) \right]}{\log(1/l_k)} \to 1 - c \quad \text{and} \quad \frac{\log \left[ N(r_k,E) \right]}{\log(1/l_k)} \to \frac{1 - c}{1 - c + c2^c}. \]
	In particular, for any $0 < c \leq 1$,
	%
	\[ \lowminkdim(E) \neq \liminf_{k \to \infty} \frac{\log \left[ N(l_k,E) \right]}{\log(1/l_k)}, \]
	%
	so no hyperdyadic sequence is a defining sequence.

%	\[ \lim_{k \to \infty} \frac{\log(K_1) + \dots + \log(K_k)}{2^{c(k+1)} - 2^c} \neq \lim_{k \to \infty} \frac{\log(K_1) + \dots + \log(K_{k+1})}{2^{c(k+1)} - 2^c} \frac{1}{1 + \log(K_{k+1}) (2^c - 1)/(2^{c(k+1)} - 2^c)} \]

%	First, assume $N_k/N_{k-1} \in \mathbf{Z}$ for each $k$, and for convenience, set $N_0 = 1$. Define $S_0 = \{ 0 \}$, and then given $S_k$, recursively define
	%
%	\[ S_{k+1} = \{ (j,1), \dots, (j,N_{k+1}/N_k) : j \in S_k \} \]
	%
%	We then set $E = \pi(\lim S_k)$. If we let $E_k = \bigcup \{ Q_j : j \in S_k \}$, then $E_{k+1}$ can be constructed by dividing each sidelength $l_k$ dyadic interval in $E_k$ into $N_{k+1}$ intervals, and selecting the initial $N_{k+1}/N_k$ intervals, which have total length $1/(N_1 \dots N_k^2)$. Then $\#(S_{k+1}) = (N_{k+1}/N_k) \#(S_k)$, and since $\#(S_0) = 1$, $\#(S_k) = N_k$. Thus
	%
%	\[ \frac{\log \left[ N(l_k,E) \right]}{\log(1/l_k)} \sim_d \frac{\log \left[ \# (S_k) \right]}{\log(1/l_k)} = \frac{\log(N_k)}{\log(N_1) + \dots + \log(N_k)}. \]
	%
%	On the other hand, if $r_k = 1/(N_1 \dots N_k^2) = l_k/N_k$, then $N(l,E) = \#(S_k) = N_k$, so
	%
%	\[ \frac{\log \left[ N(r_k,E) \right]}{\log(1/r_k)} \sim_d \frac{\log \left[ \#(S_k) \right]}{\log(1/r_k)} = \frac{\log(N_k)}{\log(N_1) + \dots + 2\log(N_k)}. \]
%	In any case to which Lemma \ref{definingsequenceminkowski} applies, both of these estimates converge to zero as $k \to \infty$, so that $E$ has Minkowski dimension zero. On the other hand, if $N_k = 2^{\lfloor \psi(k) 2^k \rfloor}$ where $\psi(k)$ is an increasing sequence tending to $\infty$, then
	%
%	\begin{align*}
%		\frac{2^{(k-1) \psi(k-1)}}{2^{\psi(1)} + 2^{2 \psi(2)} + \dots + 2^{(k-1) \psi(k-1)}} &\geq \frac{\left(2^{(k-1) \psi(k-1)} \right) \left( 2^{\psi(k-1)} - 1 \right)}{2^{k \psi(k-1)} - 2^{\psi(k-1)}} \sim 1.
%	\end{align*}
	%
%	Thus
	%
%	\[ \lim_{k \to \infty} \frac{\log \left[ N(l_k,E) \right]}{\log(1/l_k)} = 1. \]
	%
%	On the other hand,
	%
%	\[ \frac{\log \left[ N(r_k, E) \right]}{\log(1/r_k)} = \frac{\log(N_k)}{\log(N_1) + \dots + 2\log(N_k)} \leq 1/2. \]
	%
%	Thus we cannot possibly have
	%
%	\[ \lowminkdim(E) = \liminf_{k \to \infty} \frac{\log \left( N(l_k,E) \right)}{\log(1/l_k)}, \]
	%
	%
%	A simple calculation shows this result even fails if $N_k = 2^{\lfloor 2^{ck} \rfloor}$, where $c > 1$.
\end{example}

%In particular, we can define the Minkowski dimensions of $E \subset \Sigma$ as
%
%\[ \lowminkdim(E) = \liminf_{k \to \infty} \frac{\log(\#(\Sigma_k^d(E)))}{\log(1/l_k)}\quad\text{and}\quad\upminkdim(E) = \limsup_{k \to \infty} \frac{\log(\#(\Sigma_k^d(E)))}{\log(1/l_k)}. \]
%
%Similarily, the Hausdorff measures $H^s$ are obtained by setting
%
% TODO: Fix this
%\[ H^s(E) = \left\{ \sum_m l_{k_m} : Q_m \in \Sigma_{k_m}^d\ \text{for each $k$}, \text{For any} \right\} \]
%
%and define the Hausdorff dimension correspondingly. A natural question is whether $\dim(\pi(E)) = \dim(E)$ for the various fractal dimensions we consider in this thesis. This is addressed in the next section.

We now move on to calculating Hausdorff dimension dyadically. The natural quantity to consider here is the measure $H^s_{\DQ}(E) = \lim H^s_{\DQ,m}(E)$, where
%
\[ H^s_{\DQ,m}(E) = \inf \left\{ \sum_k l(Q_k)^s : E \subset \bigcup_k^\infty Q_k, Q_k \in \bigcup_{i \geq m} \DQ_i\ \text{for each $k$} \right\}. \]
%
A similar argument to the standard Hausdorff measures shows there is a unique $s_0$ such that $H^s(E) = \infty$ for $s < s_0$, and $H^s(E) = 0$ for $s > s_0$. It is obvious that $H^s_{\DQ}(E) \geq H^s(E)$ for any set $E$, so we certainly have $s_0 \geq \hausdim(E)$. But what condition on $\{ N_k \}$ guarantees this quantity is equal to $\hausdim(E)$?

\begin{lemma} \label{lemma51464}
	If, for any $\varepsilon > 0$, $N_{k+1} \leq (N_1 \dots N_k)^\varepsilon$ for sufficiently large $k$, then $H^s_{\DQ}(E) \lesssim_d H^{s-\varepsilon}(E)$ for each $\varepsilon > 0$.
\end{lemma}
\begin{proof}
	Fix $\varepsilon > 0$. Let $E \subset Q_k$, where $l(Q_k) \leq l_m$ for each $k$. Then for each $k$, we can find $i_k$ such that $l_{i_k+1} \leq l(Q_k) \leq l_{i_k}$. Then $Q_k$ is covered by $O_d(1)$ elements of $\DQ_{i_k}^d$, and
	%
	\begin{equation} \label{equation824} H^s_{\DQ,m}(E) \lesssim_d \sum l_{i_k}^s \leq \sum (l_{i_k}/l_{i_k+1})^s l(Q_k)^s \leq \sum \left( l_{i_k}/l_{i_{k+1}} \right)^s l_{i_k}^\varepsilon l(Q_k)^{s - \varepsilon} \end{equation}
	%
	By assumption, if $m$ is large enough, then
	%
	\begin{equation} \label{equation992352}
		l_{i_k+1} = l_{i_k}/N_{i_k+1} \gtrsim l_{i_k}^{1 + \varepsilon/s}.
	\end{equation}
	%
	Putting \eqref{equation824} and \eqref{equation992352} together, we conclude that $H^s_{\DQ,m}(E) \lesssim_d \sum l(Q_k)^{s-\varepsilon}$, and since the cover $\{ Q_k \}$ was arbitrary, we conclude that $H^s_{\DQ}(E) \lesssim_d H^{s-\varepsilon}(E)$ for each $\varepsilon > 0$.
\end{proof}

\begin{remark}
	The same example as for Minkowski dimension shows that bounds at hyperdyadic sequences do not preserve Hausdorff dimension.
\end{remark}

Returning to the situation before this lemma, if $\{ N_k \}$ is a defining sequence, and $H^s(E) = 0$, then $H^{s+\varepsilon}_{\DQ}(E) \lesssim_d H^s(E) = 0$ for each $\varepsilon > 0$. This is sufficient to guarantee that $s_0 \leq s$, and since $s$ was arbitrary, we conclude $s_0 \leq \hausdim(E)$. Thus we actually have $s_0 = \hausdim(E)$ under the assumptions of Lemma \ref{lemma51464}.








\section{Dimensions of Cantor-Type Sets}

Given a closed set $E$, we can associate the sequence of discretizations $\{ E_k = \Sigma_k^d \}$, where $E_k$ is the union of cubes in $\DQ^d_k(E)$. Then the sets $E_k$ are decreasing, with $E = \bigcap E_k$. Since we construct sets in this thesis by iterative processes, it is easy to obtain combinatorial bounds on the discretizations $\{ E_k \}$. Furthermore, we can associate a natural measure with this discretization, and in this section, we show how such combinatorial bounds can be used to show this measure is a Frostman measure.

Given the Cantor-type decomposition, we can recursively define a function $\mu$ by setting $\mu([0,1]^d) = 1$, and, for each $Q \in \DQ_{k+1}^d$,
%
\[ \mu(Q) = \frac{\mu(Q^*)}{\# (\DQ_k^d(E_k \cap Q^*))}\quad \text{if}\ Q \in \DQ_{k+1}^d(E_k),\quad \text{else} \quad \mu(Q) = 0. \]
%
The mass distribution principle then extends $\mu$ to a probability measure. We will refer to this as the \emph{canonical measure} associated with the Cantor-type decomposition $\{ E_k \}$. Simple combinatorial reasoning related to the construction of the set $E$ often enables one to establish bounds of the form $\mu(Q) \lesssim l(Q)^s$ for $Q \in \DQ_k^d$. If $\DQ^d = \DD^d$, then this is sufficient to show $\mu$ is a measure of dimension $s$. But if we allow the lengths to decrease at a much faster rate, allowing the branching factor to become subhyperdyadic, then we can actually still prove that $\mu$ is a Frostman measure of dimension $s - \varepsilon$ for all $\varepsilon > 0$, which is still good enough to show $E$ has dimension $s$.

\begin{theorem} \label{easyCoverTheorem}
	If, for all $\varepsilon > 0$,
	%
	\begin{equation} \label{growthEquation} N_k \leq (N_1 \dots N_{k-1})^\varepsilon \end{equation}
	%
	for large enough $k$, and if $\mu$ is a Borel measure such that $\mu(Q) \lesssim l(Q)^s$ for each $Q \in \DQ_k^d$, then $\mu$ is a Frostman measure of dimension $s - \varepsilon$ for each $\varepsilon > 0$.
\end{theorem}
\begin{proof}
	Given $Q \in \DD^d_m$, find $k$ such that $l_{k+1} \leq 1/2^m \leq l_k$. Then $Q$ is covered by $O_d(1)$ cubes in $\DQ^d_k$, which shows
	%
	\[ \mu(Q) \lesssim_d l_k^s = [(l_k/l)^s l^\varepsilon ] l^{s - \varepsilon} \leq [l_k^{s + \varepsilon} / l_{k+1}^s] l^{s - \varepsilon} = [N_k^s l_k^\varepsilon] l^{s-\varepsilon}. \]
	%
	Thus provided $N_k^s \lesssim_\varepsilon 1/l_k^\varepsilon$ for all $\varepsilon$, the proof is complete. But $l_k = 1/(N_1 \dots N_{k-1})$, so this is guaranteed by \eqref{growthEquation}.
\end{proof}

The same example as for Minkowski dimension shows that hyperdyadic sequences, fail to help us in this situation. If we are to use a faster decreasing sequence of lengths $\{ l_k \}$, we therefore must have to exploit some extra property of the decomposition which is not always present. Here, we rely on some kind of \emph{uniform mass distribution} between scales. Given the uniformity assumption, the lengths can decrease as fast as desired.

%Our final method for interpolating requires extra knowledge of the dissection process, but enables us to choose the $l_k$ arbitrarily rapidly. The idea behind this is that there is an additional sequence of lengths $r_k$ with $l_k \leq r_k \leq l_{k-1}$. The difference between $r_k$ and $l_{k-1}$ is allowed to be arbitrary, but the decay rate between $l_k$ and $r_k$ is of polynomial-type, which enables us to use the covering methods of the previous section. In addition, we rely on a `uniform mass bound' between $r_k$ and $l_k$ to cover the remaining classes of intervals. Because we can take $r_k$ arbitrarily large relative to $l_k$, this renders any constants that occur in the construction to become immediately negligible. For two quantities $A$ and $B$, we will let $A \precsim_k B$ stand for an inequality with a hidden constant depending only on parameters with index smaller than $k$, i.e. $A \leq C(l_1, \dots, l_k, r_1,\dots,r_k) B$ for some constant $C(l_1, \dots, l_k, r_1, \dots, r_k)$ depending only on parameters with indices up to $k$.

\section{Beyond Hyperdyadics}

\begin{theorem} \label{uniformHausdorffResult}
    Let $\mu$ be a measure supported on a set $E$, and $\{ l_k \}$ and $\{ r_k \}$ two decreasing sequences of lengths, with $l_{k+1} \leq r_{k+1} \leq l_k$ for each $k$. Write $l_k = 2^{-\psi(k)}$ and $r_k = 2^{-\eta(k)}$. Suppose that
    %
    \begin{enumerate}
    	\item \label{discreteBound} \emph{(Discrete Bound)}: For any $I \in \B(l_k,\RR^d)$, $\mu(I) \lesssim l_k^s$.
    	\item \label{controlledScale} \emph{(Hyperdyadic Scale Change)}: $\eta(k)/\psi(k) \to 1$.
    	\item \label{uniformDist} \emph{(Uniform Mass Distribution)}: For any $I \in \B(l_k,\RR^d)$, and $J \in \B(r_{k+1},I)$,
    	%
    	\[ \mu(J) \lesssim (r_{k+1}/l_k)^d \mu(I). \]
    \end{enumerate}
	%
	Then $\mu$ is a Frostman measure of dimension $s-\varepsilon$ for each $\varepsilon > 0$.
\end{theorem}
\begin{proof}
	Suppose an interval $I$ has length $l$, and we can find $k$ with $r_{k+1} \leq l \leq l_k$. Then we can cover $I$ by at most $(l/r_{k+1})^d$ cubes in $\B(r_{k+1},\RR^d)$. By Properties \ref{discreteBound} and \ref{uniformDist}, each of these cubes has measure at most $(r_{k+1}/l_k)^d l_k^s$, so we obtain that for any $\varepsilon > 0$,
    %
    \[ \mu(I) \lesssim (l/r_{k+1})^d (r_{k+1}/l_k)^d l_k^s = l^d / l_k^{d-s} \lesssim l^s. \]
    %
    In particular, this implies $\mu(I) \lesssim r_k^s$ if $I \in \B(r_k,\RR^d)$. On the other hand, suppose there exists $k$ with $l_k \leq l \leq r_k$. Then we can cover $I$ by $O(1)$ cubes in $\B(r_k,\RR^d)$, so we find
    %
    \[ \mu(I) \lesssim r_k^s \leq [(r_k/l_k)^s l^\varepsilon] l^{s-\varepsilon}. \]
    %
    Property \ref{controlledScale} shows $\psi(k) - \eta(k) = o(\eta(k))$, so
    %
    \[ (r_k/l_k)^s l^\varepsilon \leq r_k^{s+\varepsilon} / l_k^s \leq 2^{s \psi(k) -(s+\varepsilon) \eta(k)} = 2^{(- \varepsilon + o(1)) \eta(k)} \lesssim_\varepsilon 1, \]
    %
    so $\mu(I) \lesssim_\varepsilon l^{s-\varepsilon}$ for each $\varepsilon > 0$. This covers all possible cases, so $\mu$ is a Frostman measure of dimension $s-\varepsilon$ for each $\varepsilon > 0$.
\end{proof}

\begin{remark}
	Let us dwell on the choice of scales. The sequence of scales $\{ l_k \}$ can decrease to zero as rapidly as desired. On the other hand, the sequence $\{ l_k/r_k \}$ must be a hyperdyadic sequence. One can visualize this as allowing the gap between $l_k$ and $r_{k+1}$ to be as large as desired, between which $\mu$ has effectively a full dimensional behaviour \footnote{Technically, we only obtain an explicit bound $\mu(I) \lesssim l^{1 - \varepsilon}$ for all $I \in \B(l,E)$ for all lengths $l$ for which there is $k$ with $r_{k+1} \leq l \leq l_k$ only if the lengths decrease rapidly enough and $l$ is significantly closer to $r_{k+1}$ rather than $l_k$.}, but placing a bound on the gap between $r_{k+1}$ and $l_{k+1}$, where the measure behaves $s$ dimensionally. This is visualized in the diagram below.
	% TODO: Draw diagram.
\end{remark}

%\begin{remark}
%    The condition $\mu_\beta(J) \lesssim_{N-1} (r_N/l_N) \mu_\beta(I)$ essentially means that the probability mass on a length $l_N$ interval $I$ is uniformly distributed over the length $r_N$ intervals it contains. This is what enables us to remove the discussion of the growth of the sequence $\beta$ over time from discussion.
%\end{remark}

%Since the construction is obtained as a limit of intervals, it is often possible to construct such a $\mu$ by the {\it mass distribution principle}. That is, we let $\mu$ denote the weak limit of the probability masses $\mu_n$, where $\mu_0$ is a uniform distribution over $\mu_0$, and $\mu_{n+1}$ is obtained from $\mu_n$ by distributing the mass $\mu_n(I)$ of each length $l_n$ interval $I$ contained in $X_n$ over the portion of $I$ that remains in $X_{n+1}$. The cumulative distribution functions of the $\mu_n$ uniformly converge, hence the $\mu_n$ converge weakly to some $\mu$, which satisfy $\mu(I) = \mu_n(I)$ for each interval $I$ as above. Because of this discreteness, it is most easy to establish a bound $\mu(I) \lesssim l_n^\alpha$ when $I \subset X_n$ is a length $l_n$ interval. Since any interval $I$ of length $l_n$ is contained within at least two such intervals (or is contained in other length $l_n$ intervals that $\mu$ assigns no mass to), we have the general bound $\mu(I) \lesssim l_n^\alpha$ for all intervals $I$ of length $l_n$. Hausdorff dimension is a local property of a set\footnote{If we define $\dim_{\mathbf{H}}(x) = \lim_{r \downarrow 0} \dim_{\mathbf{H}}(B_r(x) \cap X)$ then $\dim_{\mathbf{H}}(X) = \sup_{x \in X} \dim_{\mathbf{H}}(x)$.}, so it is natural to expect that we can obtain a general bound $\mu(I) \lesssim_\alpha|I|^\alpha$ given that one has established precisely the same estimate, but restricted to intervals $I$ with $|I| = l_N$. This section concerns itself with ways that we can establish this general bound, and thus prove that $\dim_{\mathbf{H}}(X) \geq \alpha$.

%\section{BLAH}

%The collection $\DQ^d$ forms a \emph{tree} under the partial ordering induced by inclusion, with a branching factor of $H$. We let $\DB^d$ denote the set of all branches of the tree $\DQ^d$. For each branch $\mathfrak{b} = \{ Q_k : k \geq 0 \}$, the set $\bigcap_{k \geq 0} Q_k$ contains a unique point, which induces a function $\pi: \DB^d \to \RR^d$. This function is obviously surjective, but unfortunately not injective. Nonetheless, for each $x \in \RR^d$, $\pi^{-1}(x)$ contains $O_d(1)$ points. This means that from the point of view of geometric measure theory, the spaces $\DB^d$ and $\RR^d$ are isomorphic\footnote{Formally, we can define a topology on $\DB^d$ as a subset of $\prod_{k \geq 0} \DQ_k^d$, and $\pi$ is continuous with respect to this topology. We can also define the Hausdorff measures $H^s$ on $\DB^d$ as limits of $H^s_m$, where
%
%\[ H^s_m(E) = \inf \left\{ \sum_{i = 1}^\infty l(Q_i)^s : Q_i \in \DQ_{k_i}^d, k_i \geq m, E \subset \bigcup \{ \mathfrak{b} \in \DB^d : \mathfrak{b}_k = Q \} \right\} \]
%
%Then for each value $s > 0$ the map $\pi$ is an isomorphism of $(\DB^d, H^s)$ and $(\RR^d, H^s)$.}. In particular, given any configuration $\C$ on $\RR^d$, we can define a configuration $\pi^{-1}(\C)$ on $\DB^d$ as
%
%\[ \pi^{-1}(\C) = \{ (\mathfrak{b_1}, \dots, \mathfrak{b}_n) : (\pi(\mathfrak{b_1}), \dots, \pi(\mathfrak{b_n})) \in \C \}. \]
%
%If we can find an $s$ dimensional set $E \subset \DB^d$ avoiding $\pi^{-1}(\C)$, then $\pi(E)$ avoids $\C$ and is $s$ dimensinoal. Conversely, if $E \subset \RR^d$ avoids $\C$ and is $s$ dimensional, then $\pi^{-1}(E)$ avoids $\pi^{-1}(\C)$ and is $s$ dimensional.

%The Dyadic model is useful, but if the lengths decrease too fast, the model fails to reflect the geometry of $\RR^n$ at all scales.

\section{Extras: Should we Include?}
















\begin{lemma}
	Let $E$ be a set, and $\mu$ a Borel probability measure supported on $E$. Suppose that for any $\varepsilon$, there exists a constant $c_\varepsilon$ such that if $\B(1/M^k,E) \leq c_\varepsilon M^{k(s-\varepsilon)}$, then $\mu(E) \lesssim 1/k^2$. then $E$ has Hausdorff dimension $s$.
\end{lemma}
\begin{proof}
	Suppose $H^{s-\varepsilon}(E) = 0$. Then for any $N$ there exists a cover of $E$ by cubes $\{ I_k \}$, with lengths $\{ l_k \}$ such that $I_k \in \B(l_k,\RR^d)$, $l_k \leq 1/M^N$ for all $k$, and $\sum l_k^{s - \varepsilon} \leq c_\varepsilon$. For each $m$, let $A_m = \# \{ k : 1/M^{m+1} \leq l_k \leq 1/M^m \}$. Then
	%
	\[ \sum_{m = N}^\infty A_m M^{-(m+1)(s - \varepsilon)} \leq \sum_{k = 1}^\infty l_k^{s - \varepsilon} \leq c_\varepsilon \]
	%
	Thus $A_m \leq c_\varepsilon M^{(m+1)(s - \varepsilon)}$. This means that if $E_m$ is formed from the union of all intervals $I_k$ with $1/M^{m+1} \leq l_k \leq 1/M^m$, then $\# \B_s(E_k) \leq c_\varepsilon M^{(m+1)(s-\varepsilon)}$. Thus $\mu(E_m) \lesssim 1/k^2$, so
	%
	\[ \mu(E) \leq \sum_{m = N}^\infty \mu(E_m) \lesssim \sum_{m = N}^\infty 1/k^2 \]
	%
	as $N \to \infty$, we conclude $\mu(E) = 0$, which is impossible. Thus $H^{s-\varepsilon}(E) > 0$ for all $\varepsilon$, so $\hausdim(E) \geq s$.
\end{proof}

The hypothesis of Lemma 7 is certainly satisfied if $\mu(I) \lesssim_\varepsilon l_k^{s - \varepsilon}/k^2$ for each $I \in \B(l_k)$, where $l_k = 1/M^k$. Thus establishing a Frostman-type bound at a sequence of dyadic type scales is enough to obtain a dimensional result for $E$. The advantage of this proof is that we can continue the argument to give results when the sequence of scales decreases much faster than scales of dyadic type.

\begin{theorem}
	Let $E$ be a set, and $\mu$ a Borel probability measure supported on $E$. Suppose that for any $\varepsilon$, there exists a constant $c_\varepsilon$ such that if $\B(1/M^k,E') \leq c_\varepsilon M^{k(s-\varepsilon)}$ for any $E' \subset E$, then $\mu(E') \lesssim 1/k^2$. Then $E$ has Hausdorff dimension $s$.
\end{theorem}
\begin{proof}
	As before, if $H^{s-\varepsilon}(E) = 0$, consider a covering by $\{ I_k \}$ with parameters $\{ l_k \}$. Fix $\alpha > 1$, and consider
	%
	\[ A_m = \# \{ k : 1/M^{\alpha^{m + 1}} \leq l_k \leq 1/M^{\alpha^m} \} \]
	%
	Then
	%
	\[ \sum A_m/M^{(s - \varepsilon) \alpha^{m+1}} \leq \sum l_k^{s - \varepsilon} \leq c_\varepsilon \]
	%
	so $A_m \leq c_\varepsilon M^{(s - \varepsilon) \alpha^{m+1}}$. Thus if we define $E_m$ as in the last proof, then $\mu(E_m) \lesssim 1/k^2$.
\end{proof}

The requirement of this lemma is satisfied if we are able to prove $\mu(I) \lesssim_\varepsilon l_k^{s - \varepsilon}/k^2$, where $l_k = M^{- \alpha^k}$, for all $\varepsilon > 0$. These are \emph{hyperdyadic} numbers.

\begin{proof}
	Suppose that $H^{s-\varepsilon}(E) = 0$. Then for any $M$ and $c_\varepsilon$, $E$ is covered by cubes $\{ I_k \}$ with sidelengths $\{ r_k \}$ such that $I_k \in \B(r_k,\RR^d)$, $r_k \leq l_M$ for each $k$, and $\sum r_k^{s - \varepsilon} \leq c_\varepsilon$. For each $k$, let $A_m = \# \{ k: l_{m+1} \leq r_k \leq l_m \}$. Then
	%
	\[ \sum_{m = M}^\infty A_m l_{m+1}^{s - \varepsilon} \leq \sum_{k = 1}^\infty r_k^{s - \varepsilon} \leq c_\varepsilon \]
	%
	so $A_m \leq c_\varepsilon / l_{m+1}^{s-\varepsilon}$. But if we can establish an estimate $\mu(I) \lesssim_\varepsilon B_m l_m^{s - \varepsilon}$, then 
	%
	\[ \mu(E) \lesssim_\varepsilon \sum_{m = M}^\infty A_m (B_m l_m^{s - \varepsilon}) \leq \sum_{m = M}^\infty c_\varepsilon B_m (l_m/l_{m+1})^{s - \varepsilon} \]
\end{proof}

\section{Hyperdyadic Covers}

\begin{theorem}
	If $\inf_{\delta > 0} H^s_\delta(E) = 0$, then $\hausdim(E) \leq s$.
\end{theorem}
\begin{proof}
	Suppose that $H^s_\delta(E) \leq \varepsilon$. Then there is a sequence of cubes $\{ I_k \}$ with parameters $\{ l_k \}$ such that $I_k \in \B(l_k)$, $l_k \leq \delta$ for all $k$, and $\sum_{k = 1}^\infty l_k^s \leq 2\varepsilon$. Then $l_k \leq (2\varepsilon)^{1/s}$ for all $k$, so we can actually take $\delta \to 0$?
\end{proof}

\begin{theorem}
	If $X$ is strongly covered by $\{ X_k \}$, and there is $\delta$ such that
	%
	\[ \sum_{k = 1}^\infty H^s_\delta(X_k) < \infty \]
	%
	then $\hausdim(X) \leq s$.
\end{theorem}
\begin{proof}
	By subadditivity, for any $N$,
	%
	\[ H^s_\delta(X) \leq \sum_{k = N}^\infty H^s_\delta(X_k) \]
	%
	and as $N \to \infty$, we conclude $H^s_\delta(X) = 0$.
\end{proof}

For a fixed $0 < \varepsilon \ll 1$, a \emph{hyperdyadic number} is a number of the form $h_k = 2^{-\lfloor (1 + \varepsilon)^k \rfloor}$ for some $k \geq 0$. A set $E$ is \emph{$\delta$ discretized} if it is the union of balls, each with radius between $c_\varepsilon \delta^{1 + C\varepsilon}$ and $C_\varepsilon \delta^{1-C\varepsilon}$. A set $E$ is a $(\delta,s)_d$ set if it is bounded, $\delta$ discretized, and for all $\delta \leq r \leq 1$, $|E \cap B(x,r)| \leq C_\varepsilon \delta^{n-s-C\varepsilon} r^s$

\begin{lemma}
	Let $0 < s < d$, and let $E$ be a compact subset of $\RR^n$.
	%
	\begin{itemize}
		\item If $\dim(E) \leq s$, for each $k$, we can associate a $(h_k,s)_d$ set $E_k$ such that $E$ is strongly covered by the $E_k$.

		\item If $C$ is sufficiently large, and there is a $(h_k, s - C\varepsilon)_n$ set $E_k$ strongly covering $E$, then $\dim(E) \leq s$.
	\end{itemize}
\end{lemma}
\begin{proof}
	We first prove the latter claim, assuming without loss of generality that $E$ is contained in the unit ball. Suppose $E$ is strongly covered by the $E_k$. If $E_k$ is a $(h_k,s - C\varepsilon)_d$ set, then
	%
	\[ |E_k| \leq C_\varepsilon \delta^{n-(s-C\varepsilon)-C\varepsilon} = C_\varepsilon \delta^{n-s}. \]
	%
	\[ H^s_{C_\varepsilon h_k^{1 - C\varepsilon}} \]
\end{proof}










\section{Hyperdyadic Covers Take 3}

Fix two parameters $\delta > 0$ and $\varepsilon > 0$. Given two numbers $A = A_{\delta \varepsilon}$ and $B = B_{\delta \varepsilon}$, we say $A \lessapprox B$ if there exists constants $C_\varepsilon$, and $C$ such that $A \leq C_\varepsilon \delta^{-C\varepsilon} B$. We say $A \approx B$ if $A \lessapprox B$ and $B \lessapprox A$. We say a set $E$ is \emph{$\delta$ discretized} if it is a union of dyadic cubes with sidelength $\approx \delta$. We say a set $E$ is a \emph{$(\delta,\alpha)$ set} if it is $\delta$ discretized, and for any dyadic cube $I$ with $\delta \leq l(I) \leq 1$, $|E \cap I| \lessapprox \delta^{d-\alpha} l(I)^\alpha$. Thus $E$ is \emph{roughly} a $\delta$ thickening of an $\alpha$ dimensional set. A set $E$ is \emph{strongly covered} by a family of sets $\{ U_i \}$ if $E \subset \limsup_{i \to \infty} U_i$. We consider a fixed hyperdyadic sequence $l_k = 2^{- \lfloor (1 + \varepsilon)^k \rfloor}$.

\begin{lemma}
	If $C$ is sufficiently large, and for each $\varepsilon$, there is a $(\delta, \alpha - C \varepsilon)$ set $X_\delta$ for each hyperdyadic $\delta$ such that the $X_\delta$ strongly cover $X$, then $\dim(X) \leq \alpha$.
\end{lemma}
\begin{proof}
	Let $X_\delta = \bigcup I_i$, where $\{ I_i \}$ are disjoint dyadic cubes such that $l(I_i) \approx \delta$, and with $|X_\delta| \lessapprox \delta^{d-\alpha + C\varepsilon}$. Then
	%
	\[ |X_\delta(\delta/2)| \leq \sum (l(I_i) + \delta/2)^d \lessapprox \sum l(I_i)^d = |X_\delta| \lessapprox \delta^{d - \alpha + C\varepsilon}. \]
	%
	A volumetric argument then guarantees that $N(X_\delta,\delta) \lessapprox \delta^{-\alpha + C\varepsilon}$, and so
	%
	\[ H^\alpha_\infty(X_\delta) \leq N(X_\delta,\delta) \delta^\alpha \lessapprox \delta^{C\varepsilon}. \]
	%
	Thus there is $C_\varepsilon$ and $C_0$ such that $H^\alpha_\delta(X_\delta) \leq C_\varepsilon \delta^{(C - C_0) \varepsilon}$. Since $C_0$ does not depend on $C$, if we set $C > C_0$, then
	%
	\[ \sum_{i = 1}^\infty H^\alpha_\infty(X_\delta) < \infty, \]
	%
	and so $H^\alpha_\infty(X) = 0$.
\end{proof}

\begin{theorem}
	Suppose $X$ is a set with $\dim(X) \leq \alpha$. Then there exists a strong cover of $X$ by sets $X_\delta$, where $X_k$ is a union of $O((1 + \varepsilon)^{2k} l_k^{-\alpha})$ hyperdyadic cubes of sidelength $l_k$.
\end{theorem}

\begin{theorem}
	If $\dim(X) \leq \alpha$, then for each hyperdyadic $\delta$, we can associate a $(\delta,\alpha)$ set $X_\delta$ such that $\{ X_\delta \}$ strongly covers $X$.
\end{theorem}
\begin{proof}
	For each hyperdyadic number $l_k$, we can find a collection of dyadic cubes $\{ I_{k,i} \}$ covering $X$ with $l(I_{k,i}) \leq l_k$ for all $i$, and
	%
	\[ \sum_{i = 1}^\infty l(I_{k,i})^{\alpha + C\varepsilon} \lesssim 1. \]
	%
	For each $k$ and $i$, we can find $j_{k,i}$ such that $l_{j_{k,i} + 1} \leq l(I_{k,i}) \leq l_{j_{k,i}}$. Note that $l_{j_{k,i} + 1} \lesssim l_{j_{k,i}}^{1 + \varepsilon}$, and so
	%
	\begin{align*}
		\sum_{i = 1}^\infty \# \B(l_{j_{k,i}}, I_{k,i}) \cdot l_{j_{k,i}}^{\alpha + C\varepsilon} &\lesssim \sum_{i = 1}^\infty l(I_{k,i})^{\alpha + C \varepsilon} (l_{j_{k,i}} / l(I_{k,i}))^{\alpha + C\varepsilon}\\
		&\lesssim \sum_{i = 1}^\infty l(I_{k,i})^{\alpha + C\varepsilon} l_{j_{k,i}}^{-\varepsilon(\alpha + C\varepsilon)}\\
		&\lesssim \sum_{i = 1}^\infty l(I_{k,i})^{\alpha + (C - \alpha + \varepsilon) \varepsilon}
	\end{align*}
	%
	Thus, replacing $C$ with a slightly smaller constant, and replacing $I_{k,i}$ with the collection of cubes $\B(l_{j_{k,i}}, I_{k,i})$, we may assume all cubes in the decomposition are hyperdyadic. We let $Y_{k_1,k_2}$ to be the union of all cubes in the decomposition $\{ I_{k_1,i} \}$ with hyperdyadic length $l_{k_2}$. Note that $Y_{k_1,k_2}$ is the union of $O((1/l_{k_2})^{\alpha + C\varepsilon})$ cubes. We let $\mathbf{Q}_{k_1,k_2}$ be the collection of cubes covering $Y_{k_1,k_2}$ which minimize the quantity
	%
	\[ \sum_{Q \in \mathbf{Q}_{k_1,k_2}} l(Q)^\alpha \]
	%
	and such that $l(Q) \geq l_{k_2}$ for each $Q$. Then clearly
	%
	\[ \sum_{Q \in \mathbf{Q}_{k_1,k_2}} l(Q)^\alpha \lesssim l_{k_2}^{- C\varepsilon}. \]
	%
	In particular, this means $l(Q) \lesssim l_{k_2}^{-C\varepsilon/\alpha}$. Furthermore, for each $I$ with $l(I) \geq l_{k_2}$,
	%
	\[ \sum_{Q \subset I} l(Q)^\alpha \leq l(I)^\alpha. \]
	%
	Now we define $X_k$ by collection all cubes in the sets $\mathbf{Q}_{k_1,k_2}$ which have sidelength $l_k$. Then clearly $X_k$ is $l_k$ discretized. Since $X_k$ only contains cubes from $\mathbf{Q}_{k_1,k_2}$ where $k_2 \geq k_1$, and $l_k \lesssim l_{k_2}^{-C\varepsilon/\alpha}$, this means that there are only $O(\log(1/l_k)^2)$ such choices of $(k_1,k_2)$. But this means that for $Q \in \mathbf{Q}_{k_1,k_2}$
	%
	\[ \sum_{Q \subset I} l(Q)^\alpha \lesssim \log(1/l_k)^2 l(I)^\alpha \]
	%
	In particular,
	%
	\[ |X_k| \leq \sum l(Q)^d = l_k^{d - \alpha} \sum l(Q)^\alpha \lesssim \log(1/l_k)^2 l_k^{d-\alpha} \]

	%(1 + \varepsilon)^{2k}

	$|X_k| \leq \log(1/l_k)^2$
\end{proof}










\section{Hypergraphs}

\begin{lemma}[Tur\'{a}n]
    For any $k$ uniform hypergraph $H = (V,E)$ with $|E| \leq |V|^\alpha$, $V$ contains an independant set of size $\Omega(|V|^{(k-\alpha)/(k-1)})$.
\end{lemma}
\begin{proof}
    We create an independant set $I$ by the following procedure. First, select a subset $S$ of vertices, including each independantly with probability $p$. Delete a single vertex from each edge in each hypergraph entirely contained in $S$, obtaining an independant set $I$. We find that each edge in $V$ is entirely included in $S$ with probability $p^k$, and $S$ has expected size $p |V|$, so $\mathbf{E}|I| = p |V| - p^k |E|$. If $|E| = |V|^\alpha$ for $\alpha \geq 1$, then setting $p = (1/2) |V|^{(1 - \alpha)/(k-1)}$ induces a set $I$ with size
    %
    \[ |V|^{(k - \alpha)/(k-1)}(1/2 - 1/2^k) \]

    We create an independant set $I$ by the following procedure. First, select a subset $S$ of vertices, including each vertex independantly with probability $p$. Delete a single vertex from each edge in each hypergraph which is entirely contained in $S$. Then $I$ is an independant set with respect to each hypergraph, and we shall show that for an appropriate choice of $p$, $\mathbf{E} |I| \geq h$.

    Trivially, we find $\mathbf{E}|S| = p |V|$. For any $i \geq 2$, the expected number of edges of $H_i$ falling entirely in $S$ is
    %
    \[ p^i |E_i| \leq \frac{p^i |V|^i}{c_k h^{i-1}} \]
    %
    therefore
    %
    \[ \mathbf{E}|I| = p|V| - \sum_{i = 2}^k \frac{p^i |V|^i}{c_k h^{i-1}} \]
    %
    Setting $p = 2h/|V|$ and $c_k = 2^{k+1}$ gives
    %
    \[ \mathbf{E}|I| = h \left( 2 - \sum_{i = 2}^k \frac{1}{2^{k+1-i}} \right) > h \]
    %
    which completes the proof.
\end{proof}