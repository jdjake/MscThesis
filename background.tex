%% The following is a directive for TeXShop to indicate the main file
%%!TEX root = diss.tex

\chapter{Background}
\label{ch:Background}

\section{Configuration Avoidance}

We begin by describing the problem we wish to address in this thesis. We consider an ambient set $\AAA$. It's \emph{$n$-point configuration space} is
%
\[ \Config^n(\AAA) = \{ (x_1, \dots, x_n) \in X^n: x_i \neq x_j\ \text{if $i \neq j$} \}. \]
%
An \emph{$n$ point configuration}, or \emph{pattern}, is a subset of $\Config^n(\AAA)$. More generally, we define the general \emph{configuration space} of $\AAA$ as $\Config(\AAA) = \bigcup_{n = 1}^\infty \Config^n(\AAA)$, and a \emph{pattern}, or \emph{configuration}, on $\AAA$ is a subset of $\Config(\AAA)$.

Our main focus on the thesis is the \emph{pattern avoidance problem}. For a fixed configuration $\C$ on $\AAA$, we say a set $X \subset \AAA$ \emph{avoids} $\C$ if $\Config(X)$ is disjoint from $\C$. The pattern avoidance problem asks to find sets $X$ of maximal size avoiding a fixed configuration $\C$. The set $\C$ often describes the presence of algebraic or geometric structure, and so we are trying to find large sets which do not possess this structure.

\begin{example}[Isoceles Triangle Configuration]
	Let
	%
	\[ \C = \left\{ (x_1, x_2, x_3) \in \Config^3(\RR^2) : |x_1-x_2| = |x_1-x_3| \right\}. \]
	%
	Then $\C$ is a 3-point configuration, and a set $X \subset \RR^2$ avoids $\C$ if and only if it does not contain all vertices of an isoceles triangle. %Notice that $|x_1 - x_2| = |x_1 - x_3|$ holds if and only if $|x_1 - x_2|^2 = |x_1 - x_3|^2$, which is an algebraic equation in the coordinates of $x_1,x_2$, and $x_3$. Thus $\C$ is an algebraic hypersurface of degree two in $\RR^6$.
\end{example}

\begin{example}[Linear Independence Configuration]
	Let $V$ be a vector space over a field $K$. We set
	%
	\[ \C = \bigcup_{n = 1}^\infty \{ (x_1, \dots, x_n) \in \C^n(V): \text{for any}\ a_1, \dots, a_n \in K,\ a_1x_1 + \dots + a_nx_n \neq 0 \}. \]
	%
	A subset $X \subset V$ avoids $\C$ if and only if $X$ is a linearly independent subset of $X$. Note that if $V$ is infinite dimensional, then $\C$ cannot be replaced by a $n$ point configuration for any $n$; arbitrarily large tuples must be considered. We will be interested in the case where $K = \QQ$, and $V = \RR$, where we will be looking for analytically large, linearly independant subsets of $V$.
\end{example}

%\begin{example}[General Position Configuration]
%	Suppose we wish to find a subset $X$ of $\RR^d$ such that for each positive integer $k \leq d$, and for each collection of $k+1$ distinct points $x_1, \dots, x_{k+1} \in X$, the points do not lie in a $k-1$ dimensional hyperplane. For each $k \leq d$, set
	%
%	\[ \C^{k+1} = \{ (x_0, x_1, \dots, x_k) \in \Config^{k+1}(\RR^d): x_1-x_0, \dots, x_k - x_0\ \text{are linearly dependant} \}. \]
	%
%	If we define $\C = \bigcup_{k = 2}^d \C^k$, then a set $X$ avoids $\C$ precisely when all finite collection of distinct points in $X$ lie in general position. Notice that
	%
%	\[ \C^{k+1} = \bigcup \left\{ \text{span}(y_1, \dots, y_k) \times \{ y \} : y = (y_1, \dots, y_k) \in \Config^k(\RR^d) \right\} \cap \Config^{k+1}(\RR^d). \]
	%
%	so each $\C^{k+1}$ is essentially a union of $k$ dimensional hyperplanes.
%\end{example}

Even though our problem formulation assumes configurations are formed by distinct sets of points, one can still formulate avoidance problems involving repeated points in our framework by a simple trick.

\begin{example}[Sum Set Configuration]
	Let $G$ be an abelian group, and fix $Y \subset G$. Set
	%
	\[ \C^1 = \{ g \in \Config^1(G): g + g \in Y \} \quad \text{and} \quad \C^2 = \{ (g_1,g_2) \in \Config^2(G): g_1 + g_2 \in Y \}. \]
	%
	Then set $\C = \C^1 \cup \C^2$. A set $X \subset G$ avoids $\C$ if and only if $(X + X) \cap Y = \emptyset$.
\end{example}

Depending on the structure of the ambient space $\AAA$ and the configuration $\C$, there are various ways of measuring the size of sets $X \subset \AAA$:
%
\begin{itemize}
	\item If $\AAA$ is finite, the goal is to find a set $X$ with large cardinality.
	\item If $\{ \AAA_n \}$ is an increasing family of finite sets with $\AAA = \lim \AAA_n$, the goal is to find a set $X$ such that $X \cap \AAA_n$ has large cardinality asymptotically in $n$.
	\item If $\AAA = \RR^d$, but $\C$ is a discrete configuration, then a satisfactory goal is to find a set $X$ with large Lebesgue measure avoiding $\C$.
\end{itemize}
%
In this thesis, inspired by results in these three settings, we establish methods for avoiding non-discrete configurations $\C$ in $\RR^d$. Here, Lebesgue measure completely fails to measure the size of pattern avoiding solutions, as the next theorem shows, under the often true assumption that $\C$ is \emph{translation invariant}, i.e. that if $(a_1, \dots, a_n) \in \C$ and $b \in \RR^d$, $(a_1 + b, \dots, a_n + b) \in \C$.

\begin{theorem}
	Let $\C$ be a $n$-point configuration on $\RR^d$. Suppose
	%
	\begin{enumerate}
		\item \label{translationinvariance} $\C$ is translation invariant.
		\item \label{nonDiscreteConfig} For any $\varepsilon > 0$, there is $(a_1, \dots, a_n) \in \C$ with $\diam \{ a_1, \dots, a_n \} \leq \varepsilon$.
	\end{enumerate}
	%
	Then no set with positive Lebesgue measure avoids $\C$.
\end{theorem}
\begin{proof}
	Let $X \subset \RR^d$ have positive Lebesgue measure. The Lebesgue density theorem shows that there exists a point $x \in X$ such that
	%
	\begin{equation} \label{densityApplication} \lim_{l(Q) \to 0} \frac{|X \cap Q|}{|Q|} = 1, \end{equation}
	%
	where $Q$ ranges over all cubes in $\RR^d$, and $l(Q)$ denotes the sidelength of $Q$. Fix $\varepsilon > 0$, to be specified later, and choose $r$ small enough that $|X \cap Q| \geq (1 - \varepsilon) |Q|$ for any cube $Q$ with $x \in Q$ and $l(Q) \leq r$. Now let $Q_0$ denote the cube centered at $x$ with $l(Q_0) \leq r$. Applying Property \ref{nonDiscreteConfig}, we find $C = (a_1, \dots, a_n) \in \C$ such that $\diam \{ a_1, \dots, a_n \} \leq l(Q)/2$. For each $p \in Q_0$, let $C(p) = (a_1(p), \dots, a_n(p))$, where $a_i(p) = p + (a_i - a_1)$. Property \ref{translationinvariance} implies $C(p) \in \C$ for each $p \in \RR^d$. A union bound shows
	%
	\begin{equation} \label{equation548} \left| \{ p \in Q_0 : C(p) \not \in \C(X) \} \right| \leq \sum_{i = 1}^d \left| \{ p \in Q_0 : a_i(p) \not \in X \} \right|.
	\end{equation}
	%
	Note that $a_i(p) \not \in X$ precisely when $p + (a_i - a_1) \not \in X$, so
	%
	\begin{equation} \label{equation1243462}
		|\{ p \in Q_0 : a_i(p) \not \in X \}| = |(Q_0 + (a_i - a_1)) \cap X^c|.
	\end{equation}
	%
	Note $Q_0 + (a_i - a_1)$ is a cube with the same sidelength as $Q_0$. Since $|a_i - a_1| \leq r/2$, $x \in Q_0 + (a_i - a_1)$. Thus \eqref{densityApplication} shows
	%
	\begin{equation} \label{equation543} |Q_0 + (a_i - a_1)) \cap X^c| \leq \varepsilon |Q_0|. \end{equation}
	%
	Combining \eqref{equation548}, \eqref{equation1243462}, and \eqref{equation543}, we find
	%
	\[ \left| \{ p \in Q_0 : C(p) \not \in \C(X) \} \right| \leq \varepsilon d |Q_0|. \]
	%
	Provided $\varepsilon d < 1$, this means there is $p \in Q_0$ with $C(p) \in \C(X)$.
\end{proof}

Since no set of positive Lebesgue measure can avoid non-discrete configurations, we cannot use the Lebesgue measure to quantify the size of pattern avoiding sets. Fortunately, there is a quantity which can distinguish between the size of sets of measure zero. This is the \emph{fractional dimension} of a set.

%We primarily use two variants of fractional dimension in this thesis: Minkowski dimension, Hausdorff dimension. Both assign the same dimension to any smooth manifold, but vary over more singular sets. The main difference between the first two is that Minkowski dimension measures relative density at a single scale, whereas Hausdorff dimension measures relative density simultaneously at a countable set of scales.









\section{Fractional Dimension}

There are many variants of fractional dimension. Here we choose to use the Minkowski dimension, and the Hausdorff dimension. They assign the same dimension to any smooth manifold, but vary over more singular sets. The biggest difference is that Minkowski dimension measures relative density at a single scale, whereas Hausdorff dimension measures relative density at countably many scales. This makes Hausdorff dimension more stable under analytical operations.

We begin by discussing the Minkowski dimension, which is the simplest to define. Given a length $l$, and a bounded set $E \subset \RR^d$, we let $N(l,E)$ denote the length $l$ covering number of $E$, i.e. the minimum number of sidelength $l$ cubes required to cover $E$. We define the \emph{lower} and \emph{upper} Minkowski dimension as
%
\[ \lowminkdim(E) = \liminf_{l \to 0} \frac{\log(N(l,E))}{\log(1/l)} \quad \upminkdim(E) = \limsup_{l \to 0} \frac{\log(N(l,E))}{\log(1/l)}. \]
%
If $\upminkdim(E) = \lowminkdim(E)$, then we refer to this common quantity as the \emph{Minkowski dimension} of $E$, denoted $\minkdim(E)$. Thus $\lowminkdim(E) < s$ if there {\it exists} a sequence of lengths $\{ l_k \}$ converging to zero with $N(l_k,E) \leq (1/l_k)^s$, and $\upminkdim(E) < s$ if $N(l,E) \leq (1/l)^s$ for \emph{all} sufficiently small lengths $l$.

Hausdorff dimension is slightly more technical to define. For $E \subset \RR^d$ and $\delta > 0$, we define the \emph{Hausdorff content}
%
\[ H_\delta^s(E) = \inf \left\{ \sum_{k = 1}^\infty l(Q_k)^s : E \subset \bigcup_{k = 1}^\infty Q_k, l(Q_k) \leq \delta \right\}. \]
%
We then set $H^s(E) = \lim_{\delta \to 0} H_\delta^s(E)$. It is easy to see $H^s$ is an exterior measure, and $H^s(E \cup F) = H^s(E) + H^s(F)$ if the Hausdorff distance $d(E,F)$ between $E$ and $F$ is positive. So $H^s$ is actually a metric exterior measure, and the Caratheodory extension theorem shows all Borel sets are measurable with respect to $H^s$. We then define the dimension of $E$ to be the supremum of the values $s$ such that $H^s(E) < \infty$.

\begin{lemma} \label{HausdorffBoundary}
	Consider $t < s$, and a set $E$.
	%
	\begin{itemize}
		\item If $H^t(E) < \infty$, then $H^s(E) = 0$.
		\item If $H^s(E) \neq 0$, then $H^t(E) = \infty$.
	\end{itemize}
\end{lemma}
\begin{proof}
	If, for any cover of $E$ by a collection of intervals $\{ Q_k \}$, $\sum l(Q_k)^t \leq A$, and $l(Q_k) \leq \delta$, then
	%
	\[ \sum l(Q_k)^s \leq \sum l(Q_k)^{s-t} l(Q_k)^t \leq \delta^{s-t} A. \]
	%
	Thus $H^s_\delta(E) \leq \delta^{s-t} A$, and taking $\delta \to 0$, we conclude $H^s(E) = 0$. The latter point is just proved by taking contrapositives.
\end{proof}

\begin{remark}
	It is easy to see directly from the definition that $H^d$ is the Lebesgue measure on $\RR^d$. Thus $H^d[-N,N]^d = (2N)^d$. If $s > d$, Lemma \ref{HausdorffBoundary} shows $H^s[-N,N]^d = 0$, and so by countable additivity, taking $N \to \infty$ shows $H^s(\RR^d) = 0$. Thus $H^s(E) = 0$ for all $E$ if $s > d$.
\end{remark}

Given any Borel set $E$, the last remark, combined with Lemma \ref{HausdorffBoundary}, implies there is a unique value $s_0 \in [0,d]$ such that $H^s(E) = 0$ for $s > s_0$, and $H^s(E) = \infty$ for $0 \leq s < s_0$. We refer to $s_0$ as the \emph{Hausdorff dimension} of $E$, denoted $\hausdim(E)$.

\begin{theorem}
	For any bounded set $E$, $\hausdim(E) \leq \lowminkdim(E) \leq \upminkdim(E)$.
\end{theorem}
\begin{proof}
	Suppose there is a sequence $\{ l_k \}$ with $l_k \to 0$ such that for all $\varepsilon > 0$, $N(l_k,E) \leq (1/l_k)^{s - \varepsilon}$ for sufficiently large $k$. We then consider the simple bound $H^s_{l_k}(E) \leq N(l_k,E) \cdot l_k^s \leq l_k^\varepsilon$, and taking $l_k \to 0$ shows $H^s(E) = 0$. Thus $\hausdim(E) \leq s$, and since $\{ l_k \}$ was arbitrary, we conclude $\hausdim(E) \leq \lowminkdim(E)$.
\end{proof}

The fact that Hausdorff dimension is defined with respect to multiple scales makes it more stable under analytical operations. In particular,
%
\[ \hausdim \left\{ \bigcup E_k \right\} = \sup \left\{ \hausdim(E_k). \right\} \]
%
This need not be true for the Minkowski dimension; a single point has Minkowski dimension zero, but the rational numbers, which are a countable union of points, have Minkowski dimension one. An easy way to make Minkowski dimension countably stable is to define the \emph{modified Minkowski dimensions}
%
\begin{align*}
	\lmbdim(E) &= \inf \left\{ s : E \subset \bigcup_{i = 1}^\infty E_i, \lowminkdim(E_i) \leq s \right\}\\
	&\text{and}\\
	\umbdim(E) &= \inf \left\{ s : E \subset \bigcup_{i = 1}^\infty E_i, \upminkdim(E_i) \leq s \right\}.
\end{align*}
%
This notion of dimension, in a disguised form, appears later on in this thesis.

\begin{remark}
	If $\hausdim(E) < d$, then $|E| = H^d(E) = 0$. Thus any set with fractional dimension less than $d$ must have measure zero, and so the dimension is a way of distinguishing between sets of measure zero, which is precisely what we needed in the configuration avoidance problem.
\end{remark}







\section{Dyadic Scales}

It is now useful to introduce the dyadic notation we utilize throughout this thesis. We focus on $[0,1]^d$, since all the sets we construct will be a subset of the unit cube. At the cost of losing certain topological information about $[0,1]^d$, applying dyadic techniques often allows us to elegantly discretize certain problems in Euclidean space. We introduce fractional dimension through a dyadic framework, and all our constructions will be done dyadically.

We fix a sequence of positive integers $\{ N_k : k \geq 1 \}$, referred to as \emph{branching factors}. Using these integers, we define a sequence of positive rational numbers $\{ l_k : k \geq 0 \}$, recursively such that $l_0 = 1$, and $l_{k+1} = l_k/N_{k+1}$. For each $k \geq 0$, set
%
\[ \DQ_k^d = \left\{ [ a_1 l_k , (a_1 + 1) l_k ] \times \dots \times [ a_d l_k , (a_d + 1) l_k ] : a \in \mathbf{Z}^d \right\}, \]
%
and $\DQ^d = \bigcup_{k \geq 0} \DQ_k^d$. We call the set $\DQ_k^d$ the \emph{dyadic cubes of generation $k$}. For any set $E \subset [0,1]^d$, we let $\DQ_k^d(E) = \{ Q \in \DQ_k^d : Q \cap E \neq \emptyset \}$.

The most common class of dyadic cubes in analysis is obtained from setting $N_k = 2$ for each $k$. We reserve a special notation for this class of dyadic cubes: the class of all such cubes is denoted by $\DD^d$, and the generation $k$ cubes by $\DD^d_k$.
% and the resulting classes of indices by $\{ \Delta_k^d \}$.
These dyadic cubes have a constant branching factor, which makes them easy to analyze. But it is necessary in our methods to let the branching factor to tend to $\infty$. This is why we have to introduce the more general family of dyadic cubes given above.

Sometimes, we need to rely on certain `intermediary' cubes that lie between the scales $\DQ_k^d$ and $\DQ_{k+1}^d$. This is the case in an iterative construction such that each step can be split down into two different scales. In this case, we write $N_k = K_k M_k$, for two integers $K_k$ and $M_k$, write $r_k = l_{k-1}/K_k = M_k l_k$, and
%
\[ \DR_k^d = \left\{ [ a_1 r_k , (a_1 + 1) r_k ] \times \dots \times [ a_d r_k , (a_d + 1) r_k ] : a \in \mathbf{Z}^d \right\}. \]
%
Thus the classes of dyadic cubes we consider, in order from coarsest scale to finest scale, can be listed as $\{ \DQ_0, \DR_1, \DQ_1, \DR_2, \DQ_2, \dots \}$

%For the purposes of discretization, and to simplify notation, it is very useful to identify those cubes in $\DQ_k^d$ which are subsets of $[0,1]^d$ with the set
%
%\[ \Sigma_k^d = [N_1]^d \times \dots \times [N_k]^d. \]
%
%Given $j \in \Sigma_m^d$, we let $Q_j \in \DQ_m^d$ denote the cube with left-hand corner $a = \sum_{k = 1}^m j_kl_k$. Thus subcubes of a cube corresponding to an index $j \in \Sigma_k^d$ correspond to indices obtained by appending additional integers onto $j$. In the case of intermediary scales, we abuse notation, writing $[N_k] = [K_k] \times [M_k]$, we have
%
%\[ \Sigma_k^d = \big([K_1] \times [M_1] \big)^d \times \dots \times \big([K_k] \times [M_k] \big)^d, \]
%
%where $n \in [N_k]$ is equal to $(k,m) \in [K_k] \times [M_k]$, where $k M_k + m = n$.

%One very useful property of the cubes in $\DQ^d$ is that cubes are either nested within one another, or \emph{almost disjoint} from one another, in the sense that only their boundaries intersect. Thus we can think of $\DQ^d$ as a forest under the partial ordering of inclusion, with the roots corresponding to the elements of $\DQ_0^d$. Each cube $Q \in \DQ^d_k$ has $N_{k+1}^d$ children. For each cube $Q \in \DQ_{k+1}^d$, we will let $Q^* \in \DQ_k^d$ denote it's parent, i.e. the unique cube in $\DQ_k^d$ with $Q \subset Q^*$. Similarily, given an index $I = (I_0, \dots, I_{k+1}) \in \Sigma_{k+1}$, we let $I^* = (I_0, \dots, I_k) \in \Sigma_k$, so that $Q_I \subset Q_{I^*}$.

%We often construct configuration avoiding sets as limits of dyadic discretizations. It is most convenient to describe this construction in terms of a sequence of sets $\{ S_k \}$ with $S_k \subset \Sigma_k^d$ for each $k$, and $(S_{k+1})^* \subset S_k$ for each $k$. We refer to such a sequence as a \emph{constructing sequence}. We can define $E_k = \bigcup \{ Q_j : j \in S_k \}$. Then $E_{k+1} \subset E_k$ for each $k$, and we can form a set $\bigcap E_k$, which is the final, non discretized limit of the sequence of discretizations.

%We can also give the spaces $\Sigma_k^d$ and $\Sigma^d$ a metric space structure, by defining, for $I \neq J$, $d(I,J) = l_k$, where $k$ is the smallest index such that $I_k \neq J_k$. Aside from the fact that some points in $\RR^d$ are duplicated in $\Sigma^d$, the main difference between the two spaces is that the balls in $\Sigma^d$ are discretized to the scales $\{ l_k \}$. From the point of view of geometric measure theory, the first point is neglible, but we shall find that discretization does matter. The degree to which the geometry of $\Sigma^d$ models the geometry of $\RR^d$ from the perspective of geometric measure theory is a key topic in this thesis. We shall find that the rate at which the lengths $l_k$ tend to zero will be a key factor in this relationship.

\section{Frostman Measures}

%\begin{example}
%	Let $s = 0$. Then $H_\delta^0(E)$ is the number of $\delta$ balls it takes to cover $E$, which tends to $\infty$ as $\delta \to 0$ unless $E$ is finite, and in the finite case, $H_\delta^0(E) \to \# E$. Thus $H^0$ is just the counting measure.
%\end{example}

%\begin{example}
%	Let $s = d$. If $E$ has Lebesgue measure zero, then for any $\varepsilon > 0$, there exists a sequence of balls $\{ B(x_k,r_k) \}$ covering $E$ with
	%
%	\[ \sum_{k = 1}^\infty r_k^d < \varepsilon^d. \]
	%
%	Then we know $r_k < \varepsilon$, so $H^s_\varepsilon(E) < \varepsilon^d$. Letting $\varepsilon \to 0$, we conclude $H^d(E) = 0$. Thus $H^d$ is absolutely continuous with respect to the Lebesgue measure. The measure $H^d$ is translation invariant, so $H^d$ is actually a constant multiple of the Lebesgue measure.
%\end{example}

It is often easy to upper bound Hausdorff dimension, but non-trivial to \emph{lower bound} the Hausdorff dimension of a given set. A key technique to finding a lower bound is \emph{Frostman's lemma}, which says that a set has large Hausdorff dimension if and only if it supports a probability measure which obeys a certain decay law on small sets. We say a measure $\mu$ is a \emph{Frostman measure} of dimension $s$ if it is non-zero, compactly supported, and for any cube $Q$, $\mu(Q) \lesssim l(Q)^s$. The proof of Frostman's lemma will utilize a technique often useful, known as the \emph{mass distribution principle}.

\begin{lemma}[Mass Distribution Principle]
	Let $\mu: \DQ^d \to [0,\infty)$ be a function such that for any $k$, and for any $Q_0 \in \DQ^d_k$,
	%
	\[ \sum \left\{ \mu(Q) : Q \in \DQ^d_{k+1}, Q^* = Q_0 \right\} = \mu(Q_0). \]
	%
	Then $\mu$ extends uniquely to a regular Borel measure on $\RR^d$.
\end{lemma}
\begin{proof}
	We can define a sequence of regular Borel measures $\{ \mu_k \}$ by setting, for each $f \in C_c(\RR^d)$,
	%
	\[ \int f d\mu_k = \sum \left\{ \mu(Q) \int_Q f : Q \in \DQ_k^d \right\} \]
	%
	Similarily, define a family of operators $\{ E_k \}$ on regular Borel measures by the formula
	%
	\[ \int f(x) dE_k(\nu) = \sum \left\{ \nu(Q) \int_Q f : Q \in \DQ_k^d \right\}. \]
	%
	The main condition of the theorem then says that $E_j(\mu_k) = \mu_j$ if $j \leq k$. Note that the operators $\{ E_k \}$ are each continuous with respect to the weak topology; If $\nu_i \to \nu$ weakly, then $\nu_i(Q) \to \nu(Q)$ for each fixed $Q \in \DQ_k^d$. Thus if $f \in C_c(\RR^d)$, then the support of $f$ intersects only finitely many cubes in $\DQ_k^d$, and this implies
	%
	\[ \int f(x) dE_k(\nu_i) = \sum \nu_i(Q) \int_Q f\; dx \to \sum \nu(Q) \int_Q f\; dx = \int f dE_k(\nu). \]
	%
	Now fix an interval $Q \in \DQ_1^d$. The measures $\{ \mu_k|_Q \}$, restricted to $Q$, all have total mass $\mu(Q)$. Thus the Banach-Alaoglu theorem implies that is a subsequence $\mu_{k_i}|_Q$ converging weakly on $Q$ to some measure $\mu_Q$. By continuity,
	%
	\[ E_j(\mu_Q) = \lim E_j(\mu_{k_i}|_Q) = \lim E_j(\mu_{k_i})|_Q = \mu_j|_Q. \]
	%
	This means precisely that $\mu_Q$ is the extension of $\mu$ to a Borel measure on $Q$. If we patch together the measures $\mu_Q$ over all choices of $Q \in \DQ_0^d$, we obtain a measure extending $\mu$ on all of $\RR^d$. The uniqueness of the extension is guaranteed by the fact that the intervals upon which $\mu$ are defined generate the entire Borel sigma algebra.
\end{proof}

%\begin{lemma}
%	let $\mu^+$ be a function from $\B$ to $[0,\infty)$ such that for any $I \in \B(1/M^k,\RR^d)$,
	%
%	\[ \sum \left\{ \mu^+(J) :J \in \B(1/M^{k+1},I) \right\} \leq \mu^+(I) \]
	%
%	Assume there exists $c > 0$ such that for all $k$,
	%
%	\[ \sum \left\{ \mu^+(I) : I \in \B(1/M^k,I) \right\} \geq c \]
	%
%	and
	%
%	\[ \sum \left\{ \mu^+(I) : I \in \B(1,I) \right\} < \infty \]	
	%
%	Then there exists a non-zero Borel measure $\mu$ such that $\mu(I) \leq \mu^+(I)$ for $I \in \B$.
%\end{lemma}
%\begin{proof}
%	As in the last lemma, define the operators $E_k$ and the measures $\mu_k$. By weak compactness, a subsequence of these measures converge weakly to some measure $\mu$, and $E_k(\mu) = \lim E_k(\mu_{j_k}) \leq \mu_k$. The measure $\mu$ is nonzero, since $\| \mu_{j_k} \| \geq c$ for each $k$, and so $\| \mu \| \geq c$.
%\end{proof}

\begin{lemma}[Frostman's Lemma]
	If $E$ is Borel, $H^s(E) > 0$ if and only if there exists an $s$ dimensional Frostman measure supported on $E$.
\end{lemma}
\begin{proof}
	Suppose that $\mu$ is $s$ dimensional and supported on $E$. If $H^s(F) = 0$, then for $\varepsilon > 0$ there is a sequence of cubes $\{ Q_k \}$ with $\sum_{k = 1}^\infty l(Q_k)^s \leq \varepsilon$. But then
	%
	\[ \mu(F) \leq \mu \left( \bigcup_{k = 1}^\infty Q_k \right) \leq \sum_{k = 1}^\infty \mu(Q_k) \lesssim \sum_{k = 1}^\infty l(Q_k)^s \leq \varepsilon. \]
	%
	Taking $\varepsilon \to 0$, we conclude $\mu(F) = 0$. Thus $\mu$ is absolutely continuous with respect to $H^s$. But since $\mu(E) > 0$, this means that $H^s(E) > 0$.

	Conversely, suppose $H^s(E) > 0$. Then by translating, we may assume that $H^s(E \cap [0,1]^d) > 0$, and so without loss of generality we may assume $E \subset [0,1]^d$. Fix $m$, and for each $Q \in \DD_k^d$, define $\mu^+(Q) = H^s_m(E \cap Q)$. Then $\mu^+(Q) \leq 1/2^{ks}$, and $\mu^+$ is subadditive. We use it to recursively define a Frostman measure $\mu$, such that $\mu(Q) \leq \mu^+(Q)$ for each $Q \in \DD_k^d$. We initially define $\mu$ by setting $\mu([0,1]^d) = \mu^+([0,1]^d)$. Given $Q \in \DD_k^d$, we enumerate it's children as $Q_1, \dots, Q_M \in \DD_{k+1}^d$. We then consider any values $A_1, \dots, A_M \geq 0$ such that
	%
	\[ A_1 + \dots + A_M = \mu(Q),\quad \text{and}\ A_k \leq \mu^+(Q_k)\ \text{for each $k$}. \]
	%
	This is feasible to do because $\mu^+(Q_1) + \dots + \mu^+(Q_M) \geq \mu^+(Q)$. We then define $\mu(Q_k) = A_k$ for each $k$. The recursive constraint is satisfied, so $\mu$ is well defined. The mass distribution principle then implies that $\mu$ extends to a full measure, which satisfies $\mu(Q) \leq \mu^+(Q) \leq 1/2^{ks}$ for each $Q \in \DD_k^d$. Given any cube $Q$, we can cover it by $O_d(1)$ dyadic cubes with length at most twice that of $Q$. This shows $\mu(Q) \lesssim_d l(Q)^s$ for any cube $Q$. And this means that we have shown directly that $\mu$ is a Frostman measure of dimension $s$.
\end{proof}

Frostman's lemma implies that to study the Hausdorff dimension of the set, it suffices to understand the class of measures which can be supported on that set.




\section{Dyadic Fractional Dimension}

We wish to establish results about fractional dimension `dyadically'. We begin with Minkowski dimension. If we construct a set $E = \bigcap E_k$ such that $E_k$ is the union of cubes in $\DQ^d_k$, it is often easy to establish estimates on $N(l,E)$ for $l = l_k$ simply by counting the cardinality of $\DQ^d_k(E)$, as the next lemma indicates.

\begin{lemma} \label{comparableCovers}
	For any set $E$, $\#(\DQ_k^d(E)) \sim_d N(l_k,E)$.
\end{lemma}
\begin{proof}
	Since $\DQ_k^d(E)$ is a cover of $E$ by sidelength $l_k$ cubes, minimality shows $N(l_k,E) \leq \#(\DQ_k^d(E))$. On the other hand, given \emph{any} cover of $E$ by sidelength $l_k$ cubes, we can replace each cube in the cover by at most $2^d$ cubes in $\DQ_k^d$, so $N(l_k,E) \leq 2^d \#(\DQ_k^d(E))$.
\end{proof}

Thus it is natural to ask whether it is true that for any set $E$,
%
\begin{equation} \label{definingSequence}
	\begin{aligned}
		\lowminkdim(E) &= \liminf_{k \to \infty} \frac{\log \left[ \#(\DQ_k^d(E)) \right]}{\log(1/l_k)}\\
		&\text{and}\\
		\upminkdim(E) &= \limsup_{k \to \infty} \frac{\log \left[ \#(\DQ_k^d(E)) \right]}{\log(1/l_k)}.
	\end{aligned}
\end{equation}
%
The answer depends on the choice of branching factors $\{ N_k \}$.

%Subsets of $\Sigma^d$ can also be assigned a Minkowski dimension. We define
%
%\[ \lowminkdim(E) = \liminf_{k \to \infty} \frac{\log(\#(\sigma_k^d(E)))}{\log(1/l_k)}\quad\text{and}\quad\upminkdim(E) = \limsup_{k \to \infty} \frac{\log(\#(\sigma_k^d(E)))}{\log(1/l_k)}. \]
%
%This makes sense, because $\Sigma^d$ only really has `balls' of radius $\{ l_k \}$, for each $k$, and \emph{any} cover of $E$ by balls of radius $l_k$ contains $\sigma_k^d(E)$. In order 
%we have $N(E,l_k) = \# (\Sigma_k^d(E))$, since \emph{any} cover of $E$ by balls of radius $l_k$

\begin{lemma} \label{definingsequenceminkowski}
	If, for all $\varepsilon > 0$, $N_k \leq (N_1 \dots N_{k-1})^\varepsilon$ for large $k$, then \eqref{definingSequence} holds.
\end{lemma}
\begin{proof}
	Fix a length $l$, and find $k$ with $l_{k+1} \leq l \leq l_k$. Then applying Lemma \ref{comparableCovers} shows
	%
	\[ N(l,E) \leq N(l_{k+1},E) \lesssim_d \#(\DQ_{k+1}^d(E)) \]
	%
	and
	%
	\[ N(l,E) \geq N(l_k,E) \gtrsim_d \#(\DQ_k^d(E)). \]
	%
	Thus
	%
	\[ \frac{\log \left[ N(l,E) \right]}{\log(1/l)} \leq \left[ \frac{\log(1/l_{k+1})}{\log(1/l_k)} \right] \frac{\log \left[ \#(\DQ_{k+1}^d(E)) \right]}{\log(1/l_{k+1})} + O_d(1/k) \]
	%
	and
	%
	\[ \frac{\log \left[ N(l,E) \right]}{\log(1/l)} \geq \left[ \frac{\log(1/l_k)}{\log(1/l_{k+1})} \right] \frac{\log \left[ \#(\DQ_k^d(E)) \right]}{\log(1/l_k)} + O_d(1/k). \]
	%
	Thus, provided that
	%
	\begin{equation} \label{equivalenceofscales}
		\frac{\log(1/l_{k+1})}{\log(1/l_k)} \to 1,
	\end{equation}
	%
	The conclusion of the theorem is true. But $l_{k+1} = l_k/N_k$, and $l_k = 1/N_1 \dots N_{k-1}$, so \eqref{equivalenceofscales} is equivalent to the condition that
	%
	\[ \frac{\log(N_k)}{\log(N_1) + \dots + \log(N_{k-1})} \to 0, \]
	%
	and this is equivalent to the assumption of the theorem.
\end{proof}

Any constant branching factor satisfies the hypothesis of Lemma \ref{definingsequenceminkowski} for the Minkowski dimension. In particular, we can construct sets in $\DD^d$ without any problems occuring. But more importantly for our work, we can let the branching factors increase rapidly.

\begin{lemma} \label{rapidBranching}
	If $N_k = 2^{\lfloor 2^{k \psi(k)} \rfloor}$, where $\psi(k)$ is any decreasing sequence of positive numbers tending to zero, but for which $k \psi(k) \to \infty$, then $N_{k+1} \leq (N_1 \dots N_k)^\varepsilon$ for any $\varepsilon > 0$, when $k$ is sufficiently large.
\end{lemma}
\begin{proof}
	We note that $\log N_k = 2^{k \psi(k)} + O(1)$. Thus
	%
	\begin{align*}
		\frac{\log(N_k)}{\log(N_1) + \dots + \log(N_{k-1})} &\lesssim \frac{2^{k \psi(k)}}{2^{\psi(1)} + 2^{2 \psi(2)} + \dots + 2^{(k-1) \psi(k-1)}}\\
		&\leq \frac{2^{k \psi(k)}}{2^{\psi(k-1)} + 2^{2 \psi(k-1)} + \dots 2^{(k-1) \psi(k-1)}}\\
		&= \frac{2^{k \psi(k)} ( 2^{\psi(k-1)} - 1 )}{2^{k \psi(k-1)} - 2^{\psi(k-1)}} \\
		&\lesssim 2^{k[\psi(k) - \psi(k-1)]} (2^{\psi(k-1)} - 1)\\
		&\leq 2^{\psi(k-1)} - 1 \to 0.
	\end{align*}
	%
	This is equivalent to the fact that $N_{k+1} \leq (N_1 \dots N_k)^\varepsilon$ for any $\varepsilon > 0$, where $k$ is sufficiently large.
\end{proof}
%
We refer to any sequence $\{ l_k \}$ constructed by $\{ N_k \}$ satisfying the conditions of Lemma \ref{rapidBranching} as a \emph{subhyperdyadic} sequence. If a sequence is generated by branching factors of the form $2^{\lfloor 2^{ck} \rfloor}$ for a fixed $c > 0$, the lengths are referred to as \emph{hyperdyadic}. The next example shows that hyperdyadic sequences are essentially the `boundary' for sequences that can be used to measure the Minkowski dimension.

\begin{example}
	Construct a subset of $\RR$ as follows. Let $[N_k] = [K_k] \times [M_k]$, where $N_k$, $K_k$, and $M_k$ are parameters to be specified later. Define $E_0 = [0,1]$. Given $E_k$, define $E_{k+1}$ by dividing each sidelength $l_k$ dyadic interval in $E_k$ into $K_{k+1}$ intervals, and then keeping only the first interval. Then $\#(\DQ_{k+1}^d(E_{k+1})) = M_k \cdot \#(\DQ_k^d(E_k))$, and since $\#(\DQ_0^d(E_0)) = 1$, $\#(\DQ_k^d(E_k)) = M_1 \dots M_k$. Thus
	\begin{align*}
		\frac{\log \left[ N(l_k,E) \right]}{\log(1/l_k)} &\sim \frac{\log \left[ \#(\DQ_k^d(E_k)) \right]}{\log(1/l_k)}\\
		&= \frac{\log(M_1) + \dots + \log(M_k)}{\log(N_1) + \dots + \log(N_k)}\\
		&= 1 - \frac{\log(K_1) + \dots + \log(K_k)}{\log(N_1) + \dots + \log(N_k)}.
	\end{align*}
	%
	On the other hand, if $r_k = l_k/K_{k+1}$, $N(r_k,E) \sim_d \#(S_k) = M_1 \dots M_k$, so
	%
	\begin{align*}
		\frac{\log \left[ N(r_k,E) \right]}{\log(1/r_k)} &\sim \frac{\log \left[ \#(\DQ_k^d(E_k)) \right]}{\log(1/r_k)}\\
		&= \frac{\log(M_1) + \dots + \log(M_k)}{\log(N_1) + \dots + \log(N_k) + \log(K_{k+1})}\\
		&= 1 - \frac{\log(K_1) + \dots + \log(K_{k+1})}{\log(N_1) + \dots + \log(N_k) + \log(K_{k+1})}.
	\end{align*}
	%
	Set $N_k = 2^{\lfloor 2^{ck} \rfloor}$, and $K_k = 2^{\lfloor c 2^{ck} \rfloor}$. Then
	%
	\[ \log(K_1) + \dots + \log(K_k) = O(k) + c \sum_{i = 1}^k 2^{ck} = O(k) + c \frac{2^{c(k+1)} - 2^c}{2^c - 1} \]
	%
	and
	%
	\[ \log(N_1) + \dots + \log(N_k) = O(k) + \sum_{i = 1}^k 2^{ck} = O(k) + \frac{2^{c(k+1)} - 2^c}{2^c - 1}. \]
	%
	Thus
	%
	\[ \frac{\log \left[ N(l_k,E) \right]}{\log(1/l_k)} \to 1 - c \quad \text{and} \quad \frac{\log \left[ N(r_k,E) \right]}{\log(1/l_k)} \to \frac{1 - c}{1 - c + c2^c}. \]
	In particular, for any $0 < c \leq 1$,
	%
	\[ \lowminkdim(E) \neq \liminf_{k \to \infty} \frac{\log \left[ N(l_k,E) \right]}{\log(1/l_k)}, \]
	%
	so measurements at hyperdyadic scales fail to establish general results about the Minkowski dimension. This example can also be extended to show that measurements at hyperdyadic scales also fail to establish results about Hausdorff dimension, as discussed later in this section.

%	\[ \lim_{k \to \infty} \frac{\log(K_1) + \dots + \log(K_k)}{2^{c(k+1)} - 2^c} \neq \lim_{k \to \infty} \frac{\log(K_1) + \dots + \log(K_{k+1})}{2^{c(k+1)} - 2^c} \frac{1}{1 + \log(K_{k+1}) (2^c - 1)/(2^{c(k+1)} - 2^c)} \]

%	First, assume $N_k/N_{k-1} \in \mathbf{Z}$ for each $k$, and for convenience, set $N_0 = 1$. Define $S_0 = \{ 0 \}$, and then given $S_k$, recursively define
	%
%	\[ S_{k+1} = \{ (j,1), \dots, (j,N_{k+1}/N_k) : j \in S_k \} \]
	%
%	We then set $E = \pi(\lim S_k)$. If we let $E_k = \bigcup \{ Q_j : j \in S_k \}$, then $E_{k+1}$ can be constructed by dividing each sidelength $l_k$ dyadic interval in $E_k$ into $N_{k+1}$ intervals, and selecting the initial $N_{k+1}/N_k$ intervals, which have total length $1/(N_1 \dots N_k^2)$. Then $\#(S_{k+1}) = (N_{k+1}/N_k) \#(S_k)$, and since $\#(S_0) = 1$, $\#(S_k) = N_k$. Thus
	%
%	\[ \frac{\log \left[ N(l_k,E) \right]}{\log(1/l_k)} \sim_d \frac{\log \left[ \# (S_k) \right]}{\log(1/l_k)} = \frac{\log(N_k)}{\log(N_1) + \dots + \log(N_k)}. \]
	%
%	On the other hand, if $r_k = 1/(N_1 \dots N_k^2) = l_k/N_k$, then $N(l,E) = \#(S_k) = N_k$, so
	%
%	\[ \frac{\log \left[ N(r_k,E) \right]}{\log(1/r_k)} \sim_d \frac{\log \left[ \#(S_k) \right]}{\log(1/r_k)} = \frac{\log(N_k)}{\log(N_1) + \dots + 2\log(N_k)}. \]
%	In any case to which Lemma \ref{definingsequenceminkowski} applies, both of these estimates converge to zero as $k \to \infty$, so that $E$ has Minkowski dimension zero. On the other hand, if $N_k = 2^{\lfloor \psi(k) 2^k \rfloor}$ where $\psi(k)$ is an increasing sequence tending to $\infty$, then
	%
%	\begin{align*}
%		\frac{2^{(k-1) \psi(k-1)}}{2^{\psi(1)} + 2^{2 \psi(2)} + \dots + 2^{(k-1) \psi(k-1)}} &\geq \frac{\left(2^{(k-1) \psi(k-1)} \right) \left( 2^{\psi(k-1)} - 1 \right)}{2^{k \psi(k-1)} - 2^{\psi(k-1)}} \sim 1.
%	\end{align*}
	%
%	Thus
	%
%	\[ \lim_{k \to \infty} \frac{\log \left[ N(l_k,E) \right]}{\log(1/l_k)} = 1. \]
	%
%	On the other hand,
	%
%	\[ \frac{\log \left[ N(r_k, E) \right]}{\log(1/r_k)} = \frac{\log(N_k)}{\log(N_1) + \dots + 2\log(N_k)} \leq 1/2. \]
	%
%	Thus we cannot possibly have
	%
%	\[ \lowminkdim(E) = \liminf_{k \to \infty} \frac{\log \left( N(l_k,E) \right)}{\log(1/l_k)}, \]
	%
	%
%	A simple calculation shows this result even fails if $N_k = 2^{\lfloor 2^{ck} \rfloor}$, where $c > 1$.
\end{example}

%In particular, we can define the Minkowski dimensions of $E \subset \Sigma$ as
%
%\[ \lowminkdim(E) = \liminf_{k \to \infty} \frac{\log(\#(\Sigma_k^d(E)))}{\log(1/l_k)}\quad\text{and}\quad\upminkdim(E) = \limsup_{k \to \infty} \frac{\log(\#(\Sigma_k^d(E)))}{\log(1/l_k)}. \]
%
%Similarily, the Hausdorff measures $H^s$ are obtained by setting
%
% TODO: Fix this
%\[ H^s(E) = \left\{ \sum_m l_{k_m} : Q_m \in \Sigma_{k_m}^d\ \text{for each $k$}, \text{For any} \right\} \]
%
%and define the Hausdorff dimension correspondingly. A natural question is whether $\dim(\pi(E)) = \dim(E)$ for the various fractal dimensions we consider in this thesis. This is addressed in the next section.

We now move on to calculating Hausdorff dimension dyadically. The natural quantity to consider here is the measure $H^s_{\DQ}(E) = \lim H^s_{\DQ,m}(E)$, where
%
\[ H^s_{\DQ,m}(E) = \inf \left\{ \sum_k l(Q_k)^s : E \subset \bigcup_k^\infty Q_k,\ Q_k \in \bigcup_{i \geq m} \DQ_i\ \text{for each $k$} \right\}. \]
%
A similar argument to the standard Hausdorff measures shows there is a unique $s_0$ such that $H^s(E) = \infty$ for $s < s_0$, and $H^s(E) = 0$ for $s > s_0$. It is obvious that $H^s_{\DQ}(E) \geq H^s(E)$ for any set $E$, so we certainly have $s_0 \geq \hausdim(E)$. The next lemma gurantees that $s_0 = \hausdim(E)$, under the same conditions on $\{ N_k \}$ as found in Lemma \ref{definingsequenceminkowski}.

\begin{lemma} \label{lemma51464}
	If, for any $\varepsilon > 0$, $N_{k+1} \leq (N_1 \dots N_k)^\varepsilon$ for sufficiently large $k$, then $H^s_{\DQ}(E) \lesssim_d H^{s-\varepsilon}(E)$ for each $\varepsilon > 0$.
\end{lemma}
\begin{proof}
	Fix $\varepsilon > 0$. Let $E \subset Q_k$, where $l(Q_k) \leq l_m$ for each $k$. Then for each $k$, we can find $i_k$ such that $l_{i_k+1} \leq l(Q_k) \leq l_{i_k}$. Then $Q_k$ is covered by $O_d(1)$ elements of $\DQ_{i_k}^d$, and
	%
	\begin{equation} \label{equation824} H^s_{\DQ,m}(E) \lesssim_d \sum l_{i_k}^s \leq \sum (l_{i_k}/l_{i_k+1})^s l(Q_k)^s \leq \sum \left( l_{i_k}/l_{i_{k+1}} \right)^s l_{i_k}^\varepsilon l(Q_k)^{s - \varepsilon} \end{equation}
	%
	By assumption, if $m$ is large enough, then
	%
	\begin{equation} \label{equation992352}
		l_{i_k+1} = l_{i_k}/N_{i_k+1} \gtrsim l_{i_k}^{1 + \varepsilon/s}.
	\end{equation}
	%
	Putting \eqref{equation824} and \eqref{equation992352} together, we conclude that $H^s_{\DQ,m}(E) \lesssim_d \sum l(Q_k)^{s-\varepsilon}$.
\end{proof}

Finally, we consider computing whether we can establish that a measure is a Frostman measure dyadically. First, we recognize the utility of this approach from the perspective of a dyadic construction. Suppose we construct $E = \bigcap E_k$, where $\{ E_k \}$ is a decreasing sequence of sets, with $E_k$ the union of cubes in $\DQ_k^d$. Then we can define a finite measure supported on $E$ by setting $\mu([0,1]^d) = 1$, and for each $Q \in \DQ_{k+1}^d$, if $Q \subset E_{k+1}$, and $Q^*$ has $M$ children in $E_{k+1}$, we define $\mu(Q) = \mu(Q^*)/M$, and if $Q \not \subset E_{k+1}$, we set $\mu(Q) = 0$. We refer to this as the \emph{canonical measure} associated with the construction. For this measure, it is often easy to show that $\mu(Q) \lesssim l(Q)^s$ if $Q \in \DQ^d$. The conditions of Lemma \ref{definingsequenceminkowski} also are sufficient to infer from this estimate that $\mu$ is a Frostman measure of dimension $s - \varepsilon$ for all $\varepsilon > 0$.

\begin{theorem} \label{easyCoverTheorem}
	If, for all $\varepsilon > 0$, $N_k \leq (N_1 \dots N_{k-1})^\varepsilon$ for large enough $k$, and if $\mu$ is a Borel measure such that $\mu(Q) \lesssim l(Q)^s$ for each $Q \in \DQ_k^d$, then $\mu$ is a Frostman measure of dimension $s - \varepsilon$ for each $\varepsilon > 0$.
\end{theorem}
\begin{proof}
	Given $Q \in \DD^d_m$, find $k$ such that $l_{k+1} \leq 1/2^m \leq l_k$. Then $Q$ is covered by $O_d(1)$ cubes in $\DQ^d_k$, which shows
	%
	\[ \mu(Q) \lesssim_d l_k^s = [(l_k/l)^s l^\varepsilon ] l^{s - \varepsilon} \leq [l_k^{s + \varepsilon} / l_{k+1}^s] l^{s - \varepsilon} = [N_k^s l_k^\varepsilon] l^{s-\varepsilon}. \]
	%
	But by assumption, $N_k^s \lesssim_\varepsilon 1/l_k^\varepsilon$ for all $\varepsilon > 0$, so the proof is complete.
\end{proof}






%Our final method for interpolating requires extra knowledge of the dissection process, but enables us to choose the $l_k$ arbitrarily rapidly. The idea behind this is that there is an additional sequence of lengths $r_k$ with $l_k \leq r_k \leq l_{k-1}$. The difference between $r_k$ and $l_{k-1}$ is allowed to be arbitrary, but the decay rate between $l_k$ and $r_k$ is of polynomial-type, which enables us to use the covering methods of the previous section. In addition, we rely on a `uniform mass bound' between $r_k$ and $l_k$ to cover the remaining classes of intervals. Because we can take $r_k$ arbitrarily large relative to $l_k$, this renders any constants that occur in the construction to become immediately negligible. For two quantities $A$ and $B$, we will let $A \precsim_k B$ stand for an inequality with a hidden constant depending only on parameters with index smaller than $k$, i.e. $A \leq C(l_1, \dots, l_k, r_1,\dots,r_k) B$ for some constant $C(l_1, \dots, l_k, r_1, \dots, r_k)$ depending only on parameters with indices up to $k$.

\section{Beyond Hyperdyadics}

If we are to use a faster increasing sequence of branching factors than the last section guarantees, we therefore must exploit some extra property of our construction, which is not always present in general sets. Here, we rely on a \emph{uniform mass distribution} between scales. Given the uniformity assumption, the lengths can decrease as fast as desired. This decomposition requires an intermediary scale, so we write $N_k = K_k M_k$, and define $r_k = M_k l_k = l_{k-1}/K_k$, with $\DR_k^d$ are the dyadic cubes of length $r_k$.

TODO: ADD OTHER SITUATIONS WITH THE UNIFORM MASS DISTRIBUTION.

\begin{lemma}
	Let $\mu$ be a measure supported on a set $E$. Suppose that
    %
    \begin{enumerate}
    	\item \label{discreteBound} For any $Q \in \DQ_k^d$, $\mu(Q) \lesssim l_k^s$.
    	\item \label{controlledScale} For each $Q \in \DR_k^d$, $\#(\DQ_k^d(E \cap Q)) = O(1)$.
    	\item \label{uniformDist} For any $Q \in \DR_{k+1}^d$ with parent cube $Q^* \in \DQ_k^d$, $\mu(Q) \lesssim (r_{k+1}/l_k)^d \mu(Q^*)$.
    \end{enumerate}
	%
	Then $\mu$ is a Frostman measure of dimension $s$.
\end{lemma}
\begin{proof}
	Suppose $Q$ has length $l$, and that we can find $k$ with $r_{k+1} \leq l \leq l_k$. Then we can cover $Q$ by at most $O_d((l/r_{k+1})^d)$ cubes in $\DR_k^d$. By Properties \ref{discreteBound} and \ref{uniformDist}, each of these cubes has measure at most $O( (r_{k+1}/l_k)^d l_k^s)$, so we obtain that
    %
    \[ \mu(Q) \lesssim (l/r_{k+1})^d (r_{k+1}/l_k)^d l_k^s = l^d / l_k^{d-s} \lesssim l^s. \]
    %
    On the other hand, suppose there exists $k$ with $l_k \leq l \leq r_k$. Then we can cover $Q$ by $O_d(1)$ cubes in $\DR_k^d$, and for each such cube, Property \ref{uniformDist} shows there are $O(1)$ cubes in $\DQ_k^d$ which have mass. Thus
    %
    \[ \mu(Q) \lesssim l_k^s \leq l^s. \]
    %
    This addresses all cases, so $\mu$ is a Frostman measure of dimension $s$.
\end{proof}

%\begin{remark}
%    The condition $\mu_\beta(J) \lesssim_{N-1} (r_N/l_N) \mu_\beta(I)$ essentially means that the probability mass on a length $l_N$ interval $I$ is uniformly distributed over the length $r_N$ intervals it contains. This is what enables us to remove the discussion of the growth of the sequence $\beta$ over time from discussion.
%\end{remark}

%Since the construction is obtained as a limit of intervals, it is often possible to construct such a $\mu$ by the {\it mass distribution principle}. That is, we let $\mu$ denote the weak limit of the probability masses $\mu_n$, where $\mu_0$ is a uniform distribution over $\mu_0$, and $\mu_{n+1}$ is obtained from $\mu_n$ by distributing the mass $\mu_n(I)$ of each length $l_n$ interval $I$ contained in $X_n$ over the portion of $I$ that remains in $X_{n+1}$. The cumulative distribution functions of the $\mu_n$ uniformly converge, hence the $\mu_n$ converge weakly to some $\mu$, which satisfy $\mu(I) = \mu_n(I)$ for each interval $I$ as above. Because of this discreteness, it is most easy to establish a bound $\mu(I) \lesssim l_n^\alpha$ when $I \subset X_n$ is a length $l_n$ interval. Since any interval $I$ of length $l_n$ is contained within at least two such intervals (or is contained in other length $l_n$ intervals that $\mu$ assigns no mass to), we have the general bound $\mu(I) \lesssim l_n^\alpha$ for all intervals $I$ of length $l_n$. Hausdorff dimension is a local property of a set\footnote{If we define $\dim_{\mathbf{H}}(x) = \lim_{r \downarrow 0} \dim_{\mathbf{H}}(B_r(x) \cap X)$ then $\dim_{\mathbf{H}}(X) = \sup_{x \in X} \dim_{\mathbf{H}}(x)$.}, so it is natural to expect that we can obtain a general bound $\mu(I) \lesssim_\alpha|I|^\alpha$ given that one has established precisely the same estimate, but restricted to intervals $I$ with $|I| = l_N$. This section concerns itself with ways that we can establish this general bound, and thus prove that $\dim_{\mathbf{H}}(X) \geq \alpha$.

%\section{BLAH}

%The collection $\DQ^d$ forms a \emph{tree} under the partial ordering induced by inclusion, with a branching factor of $H$. We let $\DB^d$ denote the set of all branches of the tree $\DQ^d$. For each branch $\mathfrak{b} = \{ Q_k : k \geq 0 \}$, the set $\bigcap_{k \geq 0} Q_k$ contains a unique point, which induces a function $\pi: \DB^d \to \RR^d$. This function is obviously surjective, but unfortunately not injective. Nonetheless, for each $x \in \RR^d$, $\pi^{-1}(x)$ contains $O_d(1)$ points. This means that from the point of view of geometric measure theory, the spaces $\DB^d$ and $\RR^d$ are isomorphic\footnote{Formally, we can define a topology on $\DB^d$ as a subset of $\prod_{k \geq 0} \DQ_k^d$, and $\pi$ is continuous with respect to this topology. We can also define the Hausdorff measures $H^s$ on $\DB^d$ as limits of $H^s_m$, where
%
%\[ H^s_m(E) = \inf \left\{ \sum_{i = 1}^\infty l(Q_i)^s : Q_i \in \DQ_{k_i}^d, k_i \geq m, E \subset \bigcup \{ \mathfrak{b} \in \DB^d : \mathfrak{b}_k = Q \} \right\} \]
%
%Then for each value $s > 0$ the map $\pi$ is an isomorphism of $(\DB^d, H^s)$ and $(\RR^d, H^s)$.}. In particular, given any configuration $\C$ on $\RR^d$, we can define a configuration $\pi^{-1}(\C)$ on $\DB^d$ as
%
%\[ \pi^{-1}(\C) = \{ (\mathfrak{b_1}, \dots, \mathfrak{b}_n) : (\pi(\mathfrak{b_1}), \dots, \pi(\mathfrak{b_n})) \in \C \}. \]
%
%If we can find an $s$ dimensional set $E \subset \DB^d$ avoiding $\pi^{-1}(\C)$, then $\pi(E)$ avoids $\C$ and is $s$ dimensinoal. Conversely, if $E \subset \RR^d$ avoids $\C$ and is $s$ dimensional, then $\pi^{-1}(E)$ avoids $\pi^{-1}(\C)$ and is $s$ dimensional.

%The Dyadic model is useful, but if the lengths decrease too fast, the model fails to reflect the geometry of $\RR^n$ at all scales.

\section{Extras: Should we Hyperdyadic Covers?}

Nonetheless, it will be useful for us to know that we can `decompose' a set with a prescribed Hausdorff dimension hyperdyadically. We say a sequence of sets $\{ E_k \}$ is a \emph{strong cover} of a set $E$ if $E \subset \limsup E_k$, or equivalently, if every $x \in E$ lies in infinitely many of the sets $E_k$.

%Fix two parameters $\delta > 0$ and $\varepsilon > 0$. Given two numbers $A = A_{\delta \varepsilon}$ and $B = B_{\delta \varepsilon}$, we say $A \lessapprox B$ if there exists constants $C_\varepsilon$, and $C$ such that $A \leq C_\varepsilon \delta^{-C\varepsilon} B$. We say $A \approx B$ if $A \lessapprox B$ and $B \lessapprox A$. We say a set $E$ is \emph{$\delta$ discretized} if it is a union of dyadic cubes with sidelength $\approx \delta$. We say a set $E$ is a \emph{$(\delta,\alpha)$ set} if it is $\delta$ discretized, and for any dyadic cube $I$ with $\delta \leq l(I) \leq 1$, $|E \cap I| \lessapprox \delta^{d-\alpha} l(I)^\alpha$. Thus $E$ is \emph{roughly} a $\delta$ thickening of an $\alpha$ dimensional set. A set $E$ is \emph{strongly covered} by a family of sets $\{ U_i \}$ if $E \subset \limsup_{i \to \infty} U_i$. We consider a fixed hyperdyadic sequence $l_k = 2^{- \lfloor (1 + \varepsilon)^k \rfloor}$.

%\begin{lemma}
%	If $C$ is sufficiently large, and for each $\varepsilon$, there is a $(\delta, \alpha - C \varepsilon)$ set $X_\delta$ for each hyperdyadic $\delta$ such that the $X_\delta$ strongly cover $X$, then $\dim(X) \leq \alpha$.
%\end{lemma}
%\begin{proof}
%	Let $X_\delta = \bigcup I_i$, where $\{ I_i \}$ are disjoint dyadic cubes such that $l(I_i) \approx \delta$, and with $|X_\delta| \lessapprox \delta^{d-\alpha + C\varepsilon}$. Then
	%
%	\[ |X_\delta(\delta/2)| \leq \sum (l(I_i) + \delta/2)^d \lessapprox \sum l(I_i)^d = |X_\delta| \lessapprox \delta^{d - \alpha + C\varepsilon}. \]
	%
%	A volumetric argument then guarantees that $N(X_\delta,\delta) \lessapprox \delta^{-\alpha + C\varepsilon}$, and so
	%
%	\[ H^\alpha_\infty(X_\delta) \leq N(X_\delta,\delta) \delta^\alpha \lessapprox \delta^{C\varepsilon}. \]
	%
%	Thus there is $C_\varepsilon$ and $C_0$ such that $H^\alpha_\delta(X_\delta) \leq C_\varepsilon \delta^{(C - C_0) \varepsilon}$. Since $C_0$ does not depend on $C$, if we set $C > C_0$, then
	%
%	\[ \sum_{i = 1}^\infty H^\alpha_\infty(X_\delta) < \infty, \]
	%
%	and so $H^\alpha_\infty(X) = 0$.
%\end{proof}

\begin{theorem}
	Suppose $E \subset [0,1]^d$ is a set with $\dim(E) \leq \alpha$. Fix $\varepsilon > 0$, and write $l_k = 2^{-\lfloor (1 + \varepsilon)^k \rfloor}$. Then there exists a strong cover of $E$ by sets $\{ E_k \}$, where $E_k$ is a union of $O_\varepsilon(l_k^{-\alpha})$ hyperdyadic cubes of sidelength $l_k$.
\end{theorem}
\begin{proof}
	For each hyperdyadic number $l_k$, we can find a collection of cubes $\{ Q_{k,i} \}$ covering $E$ with $l(Q_{k,i}) \leq l_k$ for all $i$, and
	%
	\[ \sum_{i = 1}^\infty l(Q_{k,i})^{\alpha + C\varepsilon} \lesssim 1. \]
	%
	For each $k$ and $i$, find $j_{k,i}$ such that $l_{j_{k,i} + 1} \leq l(Q_{k,i}) \leq l_{j_{k,i}}$. Note $l_{j_{k,i} + 1} \lesssim l_{j_{k,i}}^{1 + \varepsilon}$, so
	%
	\begin{align*}
		\sum_{i = 1}^\infty l_{j_{k,i}}^{\alpha + C\varepsilon} &= \sum_{i = 1}^\infty l(Q_{k,i})^{\alpha + C \varepsilon} (l_{j_{k,i}} / l(Q_{k,i}))^{\alpha + C\varepsilon}\\
		&\lesssim \sum_{i = 1}^\infty l(Q_{k,i})^{\alpha + C\varepsilon} l_{j_{k,i}}^{-\varepsilon(\alpha + C\varepsilon)} \lesssim \sum_{i = 1}^\infty l(Q_{k,i})^{\alpha + (C - \alpha + \varepsilon) \varepsilon}.
	\end{align*}
	%
	Thus, replacing $C$ with a slightly smaller constant, and replacing $Q_{k,i}$ with the $O_d(1)$ cubes in $\DQ_{j_{k,i}}^d(Q_{k,i})$, we may assume all cubes in the decomposition are hyperdyadic. We let $Y_{k_1,k_2}$ to be the union of all cubes in the decomposition $\{ Q_{k_1,i} \}$ with hyperdyadic length $l_{k_2}$. Note that $Y_{k_1,k_2}$ is the union of $O((1/l_{k_2})^{\alpha + C\varepsilon})$ cubes. We let $\mathbf{Q}_{k_1,k_2}$ be the collection of hyperdyadic cubes covering $Y_{k_1,k_2}$ which minimize the quantity
	%
	\[ \sum_{Q \in \mathbf{Q}_{k_1,k_2}} l(Q)^\alpha \]
	%
	and such that $l(Q) \geq l_{k_2}$ for each $Q$. Then clearly
	%
	\[ \sum_{Q \in \mathbf{Q}_{k_1,k_2}} l(Q)^\alpha \lesssim l_{k_2}^{- C\varepsilon}. \]
	%
	In particular, this means $l(Q) \lesssim l_{k_2}^{-C\varepsilon/\alpha}$ for each $Q \in \mathbf{Q}_{k_1,k_2}$. Furthermore, for each hyperdyadic $Q_0$ with $l(Q_0) \geq l_{k_2}$,
	%
	\[ \sum_{Q \subset Q_0} l(Q)^\alpha \leq l(Q_0)^\alpha. \]
	%
	Now we define $E_k$ as the union of all cubes in the sets $\mathbf{Q}_{k_1,k_2}$ which have sidelength $l_k$. Then clearly $E_k$ is $l_k$ discretized. Since $E_k$ only contains cubes from $\mathbf{Q}_{k_1,k_2}$ where $k_2 \geq k_1$, and $l_{k_1} \lesssim l_{k_2}^{-C\varepsilon/\alpha}$, this means that there are only $O(\log(1/l_k)^2) = O(1 + \varepsilon)$ such choices of $(k_1,k_2)$. But this means that for $Q \in \mathbf{Q}_{k_1,k_2}$, and any hyperdyadic cube $Q_0$,
	%
	\[ \sum_{Q \subset Q_0} l(Q)^\alpha \lesssim (1 + \varepsilon)^2 l(Q_0)^\alpha \]
	%
	In particular,
	%
	\[ |E_k| = \sum l(Q)^d = l_k^{d - \alpha} \sum l(Q)^\alpha \lesssim (1 + \varepsilon)^2 l_k^{d-\alpha}, \]
	%
	which implies that $E_k$ is the union of at most $(1 + \varepsilon)^2 l_k^{-\alpha}$ cubes.
\end{proof}