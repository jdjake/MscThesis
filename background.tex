%% The following is a directive for TeXShop to indicate the main file
%%!TEX root = diss.tex

\chapter{Background}
\label{ch:Background}

This thesis discusses methods to form large sets avoiding patterns. First, we give a precise definition by what we mean by a pattern, and what it means to avoid a pattern. We consider an ambient set $\AAA$, often discrete, or equipped with some topological or metric structure. It's {\it $n$-point configuration space} is the set of distinct tuples of $n$ points in $\AAA$, i.e.
%
\[ \Config^n(\AAA) = \{ (x_1, \dots, x_n) \in X^n: x_i \neq x_j\ \text{if $i \neq j$} \}. \]
%
The {\it configuration space} of $\AAA$ is $\Config(\AAA) = \bigcup_{n = 1}^\infty \Config^n(\AAA)$. A {\it pattern}, or {\it configuration}, on $\AAA$ is a subset of $\Config(X)$, and we say a subset $Y$ of $X$ avoids a configuration $\C$ if $\Config(Y)$ is disjoint from $\C$. An {\it $n$ point configuration} on $X$ will be a configuration $\C$ which is a subset of $\Config^n(X)$.

\begin{example}[Isoceles Triangle Configuration]
	Consider the problem of finding a set avoiding the vertices of an isoceles triangle in the plane. Set
	%
	\[ \C = \{ (x_1, x_2, x_3) \in \Config^3(\RR^2) : |x_1-x_2| = |x_1-x_3| \}. \]
	%
	Then $\C$ is a 3-point configuration, and a set $X \subset \RR^2$ avoids $\C$ if and only if it contains no vertices of an isoceles triangle. Notice that $|x_1 - x_2| = |x_1 - x_3|$ holds if and only if $|x_1 - x_2|^2 = |x_1 - x_3|^2$, which is an algebraic equation in the coordinates of $x_1,x_2$, and $x_3$. Thus $\C$ is an algebraic hypersurface in $\RR^6$.
\end{example}

\begin{example}[General Position Configuration]
	Suppose we wish to find a subset of $\RR^d$ such that every collection of $k+1$ points in the set, for $k < d$, lies in `general position', i.e. they do not lie in a $k$ dimensional hyperplane. Set
	%
	\[ \C^k = \{ (x_0, x_1, \dots, x_k) \in \Config^{k+1}(\RR^d): x_1-x_0, \dots, x_k - x_0\ \text{are linearly dependant} \}. \]
	%
	and then consider the configuration $\C = \bigcup_{k = 1}^{d-1} \C^k$. A set $X$ avoids $\C$ if and only if all of it's points lie in general position. Notice that
	%
	\[ \C^k = \bigcup \left\{ \text{span}(y_1, \dots, y_k) \times \{ y \} : y = (y_1, \dots, y_k) \in \Config^k(\RR^d) \right\} \cap \Config^{k+1}(\RR^d). \]
	%
	so each $\C^k$ is essentially a union of $k$ dimensional hyperplanes.
\end{example}

Even though our problem formulation assumes configurations are formed by distinct sets of points, one can still formulate avoidance problems involving repeated points in our framework, because an instance of a configuration involving $n$ points which may contain repetitions can be seen as an instance of a configuration involving fewer than $n$ distinct points.

\begin{example}[Sum Set Configuration]
	Let $G$ be an abelian group, and fix $Y \subset G$. Set
	%
	\[ \C^1 = \{ g \in \Config^1(G): 2g \in Y \} \quad \text{and} \quad \C^2 = \{ (g_1,g_2) \in \Config^2(G): g_1 + g_2 \in Y \}. \]
	%
	Then set $\C = \C^1 \cup \C^2$. A set $X \subset G$ avoids $\C$ if and only if $(X + X) \cap Y = \emptyset$.
\end{example}

Our main focus in this thesis is on the {\it pattern avoidance problem}: Given a configuration $\C$ on $\AAA$, how large can $X \subset \AAA$ be avoiding $\C$. If $\AAA$ is discrete, i.e. finite, or a discrete limit of finite sets $\AAA_n$, the goal is to find $X$ with large cardinality, or such that $X \cap \AAA_n$ has large cardinality asymptotically in $n$. If $\AAA = \RR^d$, but $\C$ is a sufficiently discrete configuration, then a satisfactory goal is to find $X$ with large Lebesgue measure avoiding $\C$. But in this thesis we establish methods for avoiding non-discrete configurations $\C$, i.e. those for which $X^m \cap \C^m$ are dense in $X^m$, but taking inspiration from methods in the discrete setting. The next section shows that Lebesgue measure completely fails to measure the degree of success for a solution to the pattern avoidance problems, but provides an alternate measurement which does succeed to establish this point.

\section{Fractal Dimension}

The Lebesgue measure is not the correct measurement for how large a pattern-avoiding set for non-discrete patterns. This is because for most of these patterns, every pattern-avoiding set has measure zero. One intuition as to why this is true is that a set with positive Lebesgue measure behaves in many respects like an open set, and an open set certainly intersects a dense set somewhere. The rigorous instance of this phenomenon we use is the Lebesgue density theorem. It's proof takes us too far afield into differentiation theory, so we merely state the result without proof. The intuitive idea of the result is that a set of positive Lebesgue measure locally contains a large percentage of it's surrounding points.

\begin{theorem}[Lebesgue Density Theorem]
	Let $E \subset \RR^d$ have positive Lebesgue measure. Then for almost every point $x \in E$,
	%
	\[ \lim_{r \to 0} \frac{|E \cap B_r(x)|}{|B_r(x)|} = 1. \]
\end{theorem}

Under mild non-discreteness conditions, which are certainly satisfied by the example configurations given in the last section, no set with positive Lebesgue measure can avoid a configuration.

\begin{theorem}
	Let $\C$ be an $n$-point configuration on $\RR^d$ such that
	%
	\begin{enumerate}
		\item \emph{Translation Invariance}: For any $b \in \RR^d$, $\C + b \subset \C$.
		\item \emph{Non-Discrete}: For any $\varepsilon > 0$, there is an instance of the configuration $(c_1, \dots, c_n) \in C$ such that $\diam \{ c_1, \dots, c_n \} \leq \varepsilon$.
	\end{enumerate}
	%
	Then no set with positive Lebesgue measure avoids $\C$.
\end{theorem}
\begin{proof}
	Let $X \subset \RR^d$ have positive Lebesgue measure. Applying the Lebesgue density theorem, we find a point $x_0 \in X$ such that
	%
	\[ \lim_{r \to 0} \frac{|X \cap B_r(x_0)|}{|B_r(x_0)|} = 1 \]
	%
	We fix $r_0$ such that $|X \cap B_{r_0}(x_0)| \geq (1 - \varepsilon)|B_{r_0}(x)|$. Let $C = (c_1, \dots, c_n) \in \C$ be an instance of the configuration such that $\diam \{ c_1, \dots, c_n \} \leq \varepsilon r_0$. For each $p \in B_{r_0}(x_0)$, let $C(p) = (c_1(p), \dots, c_n(p)) \in \C$, where $c_i(p) = p + (c_i - c_1)$. Then a union bound gives
	%
	\begin{align*}
		\left| \bigcup_{i = 1}^n \{ p \in B_{r_0/2}(x_0) : c_i(p) \not \in X \} \right| &\leq \sum_{i = 1}^n |B_{r_0/2}(x_0) \cap (X + (c_1 - c_i))^c|\\
		&= \sum_{i = 1}^n |B_{r_0/2}(x_0)| - |B_{r_0/2}(x_0 + c_i - c_1) \cap X|\\
		&\leq n \left( |B_{r_0/2}(x_0)| - |B_{r_0(1/2 - \varepsilon)}(x_0) \cap X| \right)\\
		&\leq n \left[ (r_0/2)^d - (1 - \varepsilon)(1/2 - \varepsilon)^d r_0^d \right] \\
		&\leq n \left( 1 - (1 - \varepsilon) (1 - 2d\varepsilon) \right) (r_0/2)^d\\
		&\leq \varepsilon n (2d + 1) (r_0/2)^d
	\end{align*}
	%
	If $\varepsilon < 1/n(2d + 1)$, we conclude that
	%
	\begin{align*}
		\left| \bigcap_{i = 1}^n \{ p \in B_{r_0/2}(x_0) : c_i(p) \in X \} \right| &= |B_{r_0/2}(x_0)| - \left| \bigcap_{i = 1}^n \{ p \in B_{r_0/2}(x_0) : c_i(p) \not \in X \} \right|\\
		&\geq (r_0/2)^d - \varepsilon n(2d + 1) (r_0/2)^d > 0
	\end{align*}
	%
	Thus there exists $p$ such that $C(p) \in \Config(X)$, and so $X$ does not avoid $\C$.
\end{proof}

Fortunately, we have a second order notion of size, which is able to distinguish between sets of measure zero, known as {\it fractional dimension}.  Intuitively, the fractional dimension provides a measure of the local density of a set, and so we view a set which is more dense as `larger', for the purposes of a pattern avoidance problem. There are two definitions of fractional dimension we use in this thesis: Minkowski dimension and Hausdorff dimension. The main difference between the two is that Minkowski dimension measures relative density at a single scale, whereas Hausdorff dimension measures relative density at various small scales.

We begin by discussing the Minkowski dimension, which is the easiest of the two dimension to define. If $E$ is a bounded set in $\RR^d$, then we can consider the {\it delta thickening} $E_\delta = \{ x: d(x,E) < \delta \}$. We define the {\it upper} and {\it lower} Minkowski dimension as
%
\[ \overline{\dim}_M(E) = \limsup_{\delta \to 0} \left( d - \frac{\log|E_\delta|}{\log \delta} \right)\ \ \ \ \ \underline{\dim}_M(E) = \liminf_{\delta \to 0} \left( d - \frac{\log|E_\delta|}{\log \delta} \right) \]
%
If $\overline{\dim}_M(E) = \underline{\dim}_M(E)$, then we refer to this common quantity as the {\it Minkowski dimension} of $E$, denote $\dim_M(E)$. This means that as $\delta \to 0$, if $\dim_M(E) = \alpha$, then $|E_\delta| = \delta^{n - \alpha + o(1)}$. In particular, every set with positive Lebesgue measure has Minkowski dimension $d$, so Minkowski dimension is really only interesting for sets with Lebesgue measure zero. We can extend the lower and upper Minkowski dimension to unbounded sets $E$ by considering the supremum and infinum of the dimensions of all bounded subsets of $E$.

%\begin{example}
%	If $E = B^k \times \{ 0 \}^{n-k}$, where $B^k$ is the $k$ dimensional unit ball, then
	%
%	\[ B^k \times \delta B^{n-k} \subset E_\delta \subset (1 + \delta)B^k \times \delta B^{n-k} \]
	%
%	which shows that
	%
%	\[ \delta^{n-k} \lesssim |E_\delta| \lesssim (1 + \delta)^k \delta^{n-k} \]
	%
%	Thus $\dim_M(E) = k$.
%\end{example}

\begin{example}
	Let
	%
	\[ C = \left\{ \sum_{i = 1}^\infty a_i/4^i : a_i \in \{ 0, 3 \} \right\} \]
	%
	If $1/4^{N+1} \leq \delta \leq 1/4^N$, then
	%
	\[ \left\{ \sum_{i = 1}^\infty a_i/4^i : a_1, \dots, a_{N+1} \in \{ 0, 3 \} \right\} \subset C_\delta \subset \left\{ \sum_{i = 1}^\infty a_i/4^i : a_1, \dots, a_N \in \{ 0, 3 \} \right\} \]
	%
	The former set has volume $2^{N+1}/4^{N+1} = 1/2^{N+1} \geq \delta^{1/2}$, whereas the latter set has volume $2^N/4^N = 1/2^N \leq (2\delta)^{1/2}$. Thus $\log |C_\delta| = \log \delta/ 2 + O(1)$, and so
	%
	\[ \minkdim(C) = \lim_{\delta \to 0} \left( 1 - \frac{\log |C_\delta|}{\log \delta} \right) = 1 - 1/2 = 1/2 \]
	%
	So $C$ has Minkowski dimension $1/2$.
\end{example}

There are a few things to notice about this calculation. First, we performed an upper and lower bound on powers of $1/4^k$, and then used this to obtain bounds at all scales. And indeed, if we fix $M$, and consider $1/M^{k+1} \leq \delta \leq 1/M^k$, then as $k \to \infty$, $\log \delta \sim \log(1/M^k)$, and combined with the fact that $C_{1/M^{k+1}} \subset C_\delta \subset C_{1/M^k}$, we find
%
\[ \frac{|C_{1/M_k}|}{\log(1/M_k)} \sim \frac{|C_{1/M^k}|}{\log(\delta)} \leq \frac{|C_\delta|}{\log(\delta)} \leq \frac{|C_{1/M^{k+1}}|}{\log(\delta)} \sim \frac{|C_{1/M^{k+1}}|}{\log(1/M_{k+1})} \]
%
Thus for any set $E$ and any integer $M$,
%
\[ \lowminkdim(E) = \liminf_{n \to \infty} \frac{|C_{1/M^k}|}{\log(1/M^k)}\quad\text{and}\quad \upminkdim(E) = \limsup_{n \to \infty} \frac{|C_{1/M^k}|}{\log(1/M^k)} \]
%
The second point is that we understood the dimension of $C$ via a `Cantor-type' decomposition. Indeed, the set $C$ can be understood as the limit of a family of sets which are simple unions of intervals. If we set
%
\[ C_k = \left\{ \sum_{i = 1}^\infty a_i/4^i : a_1, \dots, a_{N+1} \in \{ 0, 3 \} \right\}. \]
%
Then $\{ C_k \}$ is a nested family of sets, with $C = \lim C_k$. If for an index $k$, we set
%
\[ \B^d_l = \{ [a_1, a_1 + l] \times \cdots \times [a_d, a_d + l] : a_k \in l \cdot \ZZ \}. \]
%
then $C_k$ is a union of $2^k$ cubes in $\B^d_{4^k}$, and
%
\[ \frac{\log(2^k)}{\log(4^k)} = \log_4(2) = 1/2. \]
%
One can also calculate the Minkowski dimension by counting the number of cubes in $\B^d_l$ intersecting the set. For a set $E$, set $\B^d_l(E)$ to be the set of all cubes in $\B^d_l$ intersecting $E$, i.e. $\B^d_l(E) = \{ I \in \B^d_l: I \cap E \neq \emptyset \}$.

\begin{lemma}
	If $E$ is a bounded set in $\RR^d$ and $M$ is an integer, then
	%
	\[ \lowminkdim(E) = \liminf_{l \to 0} \frac{\# \B^d_l(E)}{\log(1/l)}\quad\text{and}\quad \upminkdim(E) = \limsup_{l \to 0} \frac{\# \B^d_l(E)}{\log(1/l)} \]
	%
	where the limit is taken over lengths $l = 1/M^k$.
\end{lemma}
\begin{proof}
	Let $l = 1/M^k$. For each cube $I \in \B^d_l$, the $l$ thickening $I_l$ is contained in $3^d$ cubes in $\B^d_l$. Conversely, if $I \in \B^d_l(E)$, then $I \subset E_{d^{1/2} l}$, so $I \subset E_{M^{k_0} l}$ for $M^{k_0} \geq d^{1/2}$. Thus
	%
	\[ |E_l| \leq 3^d \# \B^d_l(E) l^d\quad\text{and}\quad|E_{M^{k_0} l}| \geq \# \B^d_l(E) l^d \]
	%
	So as $l \to 0$,
	%
	\[ d - \frac{\log |E_l|}{\log l} = \frac{\log(|E_l| l^{-d})}{\log(1/l)} \leq \frac{\log(3^d \# \B^d_l(E))}{\log(1/l)} = \frac{\# \B^d_l(E)}{\log(1/l)} + o_d(1) \]
	%
	and
	%
	\[ d - \frac{\log|E_{M^{k_0} l} |}{\log(M^{k_0} l)} = \frac{\log(|E_{M^{k_0} l}| (d^{1/2} l)^{-d})}{\log(1/l)} \geq o_d(1) + \frac{\# \B^d_l(E)}{\log(1/l)} \]
	%
	Taking limits completes the proof.
\end{proof}

We will often use `Cantor-type' constructions to form pattern avoiding sets. And Lemma 1 will be crucial either for calculating the Minkowski dimension of these constructions. It is especially useful when trying to work at discrete scales, because we can view $\bigcup \B^d_l(E)$ as a `discretization' of a set $E$ at the scale $l$.

\begin{example}
	We will often consider sets whose dimension behaves differently at various scales. This often occurs when performing multi-scale constructions where the scales in the construction decay inverse superexponentially. A toy example of this phenomenon can be obtained by modifying the last example slightly so that the construction of the Cantor set behaves differently at various scales. We fix an increasing sequence of integers $\{ N_k \}$, with $N_0 = 0$, and consider
	%
	\[ C = \left\{ \sum_{i = 1}^\infty a_i/4^i : a_i \in \{ 0, 3 \}\ \text{if there is $k \geq 0$ such that}\ N_{2k} \leq i \leq N_{2k+1} \right\} \]
	%
	Then $C = \lim C_n$, where
	%
	\[ C_n = \left\{ \sum_{i = 1}^\infty a_i/4^i : a_i \in \{ 0, 3 \}\ \text{if there is $k \leq n$ such that}\ N_{2k} \leq i \leq N_{2k+1} \right\} \]
	%
	Notice that $C_n$ is the union of
	%
	\[ 2^{N_1 - N_0} 4^{N_2 - N_1} 2^{N_3 - N_2} \dots 4^{N_{2n} - N_{2n-1}} = 2^{-N_1+N_2-N_3+\dots - N_{2n-1} + 2N_{2n}} \geq 2^{2N_{2n} - N_{2n-1}} \]
	% -N_1 + N_2 - N_3 + 2N_4
	sidelength $l_n$ cubes, where $l_n = 1/4^{N_{2n}}$. Each of these cubes intersects $C$, so
	%
	\[ \frac{\log \# \B^1_{l_n}(C)}{\log(1/l_n)} \geq \frac{2N_{2n} - N_{2n-1}}{2N_{2n}} = 1 - \frac{N_{2n-1}}{2N_{2n}} \]
	%
	Provided that $N_{2n-1}/N_{2n} = o(1)$, which occurs if the values $N_k$ increase superexponentially, i.e. if $N_k = 2^{k^2}$, we conclude that $\upminkdim(C) = 1$. On the other hand, $C_n$ is also the union of
	%
	\[ 2^{N_1 - N_0} 4^{N_2 - N_1} 2^{N_3 - N_2} \dots 4^{N_{2n} - N_{2n-1}} 2^{N_{2n+1} - N_{2n}} = 2^{- N_1 + N_2 - \dots + N_{2n} + N_{2n+1}} \leq 2^{N_{2n} + N_{2n+1}}. \]
	% -N_1 + N_2 - N_3 + N_4 + N_5
	%
	sidelength $r_n$ cubes, where $r_n = 1/4^{N_{2n+1}}$. Thus
	%
	\[ \frac{\log \# \B^1_{l_n}(C)}{\log(1/l_n)} \leq \frac{N_{2n} + N_{2n+1}}{2N_{2n+1}} = 1/2 + \frac{N_{2n}}{2N_{2n+1}} \]
	%
	Again, if $N_{2n}/N_{2n+1} = o(1)$, then $\lowminkdim(C) \leq 1/2$. One can fairly easily check that the values $r_n$ are the `worst case' scales, so that $\lowminkdim(C) = 1/2$. Thus the set $C$ `looks' half dimension between $l_n$ and $r_n$, for each $n$, but `looks' full dimensional between the scales $r_n$ and $l_{n+1}$.
\end{example}

Hausdorff dimension is a more stable version of fractal dimension which is obtained by finding a canonical `$s$ dimensional measure' $H^s$ on $\RR^d$ for $s \in [0,\infty)$, and then setting the dimension of $E$ to be the supremum of the values $s$ such that $H^s(E) < \infty$. A naive way the Hausdorff measure to construct is to assign a mass $r^s$ to each radius $r$ ball in $\mathbf{R}^n$, and then define
%
\[ H^{s,\infty}(E) = \inf \left\{ \sum r_k^s : E \subset \bigcup B(x_k,r_k) \right\} \]
%
This is an outer measure, and so Caratheodory's extension theorem gives a $\sigma$ algebra of measurable sets. Unfortunately, most sets are not measurable with respect to this $\sigma$ algebra. For instance, take $s = 1/2$, and $E = (a,b)$. On one hand, $H^{s,\infty}(E) \leq [(b-a)/2]^{1/2}$. On the other hand, if $(a,b)$ is covered by balls $B(x_k,r_k)$, then $\sum 2r_k \geq b - a$, so applying the concavity of $x \mapsto x^{1/2}$, we conclude
%
\[ \sum r_k^{1/2} \geq \left( \sum r_k \right)^{1/2} \geq \left( \frac{b - a}{2} \right)^{1/2} \]
%
Thus $H^{s,\infty}(E) = [(b-a)/2]^{1/2}$. But now we see that the additivity property begins to breakdown, since $H^{1/2,\infty}[0,1] = 2^{-1/2}$, whereas $H^{1/2,\infty}[0,1/2] = H^{1/2,\infty}[1/2,1] = 1/2$, and so $H^{1/2,\infty}[0,1] < H^{1/2,\infty}[0,1/2] + H^{1/2,\infty}[1/2,1]$. The reason for this is that $[0,1]$ is most efficiently covered by one large ball, rather than covering $[0,1/2]$ and $[1/2,1]$ separately. This is fixed by limiting the Hausdorff measure to be the value of the most efficient cover by arbitrarily small balls.

For a subset $E$ of Euclidean space, we define
%
\[ H_\delta^s(E) = \inf \left\{ \sum_{n = 1}^\infty \text{diam}(B_n)^s : E \subset \bigcup_{n = 1}^\infty B_n, \text{diam}(B_n) \leq \delta \right\} \]
%
We then define $H^s(E) = \lim_{\delta \to 0} H_\delta^s(E)$. Then $H^s$ is an exterior measure, and $H^s(E \cup F) = H^s(E) + H^s(F)$ if $d(E,F) > 0$. Thus all Borel sets are measurable with respect to $H^s$, which is certainly more satisfactory than the last definition.

\begin{example}
	Let $s = 0$. Then $H_\delta^0(E) = N_\delta^{\text{Ext}}(E)$, which tends to $\infty$ as $\delta \to 0$ unless $E$ is finite, and then $H_\delta^0(E) \to \# E$. Thus $H^0$ is just the counting measure.
\end{example}

\begin{example}
	Let $s = n$. If $E$ has Lebesgue measure zero, then for any $\varepsilon > 0$, there exists countable many balls $B(x_k,r_k)$ covering $E$ with $\sum r_k^n < \varepsilon$. Then $r_k < \varepsilon^{1/n}$, so $H^n_{\varepsilon^{1/n}}(E) < \varepsilon$. Letting $\varepsilon \to 0$, we conclude $H^n(E) = 0$. Thus $H^n$ is absolutely continuous with respect to the Lebesgue measure. The measure $H^n$ is translation invariant, so $H^n$ is actually a constant multiple of the Lebesgue measure. We let the constant multiple be defined $1/\omega_n$. The value $\omega_n$ can be defined as the volume of a unit ball in $\mathbf{R}^n$, since $H^n(B) = 1$ if $B$ is a unit ball.
\end{example}

The same argument shows that if $V$ is an $m$ dimensional subspace of $\mathbf{R}^n$, then $H^m$, restricted to subsets of $V$, is a constant multiple of the $m$ dimensional Lebesgue measure on $V$. More generally, $H^m$ measures the $m$ dimensional surface area of smooth, $m$ dimensional submanifolds of $\mathbf{R}^n$.

\begin{theorem}
	Let $U$ be an open subset of $\mathbf{R}^d$, and let $\phi: U \to \mathbf{R}^n$ be a smooth immersion. Then for any compact set $E$,
	%
	\[ H^d(\phi(E)) \propto \frac{1}{\omega_d} \int_E J(x)\; dx \]
	%
	where $J(x)$ is the square root of the sums of squares of the $d \times d$ minors of $D\phi(x)$.
\end{theorem}
\begin{proof}
	We may cover $E$ by finitely many open sets $U_1, \dots, U_N$, together with coordinate charts $y_1, \dots, y_N$ such that $(y_k \circ \phi)(x) = (x,f_k(x))$ for some smooth $f_k$, and fix $J_k$ such that for any $x \in U_k$, $|J(x) - J_k| < \varepsilon$. TODO: PROVE REST OF THEOREM.
\end{proof}

\begin{lemma}
	If $t < s$ and $H^t(E) < \infty$, $H^s(E) = 0$, and if $H^s(E) = \infty$, $H^t(E) = \infty$.
\end{lemma}
\begin{proof}
	If, for any cover of $E$ by balls $B(x_k,r_k)$, $\sum r_k^t \leq A$, and $r_k \leq \delta$, then $\sum r_k^s \leq \sum r_k^{s-t} r_k^t \leq \delta^{s-t} A$. Thus $H^s_\delta(E) \leq \delta^{s-t} A $, and taking $\delta \to 0$, we conclude $H^s(E) = 0$. The latter point is just proved by taking contrapositives.
\end{proof}

Thus given any Borel set $E$, there is $s$ such that $H^{s_0}(E) = 0$ for $s_0 < s$, and $H^{s_1}(E) = \infty$ for $s_1 > s$. We refer to $s$ as the Hausdorff dimension of $E$, denoted $\dim_H(E)$.

\begin{example}
	Consider $S = \{ (x,\sin(1/x)) : 0 < x \leq 1 \}$. Then for each $\delta > 0$, the set $S \cap [\delta,1] \times \mathbf{R}$ is a smooth curve, and therefore has Hausdorff dimension $1$. Thus for any $\varepsilon > 0$, $H^{1 + \varepsilon}(S \cap [\delta,1] \times \mathbf{R}) = 0$. But then taking limits as $\delta \to 0$, we conclude $H^{1+\varepsilon}(S) = 0$. Since $H^1(S) > 0$, this shows $S$ has Hausdorff dimension 1. Compare this to the Minkowski dimension $3/2$ result we obtained previously.
\end{example}

An easy way to compare the approaches to fractal dimension given by Minkowski and Hausdorff dimension is that Minkowski dimension measures the efficiency of covers of a set at a fixed scale, whereas Hausdorff dimension measures the efficiency of covers of a set at various, small scales.



% TODO: Explain how cantor set makes a continuous problem into a sequence of discrete problems.

\section{Branching Processes}

\section{Rusza: Difference Sets Without Squares}

In this section, we describe the work of Ruzsa on the discrete squarefree difference problem, which provides inspiration for our speculated results for the squarefree subset problem in the continuous setting. If $X$ and $Y$ are subsets of integers, we shall let $X \pm Y = \{ x \pm y: x \in X, y \in Y, x \pm y > 0 \}$ denote the sums and difference of the set. The {\it differences} of a set $X$ are elements of $X - X$, and so the squarefree difference set problem asks to consider how large a subset of the integers can be, whose differences do not contain the square of any positive integer. We let $D(N)$ denote the maximum number of integers which can be selected from $[1,N]$ whose differences do not contain a square.

\begin{example}
    The set $X = \{ 1, 3, 6, 8 \}$ is squarefree, because $X - X = \{ 2, 3, 5, 7 \}$, and none of these elements are perfect squares. On the other hand, $\{ 1, 3, 5 \}$ is not a squarefree subset, because $5 - 1 = 4$ is a perfect square.
\end{example}

There are a few tricks to constructing large subsets of integers avoiding squares. If $p$ is prime, then $p \mathbf{Z} \cap [1,p^2)$ avoids squares, because the difference of two numbers must be divisible by $p$, but not by $p^2$. If $N = p^2$, this gives a set with $N^{1/2}$ elements. However, we can do just as well without using any properties of the set of squares except for their sparsity, by greedily applying a sieve. We start by writing out a large list of integers $1,2,3,4,\dots,N$. Then, while we still have numbers to pick, we greedily select the smallest number $x_*$ we haven't crossed out of the list, add it to our set $X$ of squarefree numbers, and then cross out all integers $y$ such that $y - x_*$ is a positive square. Thus we cross out $x_*$, $x_* + 1$, $x_* + 4$, and so on, all the way up to $x_* + m^2$, where $m$ is the largest integer with $x_* + m^2 \leq N$. This implies $m \leq \sqrt{N - x_*} \leq \sqrt{N-1}$, hence we cross out at most $\sqrt{N-1} + 1$ integers whenever with add a new element $x_*$ to $X$. When the algorithm terminates, all integers must be crossed out, and if the algorithm runs $n$ iterations, a union bound gives that we cross out at most $n[\sqrt{N-1} + 1]$ integers, hence $n[\sqrt{N-1} + 1] \geq N$. It follows that the set $X$ we end up with contains $\Omega(N^{1/2})$ elements. What's more, this algorithm generates an increasing family of squarefree subsets of the integers as $n$ increases, so we may take the union of these subsets over all $N$ to find an infinite squarefree subset $X$ with $|X \cap [1,N]| = \Omega(\sqrt{N})$ for all $N$.

In 1978, S\'{a}rk\"{o}zy proved an upper bound on the size of squarefree subsets of the integers, showing $D(N) = O(N (\log N)^{-1/3 + \varepsilon})$ for every $\varepsilon > 0$. In particular, this proves a conjecture of Lov\'{a}sz that every infinite squarefree subset has density zero, because if $X$ is any infinite squarefree subset, then $|X \cap [1,N]| = o(N)$. S\'{a}rk\"{o}zy even conjectured that $D(N) = O(N^{1/2 + \varepsilon})$ for all $\varepsilon > 0$. Thus the sieve technique is essentially optimal, an incredibly pessimistic point of view, since the Sieve method doesn't depend on any properties of the set of perfect squares. Ruzsa's results shows we should be more optimistic, taking advantage of the digit expansion of numbers to obtain infinite squarefree subsets $X$ with $|X \cap [1,N]| = \Omega(N^{0.73})$. The method reduces the problem to a finitary problem of maximizing squarefree subsets modulo a squarefree integer $m$.

\begin{theorem}
    If $m$ is a squarefree integer, then
    %
    \[ D(N) \geq \frac{n^{\gamma_m}}{m} = \Omega_m(n^{\gamma_m}) \]
    %
    where
    %
    \[ \gamma_m = \frac{1}{2} + \frac{\log_m |R^*|}{m} \]
    %
    and $R^*$ denotes the maximal subset of $[1,m]$ whose differences contain no squares modulo $m$. Setting $m = 65$ gives
    %
    \[ \gamma_m = \frac{1}{2} \left( 1 + \frac{\log 7}{\log 65} \right) = 0.733077 \dots \]
    %
    and therefore $D(N) = \Omega(n^{0.7})$. For $m = 2$, we find $D(N) \geq \sqrt{N}/2$, which is only slightly worse than the sieve result.
\end{theorem}

\begin{remark}
    Let us look at the analysis of the sieve method backwards. Rather than fixing $N$ and trying to find optimal solutions of $[1,N]$, let's fix a particular strategy (to start with, the sieve strategy), and think of varying $N$ and seeing how the size of the solution given by the strategy on $[1,N]$ increases over time. In our analysis, the size of a solution is directly related to the number of iterations the stategy can produce before it runs out of integers to add to a solution set. Because we apply a union bound in our analysis, the cost of each particular new iteration is the same as the cost of the other iterations. If the cost of each iteration was independant of $N$, we could increase the solution size by increasing $N$ by a fixed constant, leading to family of solutions which increases on the order of $N$. However, as we increase $N$, the cost of each iteration increases on the order of $\sqrt{N}$, leading to us only being able to perform $N/\sqrt{N} = \sqrt{N}$ iterations for a fixed $N$. Rusza's method applies the properties of the perfect squares to perform a similar method of expansion. At an exponential cost, Rusza's method increases the solution size exponentially. The advantage of exponentials is that, since Rusza's is based on a particular parameter, a squarefree integer $m$, we can vary $m$ to improve the iteration numbers more naturally.
\end{remark}

The idea of Rusza's construction is to break the problem into exponentially large intervals, upon which we can solve the problem modulo an integer. More enerally, Rusza constructs a set whose differences are free of $d$'th powers.

\begin{theorem}
    Let $R \subset [1,m]$ be a subset of integers such that no difference is a power of $d$ modulo $m$, where $m$ is a {\it squarefree integer}. Construct the set
    %
    \[ A = \left\{ \sum_{k = 0}^n r_k m^k : 0 \leq n < \infty, r_k \in \left. \begin{cases} R & d\ \text{divides}\ N\\ [1,m] & \text{otherwise} \end{cases} \right\} \right\} \]
    %
    Then $A$ is squarefree.
\end{theorem}
\begin{proof}
    Suppose that we can write $\sum (r_k - r_k') m^k = N^d$. Let $s$ to be the smallest index with $r_s \neq r_s'$. Then $(r_s - r_s') m^s + M m^{s+1} = N^d$ where $M$ is some positive integer. If $s = ds_0$, then $(N/m^{s_0})^d = (r_s - r_s') + M m$, and this contradicts the fact that $r_s - r_s'$ cannot be a $d$'th power modulo $m$. On the other hand, we know $m^s$ divides $N^d$, but $m^{s+1}$ does not. This is impossible if $s$ is not divisible by $d$, because primes in $N^d$ occur in multiples of $d$, and $m$ is squarefree.
\end{proof}

For any $n$, we find
%
\[ A \cap [1,m^n - 1] = \left\{ \sum_{k = 0}^{n-1} r_km^k : r_k \in [1,m], r_k \in R\ \text{when $d$ divides $k$} \right\} \]
%
which therefore has cardinality
%
\begin{align*}
    |R|^{1 + \lfloor \frac{n-1}{d} \rfloor} m^{n-1- \lfloor \frac{n-1}{d} \rfloor} = m^n \left( \frac{|R|}{m} \right)^{1 + \lfloor \frac{n-1}{d} \rfloor} \geq m^n \left( \frac{|R|}{m} \right)^{n/d} = m^{n \gamma(m,d)}
\end{align*}
%
where $\gamma(m,d) = 1 - 1/d + \log_m |R|/d$. Therefore, for $m^{n+1} - 1 \geq k \geq m^n - 1$
%
\[ A \cap [1,k] \geq A \cap [1,m^n] \geq m^{n \gamma(m,d)} = \frac{m^{(n+1) \gamma(m,d)}}{m} \geq \frac{k^{\gamma(m,d)}}{m} \]
%
This completes Rusza's construction. Thus we have proved a more general result than was required.

\begin{theorem}
    For every $d$ and squarefree integer $m$, we can construct a set $X$ whose differences contain no $d$th powers and
    %
    \[ |X \cap [1,n]| \geq \frac{n^{\gamma(d,m)}}{m} = \Omega(n^{\gamma(d,m)}) \]
    %
    where $\gamma(d,m) = 1 - 1/d + \log_m |R^*|/d$, and $R^*$ is the largest subset of $[1,m]$ containing no $d$'th powers modulo $m$.
\end{theorem}

For $m = 65$, the group $\mathbf{Z}_{65}^* \cong \mathbf{Z}_{5}^* \times \mathbf{Z}_{13}^*$ has a set of squarefree residues of the form $\{ (0,0), (0,2), (1,8), (2,1), (2,3), (3,9), (4,7) \}$, which gives the required value for $\gamma_{65}$. In 2016, Mikhail Gabdullin proved that if $m$ is squarefree, then in $\mathbf{Z}_m$, any set $R$ such that $R - R$ is squarefree has $|R| \leq me^{-c \log m / \log \log m}$, where $n$ denotes the number of odd prime divisors of $m$, so that
%
\[ \gamma(d,m) \leq 1 - 1/d + \frac{\log(me^{-c \log m / \log \log m})}{m} \]
%
Rusza believes that we cannot choose $m$ to construct squarefree subsets of the integers growing better than $\Omega(n^{3/4})$, and he claims to have proved this assuming $m$ is squarefree and consists only of primes congruent to 1 modulo 4. Looking at some sophisticated papers in number theory (Though I forgot to write down the particular references), it seems that using modern estimates this is quite easy to prove. Thus expanding on Rusza's result in the discrete case requires a new strategy, or perhaps Rusza's result is the best possible.

Let $D(N,d)$ denote the largest subset of $[1,N]$ containing no $d$th powers of positive integer. The last part of Rusza's paper is devoted to lower bounding the polynomial growth of $D(N,d)$ asymptotically.

\begin{theorem}
    If $p$ is the least prime congruent to one modulo $2d$, then
    %
    \[ \limsup_{N \to \infty} \frac{\log D(N,d)}{\log N} \geq 1 - \frac{1}{d} + \frac{\log_p d}{d} \]
\end{theorem}
\begin{proof}
    The set $X$ we constructed in the last theorem shows that for any $m$,
    %
    \[ \frac{\log D(N,d)}{\log n} \geq \gamma(d,m) - \frac{\log m}{\log n} = 1 - \frac{1}{d} + \frac{\log_m |R^*|}{d} - \frac{\log m}{\log n} \]
    %
    Hence
    %
    \[ \limsup_{N \to \infty} \frac{\log D(N,d)}{\log n} \geq 1 - \frac{1}{d} + \frac{\log_m |R^*|}{d} \]
    %
    The claim is then completed by the following lemma.
\end{proof}

\begin{lemma}
    If $p$ is a prime congruent to $1$ modulo $2d$, then we can construct a set $R \subset [1,p]$ whose differences do not contain a $d$th power modulo $p$ with $|R| \geq d$.
\end{lemma}
\begin{proof}
    Let $Q \subset [1,p]$ be the set of powers $1^k, 2^k, \dots, p^k$ modulo $p$. We have
    %
    \[ |Q| = \frac{p-1}{k} + 1 \]
    %
    This follows because the nonzero elements of $Q$ are the images of the group homomorphism $x \mapsto x^k$ from $\mathbf{Z}_p^*$ to itself. Since $\mathbf{Z}_p^*$ is cyclic, the equation $x^k = 1$ has the same number of solutions as the equation $kx = 0$ modulo $p-1$, and since $p \equiv 1$ modulo $2k$, there are exactly $k$ solutions to this equation. The sieve method yields a $k$th power modulo $p$ free subset of size greater than or equal to
    %
    \[ p/q = \frac{p}{1 + \frac{p-1}{k}} = \frac{pk}{p + k - 1} \to k \]
    %
    as $p \to \infty$, which is greater than $k-1$ for large enough $p$. This shows the theorem is essentially trivial for large primes. However, for smaller primes a more robust analysis is required. We shall construct a sequence $b_1, \dots, b_k \in \mathbf{Z}_p$ such that $b_i - b_j \not \in Q$ for any $i,j$ and $|B_j + Q| \leq 1 + j(q-1)$. Given $b_1, \dots, b_j$, let $b_{j+1}$ be any element of $(B_j + Q + Q) - (B_j + Q)$. Since $b_{j+1} \not \in B_j + Q$, $b_{j+1} - b_i \not \in Q$ for any $i$. Since $b_{j+1} \in B_j + Q + Q$, the sets $B_j + Q$ and $b_{j+1} + Q$ are not disjoint (we have used $Q = -Q$, which is implied when $p \equiv 1$ mod $2k$), and so
    %
    \begin{align*}
        |B_{j+1} + Q| &= |(B_j + Q) \cup (b_{j+1} + Q)|\\
        &\leq |B_j + Q| + |b_{j+1} + Q| - 1\\
        &\leq 1 + j(q-1) + q - 1\\
        &= 1 + (j+1)(q-1)
    \end{align*}
    %
    This procedure ends when $B_j + Q + Q = B_j + Q$, and this can only happen if $B_j + Q = \mathbf{Z}_p$, because we can obtain all integers by adding elements of $Q$ recursively, so $1 + j(q-1) \geq p$, and thus $j \geq k$.
\end{proof}

\begin{corollary}
    In the special case of avoiding squarefree numbers, we find 
    \[ \limsup \frac{\log D(N)}{\log N} \geq \frac{1}{2} + \frac{\log_5 2}{2} = 0.71533\dots \]
    %
    which is only slightly worse than the bound we obtain with $m = 65$.
\end{corollary}

Rusza's leaves the ultimate question of whether one can calculate
%
\[ \alpha = \lim_{N \to \infty} \log D(N) / \log N \]
%
or even whether it exists at all. The consequence of this would essentially solve the squarefree integers problem, since it gives the exact growth of $D(N)$ in terms of a monomial. Because of how conclusive this problem is, we should not expect to find a nontrivial way to calculate this constant.









\section{Keleti: A Translate Avoiding Set}

Keleti's two page paper constructs a full dimensional subset $X$ of $[0,1]$ such that $X$ intersects $t + X$ in at most one place for each nonzero real number $t$. Malabika has adapted this technique to construct high dimensional subsets avoiding nontrivial solutions to differentiable functions. In this section, and in the sequel, we shall find it is most convenient to avoid certain configurations by expressing them in terms of an equation, whose properties we can then exploit. One feature of translation avoidance is that the problem is specified in terms of a linear equation.

\begin{lemma}
    A set $X$ avoids translates if and only if there do not exists values $x_1 < x_2 \leq x_3 < x_4$ in $X$ with $x_2 - x_1 = x_4 - x_3$.
\end{lemma}
\begin{proof}

    Suppose $t + X \cap X$ contains two points $a < b$. Without loss of generality, we may assume that $t > 0$. If $a \leq b - t$, then the equation $a - (a - t) = t = b - (b - t)$ satisfies the constraints, since $a - t < a \leq b - t < b$ are all elements of $X$. We also have $(b - t) - (a - t) = b - a$ which satisfies the constraints if $a - t < b - t \leq a < b$. This covers all possible cases. Conversely, if there are $x_1 < x_2 \leq x_3 < x_4$ in $X$ with $x_2 - x_1 = t = x_4 - x_3$, then $X + t$ contains $x_2 = x_1 + (x_2 - x_1)$ and $x_4 = x_3 + (x_4 - x_3)$.
\end{proof}

%\footnote{We always assume $L_n/L_{n+1}$ is an integer so that intervals in $\mathcal{B}(L_n)$ are either almost disjoint from intervals in $\mathcal{B}(L_{n+1})$ or contained completely within such an interval}

The basic, but fundamental idea to Keleti's technique is to introduce memory into Cantor set constructions. Keleti constructs a nested family of discrete sets $X_0 \supset X_1 \supset \dots$ converging to $X$, with each $X_N$ a union of disjoint intervals in $\mathcal{B}(L_N)$, for a decreasing sequence of lengths $L_N$ converging to zero, to be chosen later, but with $10 L_{N+1} \mid L_N$. We initialize $X_0 = [0,1]$, and $L_0 = 1$. Furthermore, we consider a queue of intervals, initially just containining $[0,1]$. To construct $X_1, X_2, \dots$, Keleti iteratively performs the following procedure:
%
\begin{algorithm}
    \begin{algorithmic}%[1]
        \caption{Construction of the Sets $X_N$}
        \State{Set $N = 0$}
        \MRepeat
            \State{Take off an interval $I$ from the front of the queue}

            \MForAll{\ $J \in \mathcal{B}(L_N)$ contained in $X_N$:}
                \State{Order the intervals in $\mathcal{B}(L_{N+1})$ contained in $J$ as $J_0, J_1, \dots, J_M$}

                \State{{\bf If} $J \subset I$, add all intervals $J_i$ to $X_{N+1}$ with $i \equiv 0$ modulo 10}
                \State{{\bf Else} add all $J_i$ with $i \equiv 5$ modulo 10}
            \EndForAll
            \State{Add all intervals in $\mathcal{B}(L_{N+1})$ to the end of the queue}
            \State{Increase $N$ by 1}
        \EndRepeat   
    \end{algorithmic}
\end{algorithm}

After each iteration of the algorithm, we obtain a new set $X_{N+1}$, and so leaving the algorithm to repeat infinitely produces a sequence of sets $X_1, X_2, \dots$ converging to a set $X$. We claim that with the appropriate choice of parameters, $X$ is a translate avoiding set.

If $X$ is not translate avoiding, there is $x_1 < x_2 \leq x_3 < x_4$ with $x_2 - x_1 = x_4 - x_3$. Since $L_N \to 0$, there is $N$ such that $x_1$ is contained in an interval $I \in \mathcal{B}(L_N)$ that $x_2,x_3, x_4$ are not contained in. At stage $N$ of the algorithm, the interval $I$ is added to the end of the queue, and at a much later stage $M$, the interval $I$ is retrieved. Find the startpoints $x_1^\circ, x_2^\circ$, $x_3^\circ, x_4^\circ \in L_M \mathbf{Z}$ to the intervals in $\mathcal{B}(L_M)$ containing $x_1$, $x_2$, $x_3$, and $x_4$. Then we can find $n$ and $m$ such that $x_4^\circ - x_3^\circ = (10n)L_M$, and $x_2^\circ - x_1^\circ = (10m + 5)L_M$. In particular, this means that $|(x_4^\circ - x_3^\circ) - (x_2^\circ - x_1^\circ)| \geq 5L_M$. But
%
\begin{align*}
    |(x_4^\circ - x_3^\circ) - (x_2^\circ - x_1^\circ)| &= |[(x_4^\circ - x_3^\circ) - (x_2^\circ - x_1^\circ)] - [(x_4 - x_3) - (x_2 - x_1)]|\\
    &\leq |x_1^\circ - x_1| + \dots + |x_4^\circ - x_4| \leq 4 L_M
\end{align*}
%
which gives a contradiction.

The algorithm shows that $X_N$ contains $L_{N-1} / 10 L_N$ times the number of intervals that $X_{N-1}$ has, but they are at a length $L_N$ rather than $L_{N-1}$. This means that in total, $X_N$ contains $1/10^N L_N$ intervals, of length $L_N$. Since $L_N / 10^N L_N = o(1)$, this shows our set will have Lebesgue measure zero irrespective of our parameters. However, if $L_N$ decays suitably fast, then we might have $L_N^{1 - \varepsilon}/10^N L_N \gtrsim_\varepsilon 1$ for all $\varepsilon > 0$, which would imply that $X$ has positive $1 - \varepsilon$ dimensional Hausdorff measure for all $\varepsilon$, so $X$ still has Hausdorff dimension one. For this to be true, $L_N$ must decay superexponentially, i.e. the inequalities above are equivalent to $L_N \lesssim_B 1/B^N$ for all choices of $B$. Choosing an arbitrarily fast decaying sequence, such as $L_N = 1/N! \cdot 10^N$ or $L_N = 1/10^{10^N}$, suffices to obtain a Hausdorff dimension one set.

\begin{lemma}
    If $L_N$ decays superexponentially, $X$ has Hausdorff dimension one.
\end{lemma}
\begin{proof}
%Recall Frostman's lemma, which says that the $s$ dimensional Hausdorff measure $H_s(X)$ of a Euclidean set $X$ is positive if and only if there is a finite positive Borel measure $\mu$ supported on $X$ with $\mu(B_r(x)) \lesssim r^s$, for a universal constant depending only on $\mu$. If such a measure can be constructed on a set $X$, it therefore follows that $\dim_{\mathbf{H}}(X) \geq s$. Thus to prove $X$ has dimension one, it suffices to construct a probability measure $\mu$ on $X$ with $\mu(B_r(x)) \lesssim_s r^s$, for each $s < 1$. We can construct such a measure using what is often called the {\it mass distribution principle}; we construct a probability measure $\mu_n$ supported on $X_n$ in such a way that a weak limit $\mu = \lim \mu_n$ exists, in which case $\mu$ is supported on $X$. To do this, we let $\mu_1$ be the uniform probability measure on $[0,1]$. Then, to construct $\mu_{n+1}$ from $\mu_n$, we divide the mass of each interval $J$ in $X_n$ uniformly over the intervals in $X_{n+1}$ contained in $J$. The distribution functions of these measures converge uniformly, and therefore the $\mu_n$ converge weakly to a measure $\mu$ supported on $X$.

We use the mass distribution principle, as used in our note on calculating Hausdorff dimensions. It is easy to establish the bounds $\mu_N(I) \lesssim_\varepsilon L(I)^{1-\varepsilon}$ for $I \in \mathcal{B}(L_N)$, and since we can choose $L_N$ suitably slowly decreasing to use the epsilon of room technique, this gives the result. Alternatively, we can use the uniform distribution bounds with $L_N = R_N$, since if $J \in \mathcal{B}(L_{N+1})$, $I \in \mathcal{B}(L_N)$, $\mu(J) = 1/10^{N+1} L_{N+1}$, $\mu(I) = 1/10^N L_N$, and so $\mu(J) \lesssim (L_{N+1}/L_N) \mu(I)$. This gives the result if $L_N$ grows too fast.
\end{proof}

%\begin{remark}
%    Here's why we need the tighter bound $\mu(I) \lesssim_\varepsilon |I|^{1-\varepsilon}/(n!)^{\varepsilon/2}$ at the discrete scales to successively interpolate our bounds to all interval scales, rather than just the simpler bound $\mu(I) \lesssim_\varepsilon |I|^{1-\varepsilon}$. If $L_{N+1} \leq |I| \leq L_N$, and we cover $I$ by $|I|L_{N+1}^{-1}$ length $L_{N+1}$ intervals, then we obtain that
    %
%    \[ \mu(I) \lesssim_\varepsilon |I|L_{N+1}^{-1} L_{N+1}^{1-\varepsilon} = |I| L_{N+1}^{-\varepsilon} = \left( \frac{|I|}{L_{N+1}} \right)^\varepsilon |I|^{1-\varepsilon} \]
    %
%    Similarily, if we cover $I$ by a single length $L_N$ interval, then
    %
%    \[ \mu(I) \lesssim_\varepsilon L_N^{1-\varepsilon} = \left( \frac{L_N}{|I|} \right)^{1-\varepsilon} |I|^{1-\varepsilon} \]
    %
%    If we are to hope that these bounds give us a $\lesssim_\varepsilon |I|^{1-\varepsilon}$ bound for all $\varepsilon$, then we must have
    %
%    \[ \max_{L_{N+1} \leq |I| \leq L_N} \min \left( \left( \frac{|I|}{L_{N+1}} \right)^\varepsilon, \left( \frac{L_N}{|I|} \right)^{1-\varepsilon} \right) \lesssim_\varepsilon 1 \]
    %
%    The minimization is maximized when
    %
%    \[ \left( \frac{|I|}{L_{N+1}} \right)^\varepsilon = \left( \frac{L_N}{|I|} \right)^{1-\varepsilon} \]
    %
%    or when $|I| = L_N^{1-\varepsilon} L_{N+1}^\varepsilon$. Inputting this into the formula, we obtain that
    %
%    \[ \max_{L_{N+1} \leq |I| \leq L_N} \min \left( \left( \frac{|I|}{L_{N+1}} \right)^\varepsilon, \left( \frac{L_N}{|I|} \right)^{1-\varepsilon} \right) = \left( \frac{L_N}{L_{N+1}} \right)^{\varepsilon (1 - \varepsilon)} \]
    %
%    With the choice of parameters given, we have $L_N/L_{N+1} = 8(n+1)$, and we do not have $(8(n+1))^{\varepsilon(1-\varepsilon)} \lesssim_\varepsilon 1$. Thus, with the bounds we have used, there is no way to obtain a constant coefficient bound for all scales lying inbetween the discrete scales if we use the $\mu(I) \lesssim_\varepsilon |I|^{1-\varepsilon}$ bound for the discrete scales. However, the tighter bound $\mu(I) \lesssim_\varepsilon |I|^{1-\varepsilon}/(n!)^{\varepsilon/2}$ causes the $O(n)$ term for $L_N/L_{N+1}$ to be annihilated, which results in a constant term bound at the continuous range of scales.
%\end{remark}

\begin{remark}
    Keleti briefly remarks that by replacing the 10 in the algorithm with a slowly increasing set of numbers, one can obtain a Hausdorff dimension one set which is linearly independant over the rational numbers. To see why this works, the condition of linear independence would fail if $\smash{a_1 x_1 + \dots + a_M x_M = 0}$, where $\smash{x_1 < x_2 < \dots < x_M}$, and $\smash{a_1, \dots, a_M}$ are integers with no common factor. One can again reduce this by picking intervals with indices congruent to a certain large modulus.

%     Just as before, we find $x_n^\circ$ with $0 \leq x_n - x_n^\circ \leq L_N$. Provided that $2 (a_1 + \dots + a_M) L_N < M_M$, and the $x_n^\circ$ lie at integer multiples of $\varepsilon_N$, we conclude that $a_1 x_1^\circ + \dots + a_M x_M^\circ = 0$. If $K$ is an integer not dividing $a_1$, then for suitably large $N$ we assume that each $x_2^\circ, \dots, x_M^\circ$ lies at multiples of $K\Delta_N$. Shifting $x_1^\circ$ by a single multiple of $\Delta_N$ then breaks the equation from ever occuring in the first place. In order to guarantee this, we must first set $\varepsilon_n = A_n! L_n$ where $A_n$ is an increasing sequence with $A_n \to \infty$. We also guarantee that $x_2^\circ, \dots, x_M^\circ$ lies at multiples of $A_n! \Delta_n$. This can be guaranteed by induction if $A_{n+1}! \Delta_{n+1} \divides \Delta_n, \varepsilon_{n+1}$. Thus the parameters
    %
%    \[ \Delta_n = L_n\ \ \ \varepsilon_n = A_n! L_n\ \ \ L_{n+1} = \frac{L_n}{2N_{n+1}A_{n+1}!} \]
    %
%    give a linearly independant set. Assuming the $A_n$ grow incredibly slowly relative to the $N_n$, i.e.
    %
%    \[ N_n = n\ \ A_n = \log \log n + O(1)\ \ \ \ N_n = 2^n\ \ A_n = \log n + O(1)\ \ \ \ N_n = 2^{n^2}\ \ A_n = n \]
    %
%    then we obtain a set with Hausdorff dimension one.
%Assuming the $A_n$ grow incredibly slowly, we can still hope for this set to have Hausdorff dimension one. fI we construct the probability measure $\mu$ as before, we find that for any length $l_n$ interval $J$
    %
%    \[ \mu(J) \leq \frac{2}{n!} = \frac{2}{n!l_n^{1-\varepsilon}} l_n^{1-\varepsilon} = \left( \frac{2}{(n!)^\varepsilon} \left( \prod A_m! \right)^{1-\varepsilon} \right) l_n^{1-\varepsilon} \]
    %
%    We can choose the $A_m$ to grow slowly enough that for any $\varepsilon > 0$,
    %
%    \[ \left( \prod A_m! \right)^{1-\varepsilon} \lesssim_\varepsilon (n!)^{\varepsilon/2} \]
    %
%    Testing this inequality leads to the fact that $A_{n+1}! \leq (n+1)^{\varepsilon/2(1-\varepsilon)}$ must eventually hold for $n$ large enough, so taking $A_n \to \infty$ but growing slower than any polynomial in $n$ satisfies the inequality, i.e. if $A_n!$ is the largest factorial smaller than $\log n$. Thus
    %
%    \[ \mu(J) \lesssim_\varepsilon \frac{l_n^{1-\varepsilon}}{(n!)^{\varepsilon/2}} \]
    %
%    and the interpolation bound as in the previous problem then guarantee $\mu(I) \lesssim_\varepsilon |I|^{1-\varepsilon}$ for all $I$, since $l_n/l_{n+1} = O(nA_n!) = O((n!)^{1/2})$.
\end{remark}

%\begin{remark}
%    We attempted to obtain a squarefree subset of $[0,1]$ by combining Ruzsa's squarefree discrete strategy with Keleti's decomposition approach to find a high dimensional continuous squarefree set. However, using these techniques we were only able to obtain a dimension 1/2 set, which is only slightly better than a dimension 1/3 set which exists from the general results given by Math\'{e}'s result, or Pramanik and Fraser's result, and is much less than the dimension 1 set that Malabika expects.
%\end{remark}







\section{Fraser/Pramanik: Extending Keleti Translation to Smooth Configurations}

Inspired by Keleti's result, Pramanik and Fraser obtained a generalization of the queue method which allows one to find sets avoiding solutions to {\it any} smooth function satisfying suitably mild regularity conditions. To do this, rather than making a linear shift in one of the intervals we avoid as in Keleti's approach, one must use the smoothness properties of the function to find large segments of an interval avoiding solutions to another interval.

\begin{theorem}
    Suppose that $f: \mathbf{R}^{d+1} \to \mathbf{R}$ is a $C^1$ function, and there are sets $T_0, \dots, T_d \subset [0,1]$, with each $T_n$ a union of almost disjoint closed intervals of length $1/M$ such that $A \leq |\partial_0 f|$ and $|\nabla f| \leq B$ on $T_0 \times \dots \times T_d$. There there exists a rational constant $C$ and arbitrarily large integers $N \in M \mathbf{Z}$ for which there exist subsets $S_n \subset T_n$ such that
    %
    \begin{itemize}
        \item[(i)] $f(x) \neq 0$ for $x \in S_0 \times \dots \times S_d$.

        \item[(ii)] For $n \neq 0$, if we divide each interval $T_n$ into length $1/N$ intervals, then $S_n$ contains an interval of length $C/N^d$ of each of these intervals.

        \item[(iii)] If $T_0$ is split into length $1/N$ intervals, then for a fraction $1 - 1/M$ of such intervals, $S_0$ is a union of length $C/N^d$ intervals with total length $C/N$.
    \end{itemize}
\end{theorem}
\begin{proof}
    We begin by dividing the sets $T_1, \dots, T_d$ into length $1/N$ intervals, and let $S_n$ be defined by including a length $C_0/N^d$ segment, for some constant $C_0$ to be chosen later. Then once we fix $C_0$, the $S_n$ will satisfy property (ii) of the theorem. We define
    %
    \[ \mathbf{A} = \{ a \in \mathbf{R}^{d-1} : a_n\ \text{is a startpoint of a length $1/N$ interval in}\ T_n \} \]
    %
    Then $|\mathbf{A}| \leq N^d$, since each interval $T_n$ is contained in $[0,1]$, and therefore can only contain at most $N$ almost disjoint intervals of length $1/N$. Hence if we define the set of `bad points' in $T_0$ as
    %
    \[ \mathbf{B} = \{ x \in T_0: \text{there is}\ a \in \mathbf{A}\ \text{such that}\ f(x,a) = 0 \} \]
    %
    Then $|\mathbf{B}| \leq MN^d$. This is because for each fixed $a$, the function $x \mapsto f(x,a)$ is either strictly increasing or decreasing over each interval in the decomposition of $T_0$, or which there are at most $M$ because $T_0 \subset [0,1]$. If we split $T_0$ into length $1/N$ intervals, and choose a subcollection of such intervals $I$ such that $|I \cap \mathbf{B}| \leq M^3N^{d-1}$, then we throw away at most $MN^d/M^3N^{d-1} = N/M^2$ intervals, and so we keep $(N/M)(1 - 1/M)$ intervals, which is $1 - 1/M$ of the total number of intervals in the decomposition of $T_0$. The lemma we prove after this theorem implies that there exists a constant $C_1$ such that if $x \in S_n$, and $f(y,x) = 0$, then $d(y,\mathbf{B}) \leq C_0C_1/N^d$. If we split each interval $I$ with $|I \cap \mathbf{B}| \leq M^3N^{d-1}$ into $4M^3N^{d-1}$ length $1/4M^3N^d$ intervals, and we choose $C_0$ such that $C_0C_1 < 1/4M^3$, then the set $S_0$ obtained by discarding each interval that contains or is adjacent to an interval containing an element of $\mathbf{B}$ satisfies $d(S_0,\mathbf{B}) > C_0C_1/N^d$, and therefore there does not exist any $x_n \in S_n$ and $y \in S_0$ such that $f(y,x) = 0$. $S_0$ satisfies property (iii) of the theorem since for the interval $I$ we are considering, we keep at least $M^3N^{d-1}$ length $1/4M^3N^d$ intervals, which in total has length at least $1/4N$.
\end{proof}

\begin{remark}
    The length $1/N$ portion of each interval guaranteed by (iii) is unneccesary to the Hausdorff dimension bound, since the slightly better bounds obtained on scales where an interval is dissected as a $1/N$ are decimated when we eventually divide the further subintervals into $1/N^{d-1}$ intervals. The importance of (iii) is that it implies that the set we will construct has full {\it Minkowski dimension}. The reason for this is that Minkowski dimension lacks the ability to look at varying dissection depths at once, and since, at any particular depth, there exists a length $1/N$ dissection, the process appears to Minkowski to be full dimensional, even though at later scales this $1/N$ dissection is dissected into $1/N^{d-1}$ intervals.
\end{remark}

\begin{lemma}
    Given the $f$, $T_0, \dots, T_d$, there exists a constant $C_1$ depending on these quantities, such that for any $C_0$, and $x \in S_1 \times \dots \times S_{d-1}$, if $f(y,x) = 0$, then $d(y, \mathbf{B}) \leq C_0C_1/N^{d-1}$.
\end{lemma}
\begin{proof}
    Since $T_0 \times \dots \times T_d$ breaks into finitely many cubes with sidelengths $1/M$, it suffices to prove the theorem for a particular cube $J$ in this decomposition, where we assume the zeroset of $f$ intersects $J$. If $J = I \times J'$, where $I$ is an interval, we let $U$ be the set of all $x \in J'$ for which there is $y$ in the interior of $I$ such that $f(y,x) = 0$. Then $U$ is open. The implicit function theorem implies that there exists a $C^1$ function $g: U \to I$ such that $f(x,y) = 0$ if and only if $y = g(x)$. Then the function $h(x) = f(x,g(x))$ vanishes uniformly, so
    %
    \[ 0 = \partial_n h(x) = (\partial_n f) (g(x),x) + (\partial_0 f) (g(x),x) \partial_n g(x) \]
    %
    Hence for $x \in U$,
    %
    \[ |(\nabla g)(x)| = \frac{|(\nabla f)(x)|}{|(\partial_d f)(x,g(x))|} \leq \frac{B}{A} \]
    %
    If $N$ is chosen large enough, then for every $x \in U \cap (S_1 \times \dots \times S_d)$ there is $a \in \mathbf{A} \cap U$ in the same connected component of $U$ as $x$ with $|x - a| \lesssim C_0/N^{d-1}$, and this means that
    %
    \[ |g(x) - g(a)| \leq \| \nabla g \|_\infty |x - a| \lesssim \frac{BC_0}{A N^{d-1}} \]
    %
    and $g(a) \in \mathbf{B}$, completing the proof.
\end{proof}

How do we use this lemma to construct a set avoiding solutions to $f$? We form an infinite queue which will eventually filter out all the possible zeroes of the equation. Divide the interval $[0,1]$ into $d$ intervals, and consider all orderings of $d - 1$ subsets of these intervals, and add them to the queue. Now on each iteration $N$ of the algorithm, we have a set $X_N \subset [0,1]$. We take a particular sequence of intervals $T_1, \dots, T_d$ from the queue, and then use the lemma above to dissect the $X_N \cap T_n$, which are unions of intervals, into sets avoiding solutions to the equation, and describe the remaining points as $X_{N+1}$. We then add all possible orderings of $d$ intervals created into the end of the queue, and rinse and repeat. The set $X = \lim X_n$ then avoids all solutions to the equation with distinct inputs.

What remains is to bound the Hausdorff dimension of $X$ by constructing a probability measure supported on $X$ with suitable decay. To construct our probability measure, we begin with a uniform measure on the interval, and then, whenever our interval is refined, we uniformly distribute the volume on that particular interval uniformly over the new refinement. Let $\mu$ denote the weak limit of this sequence of probability distributions. At each step $n$ of the process, we let $1/M_n$ denote the size of the intervals at the beginning of the $n$'th subdivision, $1/N_n$ denote the size of the split intervals in the lemma, and $C_n$ the $n$'th constant. We have the relation $1/M_{n+1} = C_n/N_n^{d-1}$. If $K$ is a length $1/M_{N+1}$ interval, $J$ a length $1/N_N$ interval, and $I$ a length $1/M_N$ interval with $K \subset J \subset I$ and all recieving some mass in $\mu$. To calculate a bound on their mass, we consider the decompositions considered in the algorithm:
%
\begin{itemize}
        \item If $J$ is subdivided in the non-specialized manner, then every length $1/N_N$ interval recieves the same mass, which is allocated to a single length $1/M_{N+1}$ interval it contains. Thus $\mu(K) = \mu(J) \leq (M_N/N_N) \mu(I)$.
        \item In the second case, at least a fraction $1 - 1/M_N$ of the length $1/N_N$ intervals are assigned mass, so $\mu(J) = (M_N/N_N)(1 - 1/M_N)^{-1} \leq (2M_N/N_N) \mu(I)$, and more than $C_N/N_N$ of each length $1/N_N$ interval is maintained, so
        %
        \[ \mu(K) = \frac{N_N}{C_NM_{N+1}} \mu(J) \leq \frac{2M_N}{C_NM_{N+1}} \mu(I) \]
\end{itemize}
%
Thus in both cases, we have $\mu(J) \lesssim (N_N/M_N) \mu(I)$, $\mu(K) \lesssim_N |K|$, and $N_N = M_{N+1}^{1/(d-1)}/C_N \lesssim_N M_{N+1}^{1/(d-1)}$. From this, we conclude using the results of the appendix that there exists a family of rapidly decaying parameters which gives a $1/(d-1)$ dimensional set.

\begin{remark}
    The set $X$ constructed is precisely a $1/(d-1)$ dimensional set. Recall that $X = \lim X_n$, where $X_n$ is a union of a certain number of length $1/M_n$ intervals $I_1, \dots, I_N$. For each $n$, the interval $I_i$ is inevitably subdivided at a stage $J_i$ into length $C_{J_i} N_{J_i}^{1-d}$ intervals for each length $1/N_{J_i}$ interval that $I_i$ contains. Thus
    %
    \[ H_{1/M_n}^\alpha(X) \leq \sum_{i = 1}^N \frac{N_{m_i}}{M_n} (C_{m_i} N_{m_i}^{1-d})^\alpha = \frac{1}{M_n} \sum_{i = 1}^N C_{m_i}^\alpha N_{m_i}^{1 - \alpha(d-1)} \]
    %
    We may assume that $C_{m_i} \leq 1$, so if $\alpha > 1/(d - 1)$, using the fact that $N \leq M_n$, since $X_n$ is contained in $[0,1]$, we obtain
    %
    \[ H_{1/M_n}^\alpha(X) \leq \frac{1}{M_n} \sum_{i = 1}^N N_{m_i}^{1 - \alpha(d-1)} \leq N_{\max(m_i)}^{1 - \alpha(d-1)} \leq 1 \]
    %
    Thus, taking $n \to \infty$, we conclude $H^\alpha(X) \leq 1 < \infty$, so as $\alpha \downarrow 1/(d - 1)$, we conclude that $X$ has Hausdorff dimension bounded above by $1/(d-1)$.
\end{remark}

%Thus, in both cases, we have $\mu(J) \lesssim (N_N/M_N) \mu(I)$, which means we can apply the second method of appendix to calculate Hausdorff dimension with rapidly growing constants, where $l_N = 1/M_N$ and $r_N = 1/N_N$. We have $\mu(K) \lesssim_N $ and $N_N = M_{N+1}^{1/(d-1)}/C_N$ and


%
%Thus, in both cases, we have $\mu(J) \lesssim_N 1/M_{N+1}$. If $J \subset I$ is any length $1/N_N$ interval considered in the algorithm, then either $\mu(J) = (M_N/N_N) \mu(I)$, as in the first case of the subdivision, or we can apply the second case of the subdivision, giving $\mu(J) = (M_N/N_N)(1 - 1/M_N)^{-1} \mu(I) \leq (2M_N/N_N) \mu(I)$. This means we can apply the second method in the appendix. The fact that 

%by induction, if $I$ is a length $1/M_N$ interval considered in the process, then
%
%\begin{align*}
%    \mu(I) \leq \prod_{n < N} \frac{M_n}{(C_n M_{n+1})^{\frac{1}{d-1}}} = \left( \prod_{n < N} \frac{M_{n+1}^{1-\frac{1}{d-1} }}{C_n^{\frac{1}{d-1}}} \right) \frac{1}{M_N} = \frac{A_N}{M_N}
%\end{align*}
%
%If $J \subset I$ is any length $1/N_N$ interval considered in the algorithm, then either $\mu(J) = (M_N/N_N) \mu(I)$, as in the first case, or in the second case, $\mu(J) = (M_N/N_N(1 - 1/M_N)) \leq 2M_N/N_N \mu(I)$, so in general $\mu(J) \leq 2A_N/N_N$. This means we can apply the second method in the appendix for bounding Hausdorff dimension, with $l_N = 1/M_N$ and $r_N = 1/N_N$. To obtain 

%Now if $1/N_N \leq |I| \leq 1/M_N$, then $I$ can be covered by $|I|N_N$ intervals of length $1/N_N$, and so
%
%\begin{align*}
%    \mu(I) &\leq 2|I|N_N \frac{A_N}{N_N} = 2A_N|I| = \frac{2A_{N-1} M_N^{1 - \frac{1}{d-1}}}{C_{N-1}^{\frac{1}{d-1}}} |I| \lesssim_\varepsilon |I|M_N^{1 - \frac{1}{d-1} - \varepsilon} \leq |I|^{\frac{1}{d-1} - \varepsilon}
%\end{align*}
%
%Provided that we can choose $M_N$ such that $A_N/C_N \lesssim_\varepsilon M_{N+1}^\varepsilon$ for all $\varepsilon$ (this is why it is incredibly important that the values in the lemma are independent of $N$ in the proof above). On the other hand, if $1/M_{N+1} \leq |I| \leq 1/N_N$, then $I$ can be covered by a single length $1/N_N$ interval, hence
%
%\[ \mu(I) \leq \frac{2A_N}{N_N} = \frac{2A_N}{N_N} = \frac{2A_N}{(C_NM_{N+1})^{\frac{1}{d-1}}} \lesssim_\varepsilon \frac{1}{M_{N+1}^{\frac{1}{d-1} - \varepsilon}} \leq |I|^{\frac{1}{d-1} - \varepsilon} \]
%
%Thus we obtain the theorem if $M_{N+1} = \exp(A_N/C_N)$, for instance.

\section{A Set Avoiding All Functions With A Common Derivative}

In the latter part's of their paper, Pramanik and Fraser apply an iterative technique to construct, for each $\alpha$ with $\sum \alpha_n = 0$ and $K > 0$, a set $E$ of positive Hausdorff dimension avoiding solutions to any function $f: \mathbf{R}^d \to \mathbf{R}$ satisfying wth $(\partial_n f)(0) = \alpha_n$,
%
\[ \left| f(x) - \sum \alpha_n x_n \right| \leq K \sum_{n \neq 1} (x_n - x_1)^2 \]
%
The set of such $f$ is an uncountable family, which makes this situation interesting. The technique to create such a set relies on another iterative procedure.

\begin{lemma}
    Let $I \subsetneq [1,d]$ be a strict subset of indices, and $\delta_0 > 0$. Then there exists $\varepsilon > 0$ such that for any $\lambda > 0$ and two disjoint intervals $J_1$ and $J_2$, with $J_1$ occuring before $J_2$, and if we set
    %
    \[ [a_n,b_n] = \begin{cases} J_1 & n \in I \\ J_2 & n \not \in I \end{cases} \]
    %
    then for $\delta < \delta_0$, either for all $x_n \in [a_n,a_n+\varepsilon \lambda]$ or for all $x_n \in [b_n - \varepsilon \lambda, b_n]$,
    %
    \[ \left| \sum \alpha_n x_n \right| \geq \delta \lambda \]
\end{lemma}
\begin{proof}
    If $C^* = \sum |\alpha_n|$, then for $|x_n - a_n| \leq \varepsilon \lambda$,
    %
    \[ |\sum \alpha_n (x_n - a_n)| \leq C^* \varepsilon \lambda \]
    %
    Thus if $|\sum \alpha_n a_n| > (\delta + \varepsilon C^*)\lambda$, then $|\sum \alpha_n x_n| \geq \delta \lambda$. If this does not occur
\end{proof}









\section{Equidistribution Results}

The classical Weyl equidistribution theorem says that if $\alpha$ is an irrational number, then the decimal parts of the numbers $n \alpha$ are equidistributed in $\mathbf{T} = \mathbf{R}/\mathbf{Z}$, in the sense that for any continuous function $f: \mathbf{T} \to \mathbf{R}$,
%
\[ \frac{1}{N} \sum_{n \leq N} f(n \alpha) \to \int_{\mathbf{T}} f(x)\; dx \]
%
This is equivalent to prove that for any interval $[a,b] \in [0,1)$,
%
\[ \frac{\# \{ 1 \leq n \leq N : x_n \in [a,b] \}}{N} \to b - a \]
%
as $N \to \infty$. By approximating a continuous function $f$ by a Fourier series, to prove this is true for a particular sequence, it suffices to prove it for $f(x) = e(nx) = e^{2 \pi i n x}$, for each nonzero integer $n$. Certain techniques we are developing in the theory of cantor decompositions require a higher dimensional variant of such a result, so this section details some information which might help us in the future. We will encounter sequences that are not equidistributed over the entire space, so if $G$ is any closed subgroup of $\mathbf{T}$, we say a sequence $x_n$ is equidistributed over $G$ if $x_n \in G$ for all $n$, and for any continuous function $f: G \to \mathbf{R}$,
%
\[ \frac{1}{N} \sum_{n \leq N} f(n \alpha) \to \int_G f(x)\; dx \]
%
or alternatively, if for any closed set $K$ in $G$,
%
\[ \frac{\# \{ 1 \leq n \leq N: x_n \in K \}}{N} \to |K| \]
%
where $|K|$ is taken with respect to the Haar probability measure on $G$.

\begin{theorem}
    A sequence $x_1, x_2, \dots$ is equidistributed in $\mathbf{T}^d$ if and only if for every nonzero $\xi \in \mathbf{Z}^n$,
    %
    \[ \frac{1}{N} \sum_{n \leq N} e(\xi \cdot x_n) \to 0 \]
\end{theorem}
\begin{proof}
    We prove that the exponential sum condition implies the general result, noting that the other direction is clear. Clearly the exponential sum condition implies the result for all functions $f$ which are trigonometric polynomials. But then, by the Stone Weirstrass theorem and basic Abelian Harmonic analysis, the multivariate trigonometric polynomials are dense in $C(\mathbf{T}^d)$, and we may apply a standard limiting argument.
\end{proof}

\begin{example}
    If $x_n = n \alpha + \beta$, then
    %
    \begin{align*}
        \frac{1}{N} \sum_{n \leq N} e(\xi \cdot x_n) &= \frac{e(\xi \cdot \beta)}{N} \sum_{n \leq N} e(n \xi \cdot \alpha) = \begin{cases} \frac{1}{N} \frac{e(\xi \cdot \beta)(e((n+1) \xi \cdot \alpha) - 1)}{e(\xi \cdot \alpha) - 1} & : \xi \cdot \alpha \not \in \mathbf{Z} \\ e(\xi \cdot \beta) & : \xi \cdot \alpha \in \mathbf{Z} \end{cases}
    \end{align*}
    %
    Weyl's exponential sum theorem implies that $x_n$ is equidistributed on $\mathbf{T}^n$ precisely when $\xi \cdot \alpha \not \in \mathbf{Z}$ for all $\xi \in \mathbf{Z}^n$. What's more, it is simple to see from this that $x_n$ is still equidistributed for any subsequence whose indices form an arithmetic progression.
\end{example}

Thus in one dimension, an arithmetic sequence is either equidistributed over the entire torus, or over a discrete set of points forming a {\it discrete subgroup} of the torus. We can think of this discrete subgroup as a zero dimension torus, which leads us to suspect that in higher dimensions, an arithmetic sequence is always equidistributed, but not necessarily over the whole torus, but instead over a lower dimensional subtorus.

\begin{theorem}[Ratner]
    If $x_n = n \alpha + \beta$, then we can write $\alpha = \alpha_0 + \alpha_1$, where the sequence $n \alpha_0 + \beta$ is periodic in $\mathbf{T}^d$, and $n \alpha_1 + \beta$ is equidistributed over a subtorus of $\mathbf{T}^d$. In particular, if $\beta = 0$, then Ratner's theorem says that $x_n$ has an evenly spaced subsequence equidistributed over a subtorus of the space containing the origin.
\end{theorem}
\begin{proof}
    We induct on the dimension. For $d = 1$, the theorem is obvious, since either $\alpha$ is rational, and therefore $n \alpha + \beta$ is periodic, or $\alpha$ is irrational, and $n \alpha + \beta$ is equidistributed on $\mathbf{T}$. So now let us consider a sequence on the torus $\mathbf{T}^{d+1}$. If $\alpha$ is irrational, then $x_n$ is equidistributed, and the theorem is obvious. Otherwise, there exists $\xi \in \mathbf{Z}^n$ such that $\xi \cdot \alpha \in \mathbf{Z}$. We may write $\alpha = \alpha_0 + \alpha_1$, where $\alpha_0 \in \mathbf{Q}^d$, and $\xi \cdot \alpha_1 = 0$. The sequence $n\alpha_0 + \beta$ is periodic, whereas $n \alpha_1$ takes values in the subtorus $T$ of points $x$ with $\xi \cdot x = 0$. Since $\xi \neq 0$, $T$ is a $d$ dimensional compact subgroup of $\mathbf{T}^{d+1}$ isomorphic to $\mathbf{T}^d$. To see this, we assume for simplicity that $\xi_d \neq 0$. Then $\xi^\perp$ is generated by the basis of $d$ vectors $v_n = \xi_d e_n - \xi_n e_d$, for $1 \leq n \leq d$. Thus we get a homomorphism between $\mathbf{R}^d$ and $T$ given by the map $x \mapsto \sum x_n v_n$. If $\sum x_n v_n \in \mathbf{Z}$, so that $\sum x_n v_n = 0$ on $T$, then this means that $x_n \in \mathbf{Z}/\xi_d$ for each $n$, implying that the kernel of this homomorphism is discrete. Lattice theory implies that we can write the kernel as $\bigoplus \mathbf{Z} \langle w_n \rangle$, for $d$ generating vectors $w_1, \dots, w_d$. But then the map $f(x) = \sum x_n w_n$ gives an isomorphism between $\mathbf{T}^d$ and $T$. If we set $\beta = f^{-1}(\alpha_0)$, then $f^{-1}(n\alpha_0) = n \beta$, and so by induction, we can write $\beta = \beta_0 + \beta_1$, where $n\beta_0$ is periodic, and $n \beta_1$ is equidistributed over a subtorus of $\mathbf{T}^d$. But then $n f(\beta_0) = f(n \beta_0)$ is periodic on $T$, and $n f(\beta_1) = f(n \beta_1)$ is equidistributed over a subtorus of $T$. Since the sum of two periodic sequences is periodic, $n \alpha_0 + n f(\beta_0)$ is periodic, and $n f(\beta_1)$ is equidistributed over a subtorus. We have $\alpha_0 + f(\beta_0) + f(\beta_1) = \alpha_0 + f(\beta) = \alpha_0 + \alpha_1$, completing the proof.
\end{proof}

\begin{corollary}
    Any linear sequence in $\mathbf{T}^d$ is equidistributed in a finite union of cosets of a subtorus of $\mathbf{T}^d$.
\end{corollary}

In general, ergodic theory results do not give rates on how long it takes for a sequence to equidistribute over a set. This is not a problem in the constructions we perform, since the rates that our intervals shrink can be arbitrarily fast. However, it is important to note that the convergence rates are uniform across all intervals.

\begin{theorem}
    If $x_n$ is equidistributed over a torus $\mathbf{T}^d$, and
    %
    \[ A_N = \sup_I \left| \frac{\# \{ 1 \leq n \leq N : x_n \in I \}}{N} - |I| \right| \]
    %
    where $I$ ranges over all boxes in $\mathbf{T}^d$, then $A_N \to 0$ as $N \to \infty$.
\end{theorem}
\begin{proof}
    For notational simplicity, we let
    %
    \[ \# (I,N) = \# \{ 1 \leq n \leq N: x_n \in I \} \]
    %
    For each $n$, we can partition $\mathbf{T}^d$ into finitely many disjoint cubes $\{ I_n \}$ with sidelengths $1/n$. Since there are only finitely many such cubes, there is $N_n$ such that for $M \geq N_n$, $| \#(I_n,M)/M - |I_n|| \leq 1/n$. Now given any box $J$, we can find sets $J_1$ and $J_2$, each unions of the cubes $I_n$, with $J_1 \subset J \subset J_2$ and $|J - J_1|, |J_2 - J| \lesssim_d 1/n$. Thus
    %
    \[ |J| - \frac{2}{n} \leq |J_1| - \frac{1}{n} \leq \frac{\#(J_1,M)}{M} \leq \frac{\#(J,M)}{M} \leq \frac{\#(J_2,M)}{M} \leq |J_2| + \frac{1}{n} \leq |J| + \frac{2}{n} \]
    %
    which completes the proof, since $N_n$ is independent of $J$.
\end{proof}








\section{Results about Hypergraphs}

\begin{lemma}[Tur\'{a}n]
    For any $k$ uniform hypergraph $H = (V,E)$ with $|E| \leq |V|^\alpha$, $V$ contains an independant set of size $\Omega(|V|^{(k-\alpha)/(k-1)})$.
\end{lemma}
\begin{proof}
    We create an independant set $I$ by the following procedure. First, select a subset $S$ of vertices, including each independantly with probability $p$. Delete a single vertex from each edge in each hypergraph entirely contained in $S$, obtaining an independant set $I$. We find that each edge in $V$ is entirely included in $S$ with probability $p^k$, and $S$ has expected size $p |V|$, so $\mathbf{E}|I| = p |V| - p^k |E|$. If $|E| = |V|^\alpha$ for $\alpha \geq 1$, then setting $p = (1/2) |V|^{(1 - \alpha)/(k-1)}$ induces a set $I$ with size
    %
    \[ |V|^{(k - \alpha)/(k-1)}(1/2 - 1/2^k) \]

    We create an independant set $I$ by the following procedure. First, select a subset $S$ of vertices, including each vertex independantly with probability $p$. Delete a single vertex from each edge in each hypergraph which is entirely contained in $S$. Then $I$ is an independant set with respect to each hypergraph, and we shall show that for an appropriate choice of $p$, $\mathbf{E} |I| \geq h$.

    Trivially, we find $\mathbf{E}|S| = p |V|$. For any $i \geq 2$, the expected number of edges of $H_i$ falling entirely in $S$ is
    %
    \[ p^i |E_i| \leq \frac{p^i |V|^i}{c_k h^{i-1}} \]
    %
    therefore
    %
    \[ \mathbf{E}|I| = p|V| - \sum_{i = 2}^k \frac{p^i |V|^i}{c_k h^{i-1}} \]
    %
    Setting $p = 2h/|V|$ and $c_k = 2^{k+1}$ gives
    %
    \[ \mathbf{E}|I| = h \left( 2 - \sum_{i = 2}^k \frac{1}{2^{k+1-i}} \right) > h \]
    %
    which completes the proof.
\end{proof}

\section{Hyperdyadic Covers}

Recall the definition of the Hausdorff measure $H^\alpha(E) = \lim_{\delta \to 0} H^\alpha_\delta(E)$, where $H^\alpha_\delta(E)$ is the greatest lower bound of $\sum r_n^\alpha$, over all choices of covers of $E$ by cubes $I_1, I_2, \dots$, where $I_n$ has sidelengths $r_n$. We then define the Hausdorff dimension of $E$ to be the least upper bound of the scalars $\alpha$ such that $H^\alpha(E) = 0$, or alternatively, the greatest lower bound of $\alpha$ such that $H^\alpha(E) = \infty$.

To determine the Hausdorff dimension of $E$, it suffices to consider only dyadic cubes in the cover of $E$. Define $H^\alpha(E) = \lim_{\delta \to 0} H^\alpha_{D,\varepsilon}(E)$, where $H^\alpha_{D,\varepsilon}(E)$ is the greatest lower bound of $\sum r_n^\alpha$ over {\it dyadic} covers $I_1, I_2, \dots$, with $I_n \in \mathcal{B}(r_n)$. Then $H^\alpha_D$ is comparable with $H^\alpha$.

\begin{theorem}
    For any set $E$, $H^\alpha(E) \leq H^\alpha_D(E) \leq 2^{d + \alpha} H^\alpha(E)$.
\end{theorem}
\begin{proof}
    Given any not necessarily dyadic cover $I_1, I_2, \dots$, we can replace each sidelength $r_n$ cube $I_n$ with at most $2^d$ dyadic cubes with radius at most $2r_n$, which gives $H^\alpha_{D,\varepsilon}(E) \leq 2^{1 + \alpha} H^\alpha_\varepsilon(E)$, and taking the limit as $\varepsilon \to 0$ then gives the required upper bound for $H^\alpha_D$.
\end{proof}

If we are restricting ourselves to cubes lying at a series of discrete scales, it seems as if the dyadic sequence is about as fast as we can use so that the resultant Hausdorff measure is comparable to the usual Hausdorff measure. Nonetheless, using a weak type bound we can get results for a faster decreasing family of scales. This is necessary for our calculations. We fix a positive $\delta$, and consider a sequence of {\bf hyperdyadic scales} $H_N = 2^{- \lfloor (1 + \delta)^N \rfloor}$. A {\bf hyperdyadic cube} is then a cube in $\mathcal{B}(H_N)$ for some $N$.

%To measure the difference in decay rates between hyperdyadic and dyadic scales, we note that for any $n$, and $0 < A < 1$, the number of dyadic scales between $A$ and $A^n$ is comparable to $n \log(1/A)$, whereas the number of hyperdyadic scales is comparable to $\log(n) / \log(1 + \delta)$, which is completely independant of $A$. As is expected, a naive covering approach as in the last argument doesn't suffice to give results about dimensions and hyperdyadic coverings.

\begin{proof}
    For any sidelength $L$ cube, we can cover the cube by at most $2^d$ hyperdyadic cubes with sidelength at most $2L^{1 - \delta} \geq 2L^{(1+\delta)^{-1}}$. This is because
    %
    \[ 2 H_{N+1}^{(1 + \delta)^{-1}} = 2^{1 - (1 + \delta)^{-1} \lfloor (1 + \delta)^{N+1} \rfloor} \geq 2^{1 - (1 + \delta)^N} \geq 2^{\lfloor (1 + \delta)^N \rfloor} = H_N \]
    %
    If $E$ has Hausdorff dimension $\alpha$, for every $\varepsilon$ and $N$ we can find a collection of dyadic cubes $I_1, I_2, \dots$ covering $E$ with $I_k$ sidelength $L_k \leq H_N$, and $\sum L_{N,i}^{\alpha + \varepsilon} \lesssim_\varepsilon 1$. A weak type bound implies the number of cubes $I_k$ with $H_{N+1} \leq L_k \leq H_N$ is $O_\varepsilon(1/H_{N+1}^{\alpha + \varepsilon})$. But
    %
    \[ 1/H_{N+1}^{\alpha + \varepsilon} \leq (H_N/H_{N+1})^{\alpha + \varepsilon} 1/H_N^{\alpha + \varepsilon} \lesssim 1 / H_N^{\alpha + \varepsilon + \delta} \]
    %
    and so the cover of $E$ by hyperdyadic cubes contains $O_\varepsilon(1/H_N^{\alpha + \varepsilon + \delta})$ length $H_N$ cubes for each $N$.



    If we swap each cube $I_{N,i}$ with $2^d$ hyperdyadic cubes of length at most $2L^{1 - \delta}$, we obtain
    %
    \begin{align*}
        \sum 2^d (2 L_{N,i}^{1 - \delta})^{\alpha + \varepsilon} &= 2^{d + \alpha + \varepsilon} \sum L_{N,i}^{(1 - \delta)(\alpha + \varepsilon)} \lesssim_\varepsilon 1
    \end{align*}
    %
    Thus $H^{(1 - \delta)\alpha + \varepsilon}_{HD}(E) \lesssim_\varepsilon 1$.

    We can swap each cube $I_i$ with $2^d$ hyperdyadic cubes of length at most $2L^{(1 + \delta)^{-1}}$, without effecting the estimate too much.

    Then for every hyperdyadic number $H_N$, we can find a collection of cubes $I_{N,1}, I_{N,2}, \dots$ covering $E$ with $I_{N,i}$ sidelength $r_{N,i} \leq H_N$, and $\sum r_{N,i}^{\alpha + \varepsilon} \lesssim_\varepsilon 1$. Covering each cube by $2^d$ cubes with hyperdyadic sidelengths, which magnifies $r_{N,i}$ by at most
    %
    \[ 2 \cdot 2^{(1 + \delta)^{N+1} - (1 + \delta)^N} = 2 \cdot 2^{\delta (1 + \delta)^N} \lesssim 2 \cdot r_{N,i}^{- \delta} \]
    %
    We conclude that
    %
    \[ 2^{d+\alpha+\varepsilon} 2^{(\alpha + \varepsilon) \delta(1 + \delta)^N} C_\varepsilon \]
\end{proof}

We assume $\delta$ and $\varepsilon$ are some fixed parameters. If $A(\varepsilon, \delta)$ and $B(\varepsilon,\delta)$ are two quantities depending on $\varepsilon$ and $\delta$, we write $A \preccurlyeq B$ mean $A \lesssim_\varepsilon \delta^{-C \varepsilon} B$ for some $C$, and for every $\varepsilon$. We let $A \approx B$ mean $A \preccurlyeq B$ and $B \preccurlyeq A$ hold simultaneously. We say a union of balls is $\delta$ discretized if it is the union of balls with radius $\approx \delta$. Thus there exists $C_\varepsilon$ and $C$ such that for each ball $B_r$ of radius $r$, $|r - \delta| \leq C_\varepsilon \delta^{1-C \varepsilon}$. Thus
%
\[ \delta(1 - C_\varepsilon \delta^{-C \varepsilon}) \leq r \leq \delta(1 + C_\varepsilon \delta^{- C \varepsilon}) \]
%
In particular, the dyadic scales $2^{-\lfloor (1 + \varepsilon)^k \rfloor}$ are allowed in a discretization of a hyperdyadic scale $2^{-(1+\varepsilon)^k}$, since we can choose $C_\varepsilon$ and $C$ such that
%
\[ 1 - C_\varepsilon 2^{C (1 + \varepsilon)^k \varepsilon} \leq 1 \leq 2^{(1 + \varepsilon)^k -\lfloor (1 + \varepsilon)^k \rfloor} \leq 2 \leq 1 + C_\varepsilon 2^{(1 + \varepsilon)^k C \varepsilon} \]

\begin{theorem}
    Let $E$ be a compact subset of $\mathbf{R}^n$. If $0 < \alpha < n$, and $\dim(E) \leq \alpha$, then for each hyperdyadic number $\delta$, we can associate a $\delta$ discretized set $X_\delta$ with $|X_\delta \cap B(x,r)| \preccurlyeq \delta^n (r/\delta)^\alpha$ for all $\delta \leq r \leq 1$ and $x \in \mathbf{R}^n$, and every element of $E$ is contained in infinitely many of the $X_\delta$.
\end{theorem}
\begin{proof}
    Fix $E$. For every hyperdyadic $\delta$, we can find a cover of $E$ by balls $B(x_{\delta n}, r_{\delta n})$ such that $r_{\delta n} < \delta$, and
    %
    \begin{equation} \sum_n r_{\delta n}^{\alpha + C\varepsilon} \lesssim 1 \end{equation}
    %
    Choose $m_{\delta n}$ such that $2^{-(1 + \varepsilon)^{m_{\delta n}+1}} \leq r_{\delta n} \leq 2^{-(1 + \varepsilon)^{m_{\delta n}}}$. We calculate
    %
    \begin{align*}
        \frac{2^{-(\alpha + C'\varepsilon) (1 + \varepsilon)^{m_{\delta n}}}}{r_{\delta n}^{\alpha + C\varepsilon}} &\leq \frac{2^{- (\alpha + C'\varepsilon) (1 + \varepsilon)^{m_{\delta n}}}}{2^{- (\alpha + C\varepsilon) (1 + \varepsilon)^{m_{\delta n} + 1}}}\\
        &= \left( 2^{\varepsilon (1 + \varepsilon)^{m_{\delta n}}} \right)^{\alpha + (C (1 + \varepsilon) - C')}
    \end{align*}
    %
    Provided that $C' > \alpha + C(1 + \varepsilon)$, the quantity on the left is $\leq 1$, which is independant of $\varepsilon$ provided that $\varepsilon$ is bounded from above, and so we conclude
    %
    \[ \sum_n \left( 2^{-(1 + \varepsilon)^{m_{\delta n}}} \right)^{\alpha + C' \varepsilon} \leq \sum_n r_{\delta n}^{\alpha + C\varepsilon} \]
    %
    Thus we may assume by changing the value of $C$ that the quantities $r_{\delta n}$ are hyperdyadic from the outset. This means that at each hyperdyadic scale $\delta$, the number of hyperdyadic balls at the scale $\delta$ in each cover is $\lesssim (1/\delta)^{\alpha + C\varepsilon}$. STOP IS THIS ALL WE NEED, THEN COME BACK TO THE PROOF.


    For a pair of hyperdyadic numbers $\delta$ and $\gamma$ we set
    %
    \[ Y_{\delta \gamma} = \bigcup_{r_{\delta n} = \gamma} B(x_{\delta n}, r_{\delta n}) \]
    %
    Every element of $X$ is in infinitely many of the $Y_{\delta n}$. For each $\delta$ and $\gamma$, we let $Q_{\delta \gamma}$ be the collection of hyperdyadic cubes with sidelength at least $\gamma$ covering $Y_{\delta \gamma}$ and minimizing $\sum_{Q \in Q_{\delta \gamma}} l(Q)^\alpha$. From condition (1.1) we obtain that $Y_{\delta \gamma}$ can be covered by at most $r^{-\alpha - \varepsilon}$ sidelength $r$ cubes, so
    %
    \[ \sum_{Q \in Q_{\delta \gamma}} l(Q)^\alpha \leq Cr^{-\varepsilon} \]
    %
    and so $l(Q) \leq Cr^{-\varepsilon/\alpha}$ for all $Q \in Q_{\delta \gamma}$. From the construction of $Q_{\delta \gamma}$, we see that the $Q$ are all disjoint, and for any hyperdyadic cube $I$,
    %
    \[ \sum_{\substack{Q \in Q_{\delta \gamma}\\Q \subset I}} l(Q)^\alpha \leq l(I)^\alpha \]
    %
    since otherwise we could replace such elements of $Q$ in $Q_{\delta \gamma}$ by $I$ itself.
\end{proof}

\endinput