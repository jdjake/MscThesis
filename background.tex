%% The following is a directive for TeXShop to indicate the main file
%%!TEX root = diss.tex

\chapter{Background}
\label{ch:Background}

\section{Configuration Avoidance}

This thesis discusses methods to form large sets avoiding patterns. First, we introduce notation in a general context. We consider an ambient set $\AAA$. It's \emph{$n$-point configuration space} is the set of distinct tuples of $n$ points in $\AAA$, i.e.
%
\[ \Config^n(\AAA) = \{ (x_1, \dots, x_n) \in X^n: x_i \neq x_j\ \text{if $i \neq j$} \}. \]
%
The general \emph{configuration space} of $\AAA$ is $\Config(\AAA) = \bigcup_{n = 1}^\infty \Config^n(\AAA)$. A \emph{pattern}, or \emph{configuration}, on $\AAA$ is a subset of $\Config(X)$, and we say a subset $Y$ of $X$ \emph{avoids} a configuration $\C$ if $\Config(Y)$ is disjoint from $\C$. We say $\C$ is an \emph{$n$ point configuration} if it is a subset of $\Config^n(X)$.

\begin{example}[Isoceles Triangle Configuration]
	Set
	%
	\[ \C = \left\{ (x_1, x_2, x_3) \in \Config^3(\RR^2) : |x_1-x_2| = |x_1-x_3| \right\}. \]
	%
	Then $\C$ is a 3-point configuration, and a set $X \subset \RR^2$ avoids $\C$ if and only if it contains no vertices of an isoceles triangle. %Notice that $|x_1 - x_2| = |x_1 - x_3|$ holds if and only if $|x_1 - x_2|^2 = |x_1 - x_3|^2$, which is an algebraic equation in the coordinates of $x_1,x_2$, and $x_3$. Thus $\C$ is an algebraic hypersurface of degree two in $\RR^6$.
\end{example}

%\begin{example}[General Position Configuration]
%	Suppose we wish to find a subset $X$ of $\RR^d$ such that for each positive integer $k \leq d$, and for each collection of $k+1$ distinct points $x_1, \dots, x_{k+1} \in X$, the points do not lie in a $k-1$ dimensional hyperplane. For each $k \leq d$, set
	%
%	\[ \C^{k+1} = \{ (x_0, x_1, \dots, x_k) \in \Config^{k+1}(\RR^d): x_1-x_0, \dots, x_k - x_0\ \text{are linearly dependant} \}. \]
	%
%	If we define $\C = \bigcup_{k = 2}^d \C^k$, then a set $X$ avoids $\C$ precisely when all finite collection of distinct points in $X$ lie in general position. Notice that
	%
%	\[ \C^{k+1} = \bigcup \left\{ \text{span}(y_1, \dots, y_k) \times \{ y \} : y = (y_1, \dots, y_k) \in \Config^k(\RR^d) \right\} \cap \Config^{k+1}(\RR^d). \]
	%
%	so each $\C^{k+1}$ is essentially a union of $k$ dimensional hyperplanes.
%\end{example}

Even though our problem formulation assumes configurations are formed by distinct sets of points, one can still formulate avoidance problems involving repeated points in our framework, because an instance of a configuration involving $n$ points which may contain repetitions can be seen as an instance of a configuration involving fewer than $n$ distinct points.

\begin{example}[Sum Set Configuration]
	Let $G$ be an abelian group, and fix $Y \subset G$. Set
	%
	\[ \C^1 = \{ g \in \Config^1(G): g + g \in Y \} \quad \text{and} \quad \C^2 = \{ (g_1,g_2) \in \Config^2(G): g_1 + g_2 \in Y \}. \]
	%
	Then set $\C = \C^1 \cup \C^2$. A set $X \subset G$ avoids $\C$ if and only if $(X + X) \cap Y = \emptyset$.
\end{example}

Our main focus in this thesis is on the {\it pattern avoidance problem}: Given a configuration $\C$ on $\AAA$, what is the maximal size of a set $X \subset \AAA$ avoiding $\C$? Depending on the structure of the ambient space $\AAA$ and the configuration $\C$, there are various ways of measuring the size of the set $X$:
%
\begin{itemize}
	\item If $\AAA$ is finite, the goal is to find a set $X$ with large cardinality.
	\item If $\{ \AAA_n \}$ is an increasing family of finite sets with $\AAA = \lim \AAA_n$, the goal is to find a set $X$ such that $X \cap \AAA_n$ has large cardinality asymptotically in $n$.
	\item If $\AAA = \RR^d$, but $\C$ is sufficiently discrete, then a satisfactory goal is to find a set $X$ with large Lebesgue measure avoiding $\C$.
\end{itemize}
%
In this thesis, inspired by results in these three settings, we establish methods for avoiding non-discrete configurations $\C$ in $\RR^d$. Here, Lebesgue measure completely fails to measure the size of pattern avoiding solutions, as the next theorem shows, under the often true assumption that the configuration is \emph{translation invariant}, i.e. that if $(a_1, \dots, a_n) \in \C$ and $b \in \RR^d$, $(a_1 + b, \dots, a_n + b) \in \C$.

\begin{theorem}
	Let $\C$ be a translation-invariant $n$-point configuration on $\RR^d$. Suppose
	%
	\begin{enumerate}
		\item \label{nonDiscreteConfig} For any $\varepsilon > 0$, there is $(a_1, \dots, a_n) \in \C$ with $\diam \{ a_1, \dots, a_n \} \leq \varepsilon$.
	\end{enumerate}
	%
	Then no set with positive Lebesgue measure avoids $\C$.
\end{theorem}
\begin{proof}
	Let $X \subset \RR^d$ have positive Lebesgue measure. For a point $x \in \RR^d$, we let
	%
	\[ I(x,r) = [x_1 - r/2, x_1 + r/2] \times \dots \times [x_n - r/2, x_n + r/2]. \]
	%
	denote the coordinate-axis oriented cube centered at $x$ with sidelength $r$. The Lebesgue density theorem shows that there exists a point $x \in X$ such that
	%
	\[ \lim_{r \to 0} \frac{|X \cap I(x,r)|}{|I(x,r)|} = 1. \]
	%
	We fix $\varepsilon > 0$, to be chosen later, and pick $r_0$ small enough such that
	%
	\[ |X \cap I(x,r_0)| \geq (1 - \varepsilon)|I(x,r_0)|. \]
	%
	Applying Property \ref{nonDiscreteConfig}, we find $C = (a_1, \dots, a_n) \in \C$ with $\diam \{ a_1, \dots, a_n \} \leq \varepsilon r_0$. For each $p \in I(x,r_0)$, let $C(p) = (a_1(p), \dots, a_n(p))$, where $a_i(p) = p + (a_i - a_1)$. By translation invariance, $C(p) \in \C$ for each $p \in \RR^d$. A union bound gives
	%
	\begin{align*}
		\left| \{ p \in I(x,r_0/2) : C(p) \not \in \C(X) \} \right| &= \left| \bigcup_{i = 1}^n \{ p \in I(x,r_0/2) : a_i(p) \not \in X \} \right|\\
		&\leq \sum_{i = 1}^n |I(x,r_0/2) \cap (X + (a_1 - a_i))^c|\\
		&= \sum_{i = 1}^n |I(x,r_0/2)| - |I(x+a_i-a_1,r_0/2) \cap X|\\
		&\leq n \big[ |I(x,r_0/2)| - |I(x,r_0(1/2 - \varepsilon)) \cap X| \big]\\
		&\leq n r_0^d \left[ 1/2^d - (1/2 - \varepsilon)^d \right] \\
		&\leq \varepsilon \cdot n (2d + 1) \cdot (r_0/2)^d.
	\end{align*}
	%
	If $\varepsilon < 1/n(2d + 1)$, we conclude that
	%
	\begin{align*}
		\left|\left\{ p \in I(x,r_0/2): C(p) \in \Config(X) \right\}\right| &= |I(x,r_0/2)| - \left| \left\{ p \in I(x,r_0/2): C(p) \not \in \Config(X) \right\} \right|\\
		&\geq \varepsilon \cdot n(2d + 1) (r_0/2)^d - (r_0/2)^d > 0.
	\end{align*}
	%
	Thus there is $p \in I(x,r_0/2)$ such that $C(p) \in \Config(X)$, so $X$ does not avoid $\C$.
\end{proof}

Since no set of positive Lebesgue measure can avoid non-discrete configurations, we cannot use the Lebesgue measure to quantify the size of pattern avoiding sets. Fortunately, there is a quantity which can distinguish between the size of sets of measure zero. This is the \emph{fractional dimension} of a set, introduced in the next section.







\section{Fractal Dimension}

Intuitively, fractional dimension provides a measure of the local density of a set, and so we view a set which is more dense as `larger'. There are two main definitions of fractional dimension we use in this thesis: Minkowski dimension and Hausdorff dimension. The main difference between the first two is that Minkowski dimension measures relative density at a single scale, whereas Hausdorff dimension measures relative density at various scales. In this section, we indicate the subtle differences between the two dimensional quantities which are relavent to this thesis.

We begin by discussing the Minkowski dimension, which is the easiest of the two dimensions to define. Given a length $l$, and a bounded set $E \subset \RR^d$, we let $N(l,E)$ denote the length $l$ covering number of $E$, i.e. the minimum number of sidelength $l$ cubes required to cover $E$. We define the \emph{lower} and \emph{upper} Minkowski dimension as
%
\[ \lowminkdim(E) = \liminf_{l \to 0} \left( \frac{\log(N(l,E))}{\log(1/l)} \right)\ \ \ \ \ \upminkdim(E) = \limsup_{l \to 0} \left( \frac{\log(N(l,E))}{\log(1/l)} \right). \]
%
If $\upminkdim(E) = \lowminkdim(E)$, then we refer to this common quantity as the \emph{Minkowski dimension} of $E$, denoted $\dim_M(E)$. Intuitively, $\lowminkdim(E) < s$ if there {\it exists} a sequence of lengths $\{ l_k \}$ converging to zero with $N(l_k,E) \leq (1/l_k)^s$, and $\upminkdim(E) < s$ if $N(l,E) \leq (1/l)^s$ for \emph{all} sufficiently small lengths $l$.

%It is obvious that $|E| \leq N(l,E) l^d$. If $\upminkdim(E) < s < d$, then there is a sequence $\{ l_k \}$ such that $N(l,E) \leq (1/l_k)^s$ for all $k$, which gives $|E| \leq l_k^{d-s} \to 0$. Thus $E$ has Lebesgue measure zero. Taking contrapositives, we find every set with positive Lebesgue measure has Minkowski dimension $d$. This is why the Minkowski dimension provides a `second order' for comparing the size of sets with Lebesgue measure zero. Based on Theorem 2, it provides a perfect substitute of size for the pattern avoidance problem, at least once we show that there do exist sets with positive Minkowski dimension avoiding particular examples of non-discrete configurations.

Later on, it will be helpful to further discretize the covering number of sets so that all the cubes in the cover lie on a grids. We let
%
\[ \B(l,\RR^d) = \left\{ [a_1,a_1 + l) \times \cdots \times [a_d,a_d + l): a_k \in l \cdot \ZZ \right\}. \]
%
Given any $E \subset \RR^d$, we let
%
\[ \B(l,E) = \left\{ I \in \B(l,\RR^d): I \cap E \neq \emptyset \right\}. \]
%
Up to a constant factor, $\B(l,E)$ is an optimal covering of $E$ by cubes.

\begin{lemma} \label{comparableCovers}
	For any bounded set $E$,
	%
	\[ N(l,E) \leq \#(\B(l,E)) \leq 2^d N(l,E) \]
\end{lemma}
\begin{proof}
	Since $\B(l,E)$ is a cover of $E$, and $N(l,E)$ is the size of a minimal cover, $N(l,E) \leq \#(\B(l,E))$. But if $I$ is an arbitrary sidelength $l$ cube, then it intersects at most $2^d$ cubes in $\B(l,E)$, so $\# \B(l,E) \leq 2^d N(l,E)$.
\end{proof}

It is also useful to work at a discrete sequence of `dyadic' scales. If we fix $M > 1$, and consider $1/M^{k+1} \leq l \leq 1/M^k$, then as $k \to \infty$, $\log(1/l) \sim \log(M^k)$. To determine the Minkowski dimension, it suffices to look at the scales $\{ 1/M^k \}$, which we can combine with the discretization of cubes.

\begin{corollary}
	For any set $E$,
	%
	\[ \lowminkdim(E) = \liminf_{k \to \infty} \frac{\log_M(\# \B(1/M^k,E))}{k}\ \ \ \ \ \upminkdim(E) = \limsup_{k \to \infty} \frac{\log_M(\# \B(1/M^k,E))}{k}. \]
\end{corollary}
\begin{proof}
	Suppose $1/M^{k+1} \leq l \leq 1/M^k$. Then the upper bound of Lemma 1 implies
	%
	\[ \frac{\log(N(l,E))}{\log(1/l)} \sim \frac{\log_M(N(l,E))}{k+1} \leq \frac{\log_M(N(1/M^{k+1},E))}{k+1} \leq \frac{\log_M(\# \B(1/M^{k+1},E))}{k+1}. \]
	%
	Conversely, the lower bound of Lemma 1 implies that
	%
	\begin{align*}
		\frac{\log(N(l,E))}{\log(1/l)} &\sim \frac{\log_M(N(l,E))}{k} \geq \frac{\log_M(N(1/M^k,E))}{k}\\
		&\geq \frac{\log_M(2^d \# \B(1/M^k,E))}{k} = \frac{\log_M(\# \B(1/M^k,E))}{k} + o(1).
	\end{align*}
	%
	Taking $l \to 0$, and correspondingly, $k \to \infty$, completes the proof.
\end{proof}

%Note that if $E$ is a set with positive Lebesgue measure, which we may without loss of generality assume is contained in $[0,1]^d$, and we subdivide $[0,1]^d$ into $m^d$ sidelength $1/m$ cubes, then $E$ intersects at least $|E| m^d$ of these cubes, so as $m \to \infty$,
%
%\[ \frac{\log \# \B(1/m,E)}{\log(m)} \geq \frac{\log(|E| m^d)}{\log(m)} = d + o(1) \]

%\begin{example}
%	If $E = B^k \times \{ 0 \}^{n-k}$, where $B^k$ is the $k$ dimensional unit ball, then
	%
%	\[ B^k \times \delta B^{n-k} \subset E_\delta \subset (1 + \delta)B^k \times \delta B^{n-k} \]
	%
%	which shows that
	%
%	\[ \delta^{n-k} \lesssim |E_\delta| \lesssim (1 + \delta)^k \delta^{n-k} \]
	%
%	Thus $\dim_M(E) = k$.
%\end{example}

%It is also useful to work at a discrete sequence of `dyadic-type' scales. If we fix $M$, and consider $1/M^{k+1} \leq l \leq 1/M^k$, then as $k \to \infty$, we find $\log(1/l) \sim \log(M^k)$.
%
%\[ \frac{N(1/M^k,E)}{\log(1/M_k)} \sim \frac{N(1/M^k,E)}{\log(1/l)} \leq \frac{N(l,E)}{\log(1/l)} \leq \frac{N(1/M^{k+1},E)}{\log(1/l)} \sim \frac{N(1/M^{k+1},E)}{\log(1/M_{k+1})} \]
%
%Taking $k \to \infty$, we conclude that for any set $E$ and any integer $M$,
%
%\[ \lowminkdim(E) = \liminf_{k \to \infty} \frac{N(1/M^k,E)}{\log(1/M^k)}\quad\text{and}\quad \upminkdim(E) = \limsup_{k \to \infty} \frac{N(1/M^k,E)}{\log(1/M^k)}. \]
%
%The second technique is using `Cantor-type' decompositions of sets, as indicated in the next example.

%\begin{example}
%	Let
	%
%	\[ C = \left\{ \sum_{i = 1}^\infty a_i/4^i : a_i \in \{ 0, 3 \} \right\} \]
	%
%	A natural choice of dyadic-type scales to study the Minkowski dimension of $C$ is the sequence $\{ 1/4^k \}$. We have $C = \lim_{k \to \infty} C_k$, where
	%
%	\[ C_k = \left\{ \sum_{i = 1}^\infty a_i/4^i : a_1, \dots, a_{N+1} \in \{ 0, 3 \} \right\}. \]
	%
%	The set $C_k$ is a union of $2^k$ cubes in $\B(1/4^k,\RR)$, and every cube in this union intersects $C$. Thus
	%
%	\[ \frac{\log(\# \B(1/4^k,C))}{\log(4^k)} = \frac{\log(\# \B(1/4^k,C_k))}{\log(4^k)} = \frac{\log(2^k)}{\log(4^k)} = \log_4(2) = 1/2. \]
	%
%	Thus $C$ has Minkowski dimension $1/2$.
%\end{example}

%We reemphasize that the dimension of $C$ was most easily calculated using a \emph{Cantor-type} decomposition. We took $C$ as the limit of a nested sequence of sets $\{ C_k \}$, which were simple unions of intervals in $\B(l_k,C)$, where $l_k = 1/4^k$. Our main methods for pattern avoidance will also use `Cantor-type' decompositions of sets, both for analyzing the set's dimension, and also as an intrinsic part of the construction of the set. Unfortunately, this construction must occur at a sequence of scales which decreases much faster than scales of dyadic type, and so we cannot just rely on just counting intervals to determine dimension.

%It is especially useful when trying to restrict attention to a finite set of scales, because we can view $\bigcup \B^d(l,E)$ as a `discretization' of a set $E$ at the scale $l$.

%\begin{example}
%	We will often consider sets whose dimension behaves differently at various scales. This often occurs when performing multi-scale constructions where the scales in the construction decay inverse superexponentially. A toy example of this phenomenon can be obtained by modifying the last example slightly so that the construction of the Cantor set behaves differently at various scales. We fix an increasing sequence of integers $\{ N_k \}$, with $N_0 = 0$, and consider
	%
%	\[ C = \left\{ \sum_{i = 1}^\infty a_i/4^i : a_i \in \{ 0, 3 \}\ \text{if there is $k \geq 0$ such that}\ N_{2k} \leq i \leq N_{2k+1} \right\}. \]
	%
%	Then $C = \lim C_n$, where
	%
%	\[ C_n = \left\{ \sum_{i = 1}^\infty a_i/4^i : a_i \in \{ 0, 3 \}\ \text{if there is $k \leq n$ such that}\ N_{2k} \leq i \leq N_{2k+1} \right\}. \]
	%
%	Counting the number of choices of the $a_i$ given the constraints in the definition of $C_n$, we find $C_n$ is the union of
	%
%	\[ \prod_{k = 0}^{n-1} 2^{N_{2k+1} - N_{2k}} \prod_{k = 0}^n 4^{N_{k2+2} - N_{2k+1}} = 2^{-N_1+N_2-N_3+\cdots-N_{2n-1}+2N_{2n}} \geq 2^{2N_{2n} - N_{2n-1}}. \]
	%
%	sidelength $l_n$ cubes, where $l_n = 1/4^{N_{2n}}$. Each of these cubes intersects $C$, so
	%
%	\[ \frac{\log \# \B^1(l_n,C)}{\log(1/l_n)} \geq \frac{2N_{2n} - N_{2n-1}}{2N_{2n}} = 1 - \frac{N_{2n-1}}{2N_{2n}}. \]
	%
%	Provided that $N_{2n-1}/N_{2n} = o(1)$, which occurs if the values $N_k$ increase superexponentially, i.e. if $N_k = 2^{k^2}$, we conclude that $\upminkdim(C) = 1$. On the other hand, $C_n$ is also the union of
	%
%	\[ \prod_{k = 0}^n 2^{N_{2k+1} - N_{2k}} \prod_{k = 0}^n 4^{N_{2k+2} - N_{2k+1}} = 2^{-N_1+N_2-\cdots+N_{2n}+N_{2n+1}} \leq 2^{N_{2n} + N_{2n+1}} \]
	%
%	sidelength $r_n$ cubes, where $r_n = 1/4^{N_{2n+1}}$. Thus
	%
%	\[ \frac{\log \# \B^1(r_n,C)}{\log(1/l_n)} \leq \frac{N_{2n} + N_{2n+1}}{2N_{2n+1}} = 1/2 + \frac{N_{2n}}{2N_{2n+1}}. \]
	%
%	Again, if $N_{2n}/N_{2n+1} = o(1)$, then $\lowminkdim(C) \leq 1/2$. One can fairly easily check that the values $r_n$ are the `worst case' scales, so that $\lowminkdim(C) = 1/2$. Thus the set $C$ `looks' half dimension between $l_n$ and $r_n$, for each $n$, but `looks' full dimensional between the scales $r_n$ and $l_{n+1}$. This actually impacts the Minkowski dimension of $C$ provided that the gaps between $l_n$ and $r_n$ are made inverse superexponential.
%\end{example}

Hausdorff dimension is a version of fractal dimension which is more stable under analytical operations, but is less easy to calculate. It is obtained by finding a canonical `$s$ dimensional measure' $H^s$ on $\RR^d$ for $s \in [0,\infty)$, which for $s = 1$, measures the `length' of a set, for $s = 2$, measure the `area', and so on and so forth. We then set the dimension of $E$ to be the supremum of the values $s$ such that $H^s(E) < \infty$. Given a cube $I$, we let $l(I)$ denote it's sidelength. For $E \subset \RR^d$, we define the \emph{Hausdorff content}
%
\[ H_\delta^s(E) = \inf \left\{ \sum_{k = 1}^\infty l(I_k)^s : E \subset \bigcup_{k = 1}^\infty I_k, l(I_k) \leq \delta \right\}. \]
%
We then define $H^s(E) = \lim_{\delta \to 0} H_\delta^s(E)$. It is easy to see $H^s$ is an exterior measure, and $H^s(E \cup F) = H^s(E) + H^s(F)$ if the Hausdorff distance $d(E,F)$ between $E$ and $F$ is positive. So $H^s$ is actually a metric exterior measure, and the Caratheodory extension theorem shows all Borel sets are measurable with respect to $H^s$.

%\begin{example}
%	Let $s = 0$. Then $H_\delta^0(E)$ is the number of $\delta$ balls it takes to cover $E$, which tends to $\infty$ as $\delta \to 0$ unless $E$ is finite, and in the finite case, $H_\delta^0(E) \to \# E$. Thus $H^0$ is just the counting measure.
%\end{example}

%\begin{example}
%	Let $s = d$. If $E$ has Lebesgue measure zero, then for any $\varepsilon > 0$, there exists a sequence of balls $\{ B(x_k,r_k) \}$ covering $E$ with
	%
%	\[ \sum_{k = 1}^\infty r_k^d < \varepsilon^d. \]
	%
%	Then we know $r_k < \varepsilon$, so $H^s_\varepsilon(E) < \varepsilon^d$. Letting $\varepsilon \to 0$, we conclude $H^d(E) = 0$. Thus $H^d$ is absolutely continuous with respect to the Lebesgue measure. The measure $H^d$ is translation invariant, so $H^d$ is actually a constant multiple of the Lebesgue measure.
%\end{example}

\begin{lemma}
	If $t < s$ and $H^t(E) < \infty$, $H^s(E) = 0$, and if $H^s(E) \neq 0$, $H^t(E) = \infty$.
\end{lemma}
\begin{proof}
	If, for any cover of $E$ by intervals $I_k$, $\sum l(I_k)^t \leq A$, and $l(I_k) \leq \delta$, then
	%
	\[ \sum l(I_k)^s \leq \sum l(I_k)^{s-t} l(I_k)^t \leq \delta^{s-t} A. \]
	%
	Thus $H^s_\delta(E) \leq \delta^{s-t} A$, and taking $\delta \to 0$, we conclude $H^s(E) = 0$. The latter point is just proved by taking contrapositives.
\end{proof}

\begin{example}
	We note $H^1[-N,N]^d = (2N)^d$. If $s > d$, then Lemma 2 shows $H^s[-N,N]^d = 0$, and so by countable additivity, $H^s(\RR^d) = 0$. Thus $H^s$ is trivial if $s > d$.
\end{example}

Given any Borel set $E$, the last example, combined with Lemma 2, implies there is a unique value $s_0 \in [0,d]$ such that $H^s(E) = 0$ for $s > s_0$, and $H^s(E) = \infty$ for $0 \leq s < s_0$, though it is possible for $H^{s_0}(E)$ to take any value in $[0,\infty]$. We refer to $s_0$ as the \emph{Hausdorff dimension} of $E$, denoted $\hausdim(E)$.

\begin{theorem}
	For any bounded set $E$, $\hausdim(E) \leq \lowminkdim(E) \leq \upminkdim(E)$.
\end{theorem}
\begin{proof}
	We consider the simple bound $H^s_l(E) \leq \# \B(l,E) \cdot l^s$. Taking $l \to 0$, we conclude that $H^s(E) \leq \liminf_{l \to 0}\; \# \B(l,E) \cdot l^s$. Fix $\varepsilon > 0$. If $s \geq \lowminkdim(E) + 2\varepsilon$, consider a sequence of lengths $\{ l_k \}$ converging to zero with
	%
	\[ \frac{\log(\# \B(l_k,E))}{\log(1/l_k)} \leq \lowminkdim + \varepsilon \leq s - \varepsilon. \]
	%
	Thus $\# \B(l_k,E) \leq (1/l_k)^{s - \varepsilon}$, so $H^s(E) \leq \lim \# \B(l_k,E) l_k^s = \lim l_k^\varepsilon = 0$. Taking $\varepsilon \to 0$ proves the claim.
\end{proof}

A simple intuition behind the two approaches to fractal dimension we have considered here is that Minkowski dimension measures the efficiency of covers of a set at a fixed scale, whereas Hausdorff dimension measures the efficiency of covers of a set at a simultaneous set of infinitely many scales, which explains both why in certain cases the Hausdorff dimension is smaller than the Minkowski dimension, and also why the Hausdorff dimension is more stable under analytical operations. For instance, given any sequence $\{ E_k \}$,
%
\[ \hausdim(\bigcup_{k = 1}^\infty E_k) = \sup \hausdim(E_k). \]
%
This need not be true for the Minkowski dimension; a single point has Minkowski dimension zero, but the rational numbers, which are a countable union of points, have Minkowski dimension one.

Just like with Minkowski dimension, we can also discretize Hausdorff dimension to cubes lying on a grid, and also lying on a set of dyadic scales.

\begin{lemma}
	For a fixed $M$, let
	%
	\[ H^{s,M}_N(E) = \inf \left\{ \sum_{k = 1}^\infty 1/M^{s i_k} : E \subset \bigcup_{k = 1}^\infty I_k,\; I_k \in \B(1/M^{i_k},\RR^d), \; \text{and}\ i_k \geq N \right\}, \]
	%
	and let $H^{s,M}(E) = \lim_{N \to \infty} H^{s,M}_N(E)$. Then
	%
	\[ H^s(E) \leq H^{s,M}(E) \leq 3^d M^s H^s(E), \]
	%
	for any Borel set $E$.
\end{lemma}
\begin{proof}
	Let $E \subset \bigcup_{k = 1}^\infty I_k$, where $l(I_k) \leq 1/M^N$ for each $k$. If $1/M^{i_k}$ is the largest power of $i_k$ such that $1/M^{i_k} \geq l(I_k)$, then $i_k \geq N$, and $\# \B(1/M^{i_k},I_k) \leq 3^d$. If we now consider the infinite family of cubes $\bigcup_{k = 1}^\infty \B(1/M^{i_k}, I_k)$, then
	%
	\[ H^{s,M}_N(E) \leq \sum_{k = 1}^\infty \# \B(1/M^{i_k},I_k) \cdot 1/M^{s i_k} \leq 3^d M^s \sum_{k = 1}^\infty l(I_k)^s \]
	%
	Since the cover was arbitrary, we conclude that $H^{s,M}_N(E) \leq 3^d M^s H^s_{1/M^N}(E)$, and taking $N \to \infty$ gives $H^{s,M}(E) \leq 3^d M^s H^s(E)$. The lower bound $H^{s,M}(E) \geq H^s(E)$ is trivial.
\end{proof}

It is often easy to upper bound Hausdorff dimension, but non-trivial to \emph{lower bound} the Hausdorff dimension of a given set. A key technique to finding a lower bound is \emph{Frostman's lemma}, which says that a set has large Hausdorff dimension if and only if it supports a probability measure which is suitably sparse. We say a measure $\mu$ is a \emph{Frostman measure} of dimension $s$ if it is non-zero, compactly supported, and for any length $l$, if $I$ is a cube with sidelength $l$, then $\mu(I) \lesssim l^s$. The proof of Frostman's lemma will indicate an important technique, known as the \emph{mass distribution principle}.

\begin{lemma}[Mass Distribution Principle]
	Let $\{ l_k \}$ be a decreasing sequence of lengths, and let $\B = \bigcup_{k = 1}^\infty \B(l_k,\RR^d)$. Suppose $\mu$ is a function from $\B$ to $[0,\infty)$ such that for each $k$, and for any $I \in \B(l_k,\RR^d)$,
	%
	\[ \sum \left\{ \mu(J) : J \in \B(l_{k+1},I) \right\} = \mu(I) \]
	%
	Then $\mu$ extends uniquely to a regular Borel measure on $\RR^d$.
\end{lemma}
\begin{proof}
	We can define a sequence of regular Borel measures $\mu_k$ by setting for each $f \in C_c(\RR^d)$,
	%
	\[ \int f d\mu_k = \sum \left\{ \mu(I) \int_I f\; dx : I \in \B(1/M^k,\RR^d) \right\} \]
	%
	Let $E_k$ be the operator on regular Borel measures given by the formula
	%
	\[ \int f(x) dE_k(\nu) = \sum \left\{ \nu(I) \int_I f(x) : I \in \B(l_k,\RR^d) \right\}. \]
	%
	The main condition of the theorem then says that $E_j(\mu_k) = \mu_j$ if $j \leq k$. Note that the operators $E_k$ are each continuous with respect to the weak topology; If $\nu_i \to \nu$ weakly, then $\nu_i(I) \to \nu(I)$ for each fixed $I \in \B(l_k,\RR^d)$. Thus if $f \in C_c(\RR^d)$, then the support of $f$ intersects only finitely many intervals in $\B(l_k,\RR^d)$, so
	%
	\[ \int f(x) dE_k(\nu_i) = \sum \nu_i(I) \int_I f\; dx \to \sum \nu(I) \int_I f\; dx = \int f dE_k(\nu). \]
	%
	Now fix an interval $I \in \B(l_0,\RR^d)$. The measures $\{ \mu_k|_I \}$, restricted to $I$, are uniformly bounded by $\mu(I)$. Thus the Banach-Alaoglu theorem implies that is a subsequence $\mu_{k_i}$ converging weakly on $I$ to some measure $\mu_I$. By continuity,
	%
	\[ E_j(\mu_I) = \lim E_j(\mu_{k_i}|_I) = \lim E_j(\mu_{k_i})|_I = \mu_j|_I. \]
	%
	This means precisely that $\mu_I$ is the extension of $\mu$ to a Borel measure on $I$. If we patch together the measures $\mu_I$ over all $I$, we obtain a measure extending $\mu$ on all of $\RR^d$. The uniqueness of the extension is guaranteed by the fact that the intervals upon which $\mu$ are defined generate the entire Borel sigma algebra.
\end{proof}

In the next lemma, it will help to notice that the definition of the Frostman measure is also robust to working over dyadic-type lengths. Suppose we can establish a result $\mu(I) \lesssim 1/M^{ks}$ for all $I \in \B(1/M^k,\RR^d)$ and all indices $k$. Given any length $l$, there is a value of $k$ such that $1/M^{k+1} \leq l \leq 1/M^k$. For any $I \in \B(l,\RR^d)$, $\# \B(1/M^k,I) \leq 3^d$, so
%
\[ \mu(I) \leq \mu \left(\bigcup \B(1/M^k,I) \right) \lesssim 3^d/M^{ks} \leq 3^d(l/M)^s \lesssim l^s. \]
%
Thus $\mu$ is a Frostman measure.

%\begin{lemma}
%	let $\mu^+$ be a function from $\B$ to $[0,\infty)$ such that for any $I \in \B(1/M^k,\RR^d)$,
	%
%	\[ \sum \left\{ \mu^+(J) :J \in \B(1/M^{k+1},I) \right\} \leq \mu^+(I) \]
	%
%	Assume there exists $c > 0$ such that for all $k$,
	%
%	\[ \sum \left\{ \mu^+(I) : I \in \B(1/M^k,I) \right\} \geq c \]
	%
%	and
	%
%	\[ \sum \left\{ \mu^+(I) : I \in \B(1,I) \right\} < \infty \]	
	%
%	Then there exists a non-zero Borel measure $\mu$ such that $\mu(I) \leq \mu^+(I)$ for $I \in \B$.
%\end{lemma}
%\begin{proof}
%	As in the last lemma, define the operators $E_k$ and the measures $\mu_k$. By weak compactness, a subsequence of these measures converge weakly to some measure $\mu$, and $E_k(\mu) = \lim E_k(\mu_{j_k}) \leq \mu_k$. The measure $\mu$ is nonzero, since $\| \mu_{j_k} \| \geq c$ for each $k$, and so $\| \mu \| \geq c$.
%\end{proof}

\begin{lemma}[Frostman's Lemma]
	If $E$ is Borel, $H^s(E) > 0$ if and only if there exists an $s$ dimensional Frostman measure supported on $E$.
\end{lemma}
\begin{proof}
	Suppose that $\mu$ is $s$ dimensional and supported on $E$. If $H^s(F) = 0$, then for $\varepsilon > 0$ there is a sequence of cubes $\{ I_k \}$ with $\sum_{k = 1}^\infty l(I_k)^s \leq \varepsilon$. But then
	%
	\[ \mu(F) \leq \mu \left( \bigcup_{k = 1}^\infty I_k \right) \leq \sum_{k = 1}^\infty \mu(I_k) \lesssim \sum_{k = 1}^\infty l(I_k)^s \leq \varepsilon. \]
	%
	Taking $\varepsilon \to 0$, we conclude $\mu(F) = 0$, so $\mu$ is absolutely continuous with respect to $H^s$. But since $\mu(E) > 0$, this means that $H^s(E) > 0$.

	Conversely, suppose $H^s(E) > 0$. Then by translating, we may assume that $H^s(E \cap [0,1)^d) > 0$, and so without loss of generality we may assume that $E \subset [0,1)^d$. Fix $M$, and for each $I \in \B(1/M^k,\RR^d)$, define
	%
	\[ \mu^+(I) = H^{s,M}_{1/M^k}(E \cap I) \]
	%
	Then $\mu^+(I) \leq 1/M^{ks}$, and $\mu^+$ is subadditive. We use it to recursively define a measure $\mu$ to which we can apply Lemma 4, such that $\mu(I) \leq \mu^+(I)$ for each $I \in \B(1/M^k)$. We initially define $\mu$ by setting $\mu([0,1)^d) = \mu^+([0,1)^d)$. Given $I \in \B(1/M^k,[0,1)^d)$, we enumerate all the children $J_1, \dots, J_M \in \B(1/M^{k+1},I)$. We then consider any values of $A_1, \dots, A_M$ such that
	%
	\[ A_1 + \dots + A_M = \mu(I)\quad\text{and}\quad A_i \leq \mu^+(J_i) \]
	%
	This is feasible to do because $\sum_{i = 1}^M \mu^+(J_i) \geq \mu^+(I) \geq \mu(I)$. We then define $\mu(J_i) = A_i$. The recursive constraint is satisfied, so $\mu$ is well defined. The mass distribution principle then implies that $\mu$ extends to a full measure, which satisfies $\mu(I) \leq \mu^+(I) \leq 1/M^{ks}$ for each $I \in \B(1/M^k,\RR^d)$, so it is a Frostman measure with dimension $s$.
\end{proof}







\section{Dimensions of Cantor-Type Sets}

A \emph{Cantor-type} decomposition of a set $E$ is a decreasing sequence of sets $\{ E_k \}$, together with a decreasing sequence of lengths $\{ l_k \}$, such that each $E_k$ is a finite union of cubes in $\B(l_k,\RR^d)$, and $E = \lim_{k \to \infty} E_k$. For simplicity, we assume $E \subset [0,1]^d$, and $l_0 = 1$. If a sequence of lengths $\{ l_k \}$ is fixed, then given a cube $I \in \B(l_{k+1},\RR^d)$, we let $I^* \in \B(l_k,\RR^k)$ denote the unique \emph{parent cube} of $I$, i.e. the unique cube with $I \subset I^*$.

In this thesis, Cantor-type decompositions naturally arise from the constructions of sets we produce. One advantage of the decomposition is the ability to construct Frostman measures. For instance, a natural choice is obtained by recursively defining a function $\mu$ by setting $\mu[0,1]^d = 1$, and setting, for each $I \in \B(l_{k+1},\RR^d)$,
%
\[ \mu(I) = \frac{\mu(I^*)}{\# \B(l_k,E_k \cap I^*)} \]
%
if $I \in \B(l_{k+1},E_k)$, and $\mu(I) = 0$ otherwise. The mass distribution principle then extends $\mu$ to a probability measure. We will refer to this as the \emph{canonical construction}. Simple combinatorial reasoning related to the construction of the set $E$ often enables one to establish bounds of the form $\mu(I) \lesssim l_k^s$ for $I \in \B(l_k,\RR^k)$. If the lengths $\{ l_k \}$ are dyadic, we have already shown this is sufficient to show $\mu$ is a Frostman measure of dimension $s$. But if we weaken the condition to being a Frostman measure of dimension $s - \varepsilon$ for all $\varepsilon > 0$, we can allow the lengths to decrease at a much faster rate, even up to where the lengths are \emph{hyperdyadic}.

\begin{theorem} \label{easyCoverTheorem}
	Let $l_k = 2^{-a_k}$, and suppose $a_{k+1} - a_k = o(a_k)$. If $\mu$ is a Borel measure such that $\mu(I) \lesssim l_k^s$ for each $I \in \B(l_k,\RR^d)$, then $\mu$ is a Frostman measure of dimension $s - \varepsilon$ for each $\varepsilon > 0$.
\end{theorem}
\begin{proof}
	The idea of the argument, which has been implicit in most of our other discretization approaches, is a covering argument. Given a cube $I$ with sidelength $l$, find $k$ such that $l_{k+1} \leq l \leq l_k$. Then we can cover $I$ by $O(1)$ cubes in $\B(l_k,\RR^d)$, which shows
	%
	\[ \mu(I) \lesssim l_k^s = [(l_k/l)^s l^\varepsilon ] l^{s-\varepsilon}. \]
	%
	The proof is completed by noticing
	%
	\[ (l_k/l)^s l^\varepsilon \leq (l_k/l_{k+1})^s l^\varepsilon \leq \exp(s(a_{k+1} - a_k) - \varepsilon a_k) \lesssim_\varepsilon 1, \]
	%
	so $\mu(I) \lesssim_\varepsilon l^{s-\varepsilon}$.
\end{proof}

This result applies, for instance, in the case where $l_k = 2^{-k^n}$ for some fixed integer $n$, since then $(k+1)^n - k^n = O(k^{n-1}) = o(k^n)$. We can even get `just' over polynomial powers of $n$. Let $l_k = 1/2^{2^{\psi(k) k}}$, where $\psi$ is a decreasing function of $k$, which tends to zero as $k \to \infty$, but for which $\psi(k) k$ is an increasing function which tends to $\infty$. Such a sequence is often called \emph{hyperdyadic}. It is simple to calculate that
%
\[ 2^{\psi(k+1) (k+1)} - 2^{\psi(k)k} \leq 2^{\psi(k)k}(2^{\psi(k)} - 1) = o(2^{\psi(k)k}). \]
%
thus it suffices to analyze Frostman measures at a hyperdyadic set of lengths. But it is of course easy to construct superexponential examples of $l_k$ where we cannot use this technique. And the next example shows we shouldn't be too hopeful in this scenario.

\begin{example}
	Let $l_k = 1/2^{a_k}$, where $a_{k+1} \geq 2a_k$ for each $k$. Form a set $E = \lim_{k \to \infty} E_k$, where $E_0 = [0,1]$, each $E_k$ is a union of sidelength $l_k^2$ cubes, and $E_{k+1}$ is obtained by selecting a single cube in $\B(l_{k+1}^2,I)$ from each element $I \in \B(l_{k+1}, E_k)$. For each $k$, $E$ is covered by at most $1/l_k$ sidelength $l_k^2$ cubes, so $E$ has lower Minkowski dimension at most $1/2$, and thus Hausdorff dimension at most $1/2$. Nonetheless, if we define the canonical measure $\mu$ from the Cantor-type decomposition $\{ E_k \}$, then one verifies that $\mu$ assigns the same amount of mass to each cube in $\B(l_k,E)$, and
	%
	\[ \# \B(l_k,E) = \# \B(l_{k-1},E) (l_{k-1}^2/l_k). \]
	%
	Thus as $k \to \infty$,
	%
	\[ \# \B(l_k,E) = \frac{l_{k-1}^2 \dots l_1^2}{l_k \dots l_1} = \frac{l_{k-1} \dots l_1}{l_k}. \]
	%
	Provided $a_1 + \dots + a_{k-1} = o(a_k)$, this implies that $\# \B(l_k,E) \gtrsim l_k^{1 - o(1)}$, so if $I \in \B(l_k,E)$, $\mu(I) \lesssim l_k^{1 - o(1)}$. Thus at the scales $\{ l_k \}$, $\mu$ looks like a Frostman measure of full dimension, which cannot be the case. This example works if $a_k = 2^{k^{1 + \varepsilon}}$, for which the lengths $\{ l_k \}$ are \emph{just} past the point of being hyperdyadic.
\end{example}

Thus, if we are to use a faster decreasing sequence of lengths $\{ l_k \}$ than hyperdyadic, we must have to exploit some extra property of the decomposition which is not always present. Here, we rely on some kind of \emph{uniformity} between scales. Given the uniformity assumption, the lengths can decrease as fast as desired.

%Our final method for interpolating requires extra knowledge of the dissection process, but enables us to choose the $l_k$ arbitrarily rapidly. The idea behind this is that there is an additional sequence of lengths $r_k$ with $l_k \leq r_k \leq l_{k-1}$. The difference between $r_k$ and $l_{k-1}$ is allowed to be arbitrary, but the decay rate between $l_k$ and $r_k$ is of polynomial-type, which enables us to use the covering methods of the previous section. In addition, we rely on a `uniform mass bound' between $r_k$ and $l_k$ to cover the remaining classes of intervals. Because we can take $r_k$ arbitrarily large relative to $l_k$, this renders any constants that occur in the construction to become immediately negligible. For two quantities $A$ and $B$, we will let $A \precsim_k B$ stand for an inequality with a hidden constant depending only on parameters with index smaller than $k$, i.e. $A \leq C(l_1, \dots, l_k, r_1,\dots,r_k) B$ for some constant $C(l_1, \dots, l_k, r_1, \dots, r_k)$ depending only on parameters with indices up to $k$.

\begin{theorem} \label{uniformHausdorffResult}
    Let $\mu$ be a measure supported on a set $E$, and $\{ l_k \}$ and $\{ r_k \}$ two decreasing sequences of lengths, with $l_{k+1} \leq r_{k+1} \leq l_k$ for each $k$. Write $l_k = 2^{-a_k}$ and $r_k = 2^{-b_k}$. Suppose that
    %
    \begin{enumerate}
    	\item \label{discreteBound} \emph{(Discrete Bound)}: For any $I \in \B(l_k,\RR^d)$, $\mu(I) \lesssim l_k^s$.
    	\item \label{controlledScale} \emph{(Hyperdyadic Scale Change)}: $b_k - a_k = o(a_k)$.
    	\item \label{uniformDist} \emph{(Uniform Mass Distribution)}: For any $I \in \B(l_k,\RR^d)$, and $J \in \B(r_{k+1},I)$,
    	%
    	\[ \mu(J) \lesssim (r_{k+1}/l_k)^d \mu(I). \]
    \end{enumerate}
	%
	Then $\mu$ is a Frostman measure of dimension $s-\varepsilon$ for each $\varepsilon > 0$.
\end{theorem}
\begin{proof}
	Suppose an interval $I$ has length $l$, and we can find $k$ with $r_{k+1} \leq l \leq l_k$. Then we can cover $I$ by at most $(l/r_{k+1})^d$ cubes in $\B(r_{k+1},\RR^d)$. By Properties \ref{discreteBound} and \ref{uniformDist}, each of these cubes has measure at most $(r_{k+1}/l_k)^d l_k^s$, so we obtain that for any $\varepsilon > 0$,
    %
    \[ \mu(I) \lesssim (l/r_{k+1})^d (r_{k+1}/l_k)^d l_k^s = l^d / l_k^{d-s} \lesssim l^s. \]
    %
    In particular, this implies $\mu(I) \lesssim r_k^s$ if $I \in \B(r_k,\RR^d)$. On the other hand, suppose there exists $k$ with $l_k \leq l \leq r_k$. Then we can cover $I$ by $O(1)$ cubes in $\B(r_k,\RR^d)$, so we find
    %
    \[ \mu(I) \lesssim r_k^s \leq [(r_k/l_k)^s l^\varepsilon] l^{s-\varepsilon}. \]
    %
    Property \ref{controlledScale} shows $(r_k/l_k)^s l^\varepsilon \lesssim_\varepsilon 1$, so $\mu(I) \lesssim_\varepsilon l^{s-\varepsilon}$ for each $\varepsilon > 0$. This covers all possible cases, so $\mu$ is a Frostman measure of dimension $s-\varepsilon$ for each $\varepsilon > 0$.
\end{proof}

%\begin{remark}
%    The condition $\mu_\beta(J) \lesssim_{N-1} (r_N/l_N) \mu_\beta(I)$ essentially means that the probability mass on a length $l_N$ interval $I$ is uniformly distributed over the length $r_N$ intervals it contains. This is what enables us to remove the discussion of the growth of the sequence $\beta$ over time from discussion.
%\end{remark}

%Since the construction is obtained as a limit of intervals, it is often possible to construct such a $\mu$ by the {\it mass distribution principle}. That is, we let $\mu$ denote the weak limit of the probability masses $\mu_n$, where $\mu_0$ is a uniform distribution over $\mu_0$, and $\mu_{n+1}$ is obtained from $\mu_n$ by distributing the mass $\mu_n(I)$ of each length $l_n$ interval $I$ contained in $X_n$ over the portion of $I$ that remains in $X_{n+1}$. The cumulative distribution functions of the $\mu_n$ uniformly converge, hence the $\mu_n$ converge weakly to some $\mu$, which satisfy $\mu(I) = \mu_n(I)$ for each interval $I$ as above. Because of this discreteness, it is most easy to establish a bound $\mu(I) \lesssim l_n^\alpha$ when $I \subset X_n$ is a length $l_n$ interval. Since any interval $I$ of length $l_n$ is contained within at least two such intervals (or is contained in other length $l_n$ intervals that $\mu$ assigns no mass to), we have the general bound $\mu(I) \lesssim l_n^\alpha$ for all intervals $I$ of length $l_n$. Hausdorff dimension is a local property of a set\footnote{If we define $\dim_{\mathbf{H}}(x) = \lim_{r \downarrow 0} \dim_{\mathbf{H}}(B_r(x) \cap X)$ then $\dim_{\mathbf{H}}(X) = \sup_{x \in X} \dim_{\mathbf{H}}(x)$.}, so it is natural to expect that we can obtain a general bound $\mu(I) \lesssim_\alpha|I|^\alpha$ given that one has established precisely the same estimate, but restricted to intervals $I$ with $|I| = l_N$. This section concerns itself with ways that we can establish this general bound, and thus prove that $\dim_{\mathbf{H}}(X) \geq \alpha$.

\endinput


















\begin{lemma}
	Let $E$ be a set, and $\mu$ a Borel probability measure supported on $E$. Suppose that for any $\varepsilon$, there exists a constant $c_\varepsilon$ such that if $\B(1/M^k,E) \leq c_\varepsilon M^{k(s-\varepsilon)}$, then $\mu(E) \lesssim 1/k^2$. then $E$ has Hausdorff dimension $s$.
\end{lemma}
\begin{proof}
	Suppose $H^{s-\varepsilon}(E) = 0$. Then for any $N$ there exists a cover of $E$ by cubes $\{ I_k \}$, with lengths $\{ l_k \}$ such that $I_k \in \B(l_k,\RR^d)$, $l_k \leq 1/M^N$ for all $k$, and $\sum l_k^{s - \varepsilon} \leq c_\varepsilon$. For each $m$, let $A_m = \# \{ k : 1/M^{m+1} \leq l_k \leq 1/M^m \}$. Then
	%
	\[ \sum_{m = N}^\infty A_m M^{-(m+1)(s - \varepsilon)} \leq \sum_{k = 1}^\infty l_k^{s - \varepsilon} \leq c_\varepsilon \]
	%
	Thus $A_m \leq c_\varepsilon M^{(m+1)(s - \varepsilon)}$. This means that if $E_m$ is formed from the union of all intervals $I_k$ with $1/M^{m+1} \leq l_k \leq 1/M^m$, then $\# \B_s(E_k) \leq c_\varepsilon M^{(m+1)(s-\varepsilon)}$. Thus $\mu(E_m) \lesssim 1/k^2$, so
	%
	\[ \mu(E) \leq \sum_{m = N}^\infty \mu(E_m) \lesssim \sum_{m = N}^\infty 1/k^2 \]
	%
	as $N \to \infty$, we conclude $\mu(E) = 0$, which is impossible. Thus $H^{s-\varepsilon}(E) > 0$ for all $\varepsilon$, so $\hausdim(E) \geq s$.
\end{proof}

The hypothesis of Lemma 7 is certainly satisfied if $\mu(I) \lesssim_\varepsilon l_k^{s - \varepsilon}/k^2$ for each $I \in \B(l_k)$, where $l_k = 1/M^k$. Thus establishing a Frostman-type bound at a sequence of dyadic type scales is enough to obtain a dimensional result for $E$. The advantage of this proof is that we can continue the argument to give results when the sequence of scales decreases much faster than scales of dyadic type.

\begin{theorem}
	Let $E$ be a set, and $\mu$ a Borel probability measure supported on $E$. Suppose that for any $\varepsilon$, there exists a constant $c_\varepsilon$ such that if $\B(1/M^k,E') \leq c_\varepsilon M^{k(s-\varepsilon)}$ for any $E' \subset E$, then $\mu(E') \lesssim 1/k^2$. Then $E$ has Hausdorff dimension $s$.
\end{theorem}
\begin{proof}
	As before, if $H^{s-\varepsilon}(E) = 0$, consider a covering by $\{ I_k \}$ with parameters $\{ l_k \}$. Fix $\alpha > 1$, and consider
	%
	\[ A_m = \# \{ k : 1/M^{\alpha^{m + 1}} \leq l_k \leq 1/M^{\alpha^m} \} \]
	%
	Then
	%
	\[ \sum A_m/M^{(s - \varepsilon) \alpha^{m+1}} \leq \sum l_k^{s - \varepsilon} \leq c_\varepsilon \]
	%
	so $A_m \leq c_\varepsilon M^{(s - \varepsilon) \alpha^{m+1}}$. Thus if we define $E_m$ as in the last proof, then $\mu(E_m) \lesssim 1/k^2$.
\end{proof}

The requirement of this lemma is satisfied if we are able to prove $\mu(I) \lesssim_\varepsilon l_k^{s - \varepsilon}/k^2$ where $l_k = M^{- \alpha^k}$. These are \emph{hyperdyadic} numbers.

\begin{proof}
	Suppose that $H^{s-\varepsilon}(E) = 0$. Then for any $M$ and $c_\varepsilon$, $E$ is covered by cubes $\{ I_k \}$ with sidelengths $\{ r_k \}$ such that $I_k \in \B(r_k,\RR^d)$, $r_k \leq l_M$ for each $k$, and $\sum r_k^{s - \varepsilon} \leq c_\varepsilon$. For each $k$, let $A_m = \# \{ k: l_{m+1} \leq r_k \leq l_m \}$. Then
	%
	\[ \sum_{m = M}^\infty A_m l_{m+1}^{s - \varepsilon} \leq \sum_{k = 1}^\infty r_k^{s - \varepsilon} \leq c_\varepsilon \]
	%
	so $A_m \leq c_\varepsilon / l_{m+1}^{s-\varepsilon}$. But if we can establish an estimate $\mu(I) \lesssim_\varepsilon B_m l_m^{s - \varepsilon}$, then 
	%
	\[ \mu(E) \lesssim_\varepsilon \sum_{m = M}^\infty A_m (B_m l_m^{s - \varepsilon}) \leq \sum_{m = M}^\infty c_\varepsilon B_m (l_m/l_{m+1})^{s - \varepsilon} \]
\end{proof}

\section{Hyperdyadic Covers}

\begin{theorem}
	If $\inf_{\delta > 0} H^s_\delta(E) = 0$, then $\hausdim(E) \leq s$.
\end{theorem}
\begin{proof}
	Suppose that $H^s_\delta(E) \leq \varepsilon$. Then there is a sequence of cubes $\{ I_k \}$ with parameters $\{ l_k \}$ such that $I_k \in \B(l_k)$, $l_k \leq \delta$ for all $k$, and $\sum_{k = 1}^\infty l_k^s \leq 2\varepsilon$. Then $l_k \leq (2\varepsilon)^{1/s}$ for all $k$, so we can actually take $\delta \to 0$?
\end{proof}

\begin{theorem}
	If $X$ is strongly covered by $\{ X_k \}$, and there is $\delta$ such that
	%
	\[ \sum_{k = 1}^\infty H^s_\delta(X_k) < \infty \]
	%
	then $\hausdim(X) \leq s$.
\end{theorem}
\begin{proof}
	By subadditivity, for any $N$,
	%
	\[ H^s_\delta(X) \leq \sum_{k = N}^\infty H^s_\delta(X_k) \]
	%
	and as $N \to \infty$, we conclude $H^s_\delta(X) = 0$.
\end{proof}

For a fixed $0 < \varepsilon \ll 1$, a \emph{hyperdyadic number} is a number of the form $h_k = 2^{-\lfloor (1 + \varepsilon)^k \rfloor}$ for some $k \geq 0$. A set $E$ is \emph{$\delta$ discretized} if it is the union of balls, each with radius between $c_\varepsilon \delta^{1 + C\varepsilon}$ and $C_\varepsilon \delta^{1-C\varepsilon}$. A set $E$ is a $(\delta,s)_d$ set if it is bounded, $\delta$ discretized, and for all $\delta \leq r \leq 1$, $|E \cap B(x,r)| \leq C_\varepsilon \delta^{n-s-C\varepsilon} r^s$

\begin{lemma}
	Let $0 < s < d$, and let $E$ be a compact subset of $\RR^n$.
	%
	\begin{itemize}
		\item If $\dim(E) \leq s$, for each $k$, we can associate a $(h_k,s)_d$ set $E_k$ such that $E$ is strongly covered by the $E_k$.

		\item If $C$ is sufficiently large, and there is a $(h_k, s - C\varepsilon)_n$ set $E_k$ strongly covering $E$, then $\dim(E) \leq s$.
	\end{itemize}
\end{lemma}
\begin{proof}
	We first prove the latter claim, assuming without loss of generality that $E$ is contained in the unit ball. Suppose $E$ is strongly covered by the $E_k$. If $E_k$ is a $(h_k,s - C\varepsilon)_d$ set, then
	%
	\[ |E_k| \leq C_\varepsilon \delta^{n-(s-C\varepsilon)-C\varepsilon} = C_\varepsilon \delta^{n-s}. \]
	%
	\[ H^s_{C_\varepsilon h_k^{1 - C\varepsilon}} \]
\end{proof}

\section{Hyperdyadic Covers}

Recall the definition of the Hausdorff measure $H^\alpha(E) = \lim_{\delta \to 0} H^\alpha_\delta(E)$, where $H^\alpha_\delta(E)$ is the greatest lower bound of $\sum r_n^\alpha$, over all choices of covers of $E$ by cubes $I_1, I_2, \dots$, where $I_n$ has sidelengths $r_n$. We then define the Hausdorff dimension of $E$ to be the least upper bound of the scalars $\alpha$ such that $H^\alpha(E) = 0$, or alternatively, the greatest lower bound of $\alpha$ such that $H^\alpha(E) = \infty$.

To determine the Hausdorff dimension of $E$, it suffices to consider only dyadic cubes in the cover of $E$. Define $H^\alpha(E) = \lim_{\delta \to 0} H^\alpha_{D,\varepsilon}(E)$, where $H^\alpha_{D,\varepsilon}(E)$ is the greatest lower bound of $\sum r_n^\alpha$ over \emph{dyadic} covers $I_1, I_2, \dots$, with $I_n \in \mathcal{B}(r_n)$. Then $H^\alpha_D$ is comparable with $H^\alpha$.

\begin{theorem}
    For any set $E$, $H^\alpha(E) \leq H^\alpha_D(E) \leq 2^{d + \alpha} H^\alpha(E)$.
\end{theorem}
\begin{proof}
    Given any not necessarily dyadic cover $I_1, I_2, \dots$, we can replace each sidelength $r_n$ cube $I_n$ with at most $2^d$ dyadic cubes with radius at most $2r_n$, which gives $H^\alpha_{D,\varepsilon}(E) \leq 2^{1 + \alpha} H^\alpha_\varepsilon(E)$, and taking the limit as $\varepsilon \to 0$ then gives the required upper bound for $H^\alpha_D$.
\end{proof}

If we are restricting ourselves to cubes lying at a series of discrete scales, it seems as if the dyadic sequence is about as fast as we can use so that the resultant Hausdorff measure is comparable to the usual Hausdorff measure. Nonetheless, using a weak type bound we can get results for a faster decreasing family of scales. This is necessary for our calculations. We fix a positive $\delta$, and consider a sequence of {\bf hyperdyadic scales} $H_N = 2^{- \lfloor (1 + \delta)^N \rfloor}$. A {\bf hyperdyadic cube} is then a cube in $\mathcal{B}(H_N)$ for some $N$.

%To measure the difference in decay rates between hyperdyadic and dyadic scales, we note that for any $n$, and $0 < A < 1$, the number of dyadic scales between $A$ and $A^n$ is comparable to $n \log(1/A)$, whereas the number of hyperdyadic scales is comparable to $\log(n) / \log(1 + \delta)$, which is completely independant of $A$. As is expected, a naive covering approach as in the last argument doesn't suffice to give results about dimensions and hyperdyadic coverings.

\begin{proof}
    For any sidelength $L$ cube, we can cover the cube by at most $2^d$ hyperdyadic cubes with sidelength at most $2L^{1 - \delta} \geq 2L^{(1+\delta)^{-1}}$. This is because
    %
    \[ 2 H_{N+1}^{(1 + \delta)^{-1}} = 2^{1 - (1 + \delta)^{-1} \lfloor (1 + \delta)^{N+1} \rfloor} \geq 2^{1 - (1 + \delta)^N} \geq 2^{\lfloor (1 + \delta)^N \rfloor} = H_N \]
    %
    If $E$ has Hausdorff dimension $\alpha$, for every $\varepsilon$ and $N$ we can find a collection of dyadic cubes $I_1, I_2, \dots$ covering $E$ with $I_k$ sidelength $L_k \leq H_N$, and $\sum L_{N,i}^{\alpha + \varepsilon} \lesssim_\varepsilon 1$. A weak type bound implies the number of cubes $I_k$ with $H_{N+1} \leq L_k \leq H_N$ is $O_\varepsilon(1/H_{N+1}^{\alpha + \varepsilon})$. But
    %
    \[ 1/H_{N+1}^{\alpha + \varepsilon} \leq (H_N/H_{N+1})^{\alpha + \varepsilon} 1/H_N^{\alpha + \varepsilon} \lesssim 1 / H_N^{\alpha + \varepsilon + \delta} \]
    %
    and so the cover of $E$ by hyperdyadic cubes contains $O_\varepsilon(1/H_N^{\alpha + \varepsilon + \delta})$ length $H_N$ cubes for each $N$.



    If we swap each cube $I_{N,i}$ with $2^d$ hyperdyadic cubes of length at most $2L^{1 - \delta}$, we obtain
    %
    \begin{align*}
        \sum 2^d (2 L_{N,i}^{1 - \delta})^{\alpha + \varepsilon} &= 2^{d + \alpha + \varepsilon} \sum L_{N,i}^{(1 - \delta)(\alpha + \varepsilon)} \lesssim_\varepsilon 1
    \end{align*}
    %
    Thus $H^{(1 - \delta)\alpha + \varepsilon}_{HD}(E) \lesssim_\varepsilon 1$.

    We can swap each cube $I_i$ with $2^d$ hyperdyadic cubes of length at most $2L^{(1 + \delta)^{-1}}$, without effecting the estimate too much.

    Then for every hyperdyadic number $H_N$, we can find a collection of cubes $I_{N,1}, I_{N,2}, \dots$ covering $E$ with $I_{N,i}$ sidelength $r_{N,i} \leq H_N$, and $\sum r_{N,i}^{\alpha + \varepsilon} \lesssim_\varepsilon 1$. Covering each cube by $2^d$ cubes with hyperdyadic sidelengths, which magnifies $r_{N,i}$ by at most
    %
    \[ 2 \cdot 2^{(1 + \delta)^{N+1} - (1 + \delta)^N} = 2 \cdot 2^{\delta (1 + \delta)^N} \lesssim 2 \cdot r_{N,i}^{- \delta} \]
    %
    We conclude that
    %
    \[ 2^{d+\alpha+\varepsilon} 2^{(\alpha + \varepsilon) \delta(1 + \delta)^N} C_\varepsilon \]
\end{proof}

We assume $\delta$ and $\varepsilon$ are some fixed parameters. If $A(\varepsilon, \delta)$ and $B(\varepsilon,\delta)$ are two quantities depending on $\varepsilon$ and $\delta$, we write $A \preccurlyeq B$ mean $A \lesssim_\varepsilon \delta^{-C \varepsilon} B$ for some $C$, and for every $\varepsilon$. We let $A \approx B$ mean $A \preccurlyeq B$ and $B \preccurlyeq A$ hold simultaneously. We say a union of balls is $\delta$ discretized if it is the union of balls with radius $\approx \delta$. Thus there exists $C_\varepsilon$ and $C$ such that for each ball $B_r$ of radius $r$, $|r - \delta| \leq C_\varepsilon \delta^{1-C \varepsilon}$. Thus
%
\[ \delta(1 - C_\varepsilon \delta^{-C \varepsilon}) \leq r \leq \delta(1 + C_\varepsilon \delta^{- C \varepsilon}) \]
%
In particular, the dyadic scales $2^{-\lfloor (1 + \varepsilon)^k \rfloor}$ are allowed in a discretization of a hyperdyadic scale $2^{-(1+\varepsilon)^k}$, since we can choose $C_\varepsilon$ and $C$ such that
%
\[ 1 - C_\varepsilon 2^{C (1 + \varepsilon)^k \varepsilon} \leq 1 \leq 2^{(1 + \varepsilon)^k -\lfloor (1 + \varepsilon)^k \rfloor} \leq 2 \leq 1 + C_\varepsilon 2^{(1 + \varepsilon)^k C \varepsilon} \]

\begin{theorem}
    Let $E$ be a compact subset of $\mathbf{R}^n$. If $0 < \alpha < n$, and $\dim(E) \leq \alpha$, then for each hyperdyadic number $\delta$, we can associate a $\delta$ discretized set $X_\delta$ with $|X_\delta \cap B(x,r)| \preccurlyeq \delta^n (r/\delta)^\alpha$ for all $\delta \leq r \leq 1$ and $x \in \mathbf{R}^n$, and every element of $E$ is contained in infinitely many of the $X_\delta$.
\end{theorem}
\begin{proof}
    Fix $E$. For every hyperdyadic $\delta$, we can find a cover of $E$ by balls $B(x_{\delta n}, r_{\delta n})$ such that $r_{\delta n} < \delta$, and
    %
    \begin{equation} \sum_n r_{\delta n}^{\alpha + C\varepsilon} \lesssim 1 \end{equation}
    %
    Choose $m_{\delta n}$ such that $2^{-(1 + \varepsilon)^{m_{\delta n}+1}} \leq r_{\delta n} \leq 2^{-(1 + \varepsilon)^{m_{\delta n}}}$. We calculate
    %
    \begin{align*}
        \frac{2^{-(\alpha + C'\varepsilon) (1 + \varepsilon)^{m_{\delta n}}}}{r_{\delta n}^{\alpha + C\varepsilon}} &\leq \frac{2^{- (\alpha + C'\varepsilon) (1 + \varepsilon)^{m_{\delta n}}}}{2^{- (\alpha + C\varepsilon) (1 + \varepsilon)^{m_{\delta n} + 1}}}\\
        &= \left( 2^{\varepsilon (1 + \varepsilon)^{m_{\delta n}}} \right)^{\alpha + (C (1 + \varepsilon) - C')}
    \end{align*}
    %
    Provided that $C' > \alpha + C(1 + \varepsilon)$, the quantity on the left is $\leq 1$, which is independant of $\varepsilon$ provided that $\varepsilon$ is bounded from above, and so we conclude
    %
    \[ \sum_n \left( 2^{-(1 + \varepsilon)^{m_{\delta n}}} \right)^{\alpha + C' \varepsilon} \leq \sum_n r_{\delta n}^{\alpha + C\varepsilon} \]
    %
    Thus we may assume by changing the value of $C$ that the quantities $r_{\delta n}$ are hyperdyadic from the outset. This means that at each hyperdyadic scale $\delta$, the number of hyperdyadic balls at the scale $\delta$ in each cover is $\lesssim (1/\delta)^{\alpha + C\varepsilon}$. STOP IS THIS ALL WE NEED, THEN COME BACK TO THE PROOF.


    For a pair of hyperdyadic numbers $\delta$ and $\gamma$ we set
    %
    \[ Y_{\delta \gamma} = \bigcup_{r_{\delta n} = \gamma} B(x_{\delta n}, r_{\delta n}) \]
    %
    Every element of $X$ is in infinitely many of the $Y_{\delta n}$. For each $\delta$ and $\gamma$, we let $Q_{\delta \gamma}$ be the collection of hyperdyadic cubes with sidelength at least $\gamma$ covering $Y_{\delta \gamma}$ and minimizing $\sum_{Q \in Q_{\delta \gamma}} l(Q)^\alpha$. From condition (1.1) we obtain that $Y_{\delta \gamma}$ can be covered by at most $r^{-\alpha - \varepsilon}$ sidelength $r$ cubes, so
    %
    \[ \sum_{Q \in Q_{\delta \gamma}} l(Q)^\alpha \leq Cr^{-\varepsilon} \]
    %
    and so $l(Q) \leq Cr^{-\varepsilon/\alpha}$ for all $Q \in Q_{\delta \gamma}$. From the construction of $Q_{\delta \gamma}$, we see that the $Q$ are all disjoint, and for any hyperdyadic cube $I$,
    %
    \[ \sum_{\substack{Q \in Q_{\delta \gamma}\\Q \subset I}} l(Q)^\alpha \leq l(I)^\alpha \]
    %
    since otherwise we could replace such elements of $Q$ in $Q_{\delta \gamma}$ by $I$ itself.
\end{proof}










\section{Hypergraphs}

\begin{lemma}[Tur\'{a}n]
    For any $k$ uniform hypergraph $H = (V,E)$ with $|E| \leq |V|^\alpha$, $V$ contains an independant set of size $\Omega(|V|^{(k-\alpha)/(k-1)})$.
\end{lemma}
\begin{proof}
    We create an independant set $I$ by the following procedure. First, select a subset $S$ of vertices, including each independantly with probability $p$. Delete a single vertex from each edge in each hypergraph entirely contained in $S$, obtaining an independant set $I$. We find that each edge in $V$ is entirely included in $S$ with probability $p^k$, and $S$ has expected size $p |V|$, so $\mathbf{E}|I| = p |V| - p^k |E|$. If $|E| = |V|^\alpha$ for $\alpha \geq 1$, then setting $p = (1/2) |V|^{(1 - \alpha)/(k-1)}$ induces a set $I$ with size
    %
    \[ |V|^{(k - \alpha)/(k-1)}(1/2 - 1/2^k) \]

    We create an independant set $I$ by the following procedure. First, select a subset $S$ of vertices, including each vertex independantly with probability $p$. Delete a single vertex from each edge in each hypergraph which is entirely contained in $S$. Then $I$ is an independant set with respect to each hypergraph, and we shall show that for an appropriate choice of $p$, $\mathbf{E} |I| \geq h$.

    Trivially, we find $\mathbf{E}|S| = p |V|$. For any $i \geq 2$, the expected number of edges of $H_i$ falling entirely in $S$ is
    %
    \[ p^i |E_i| \leq \frac{p^i |V|^i}{c_k h^{i-1}} \]
    %
    therefore
    %
    \[ \mathbf{E}|I| = p|V| - \sum_{i = 2}^k \frac{p^i |V|^i}{c_k h^{i-1}} \]
    %
    Setting $p = 2h/|V|$ and $c_k = 2^{k+1}$ gives
    %
    \[ \mathbf{E}|I| = h \left( 2 - \sum_{i = 2}^k \frac{1}{2^{k+1-i}} \right) > h \]
    %
    which completes the proof.
\end{proof}